{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.3.0.tar.gz (1.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightgbm) (1.24.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightgbm) (1.12.0)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for lightgbm \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[41 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2024-02-27 10:41:15,797 - scikit_build_core - INFO - RUN: /private/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/pip-build-env-q9h9owmx/normal/lib/python3.10/site-packages/cmake/data/bin/cmake --version\n",
      "  \u001b[31m   \u001b[0m 2024-02-27 10:41:15,803 - scikit_build_core - INFO - CMake version: 3.28.3\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.8.1\u001b[0m using \u001b[94mCMake 3.28.3\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2024-02-27 10:41:15,806 - scikit_build_core - INFO - Build directory: /private/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/tmpqtvcbe_0/build\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2024-02-27 10:41:15,814 - scikit_build_core - INFO - RUN: /private/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/pip-build-env-q9h9owmx/normal/lib/python3.10/site-packages/ninja/data/bin/ninja --version\n",
      "  \u001b[31m   \u001b[0m 2024-02-27 10:41:16,016 - scikit_build_core - INFO - Ninja version: 1.11.1\n",
      "  \u001b[31m   \u001b[0m 2024-02-27 10:41:16,017 - scikit_build_core - WARNING - libdir/ldlibrary: /Library/Frameworks/Python.framework/Versions/3.10/lib/Python.framework/Versions/3.10/Python is not a real file!\n",
      "  \u001b[31m   \u001b[0m 2024-02-27 10:41:16,018 - scikit_build_core - INFO - RUN: /private/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/pip-build-env-q9h9owmx/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -S. -B/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/tmpqtvcbe_0/build -DCMAKE_BUILD_TYPE:STRING=Release -C/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/tmpqtvcbe_0/build/CMakeInit.txt -DCMAKE_MAKE_PROGRAM=/private/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/pip-build-env-q9h9owmx/normal/lib/python3.10/site-packages/ninja/data/bin/ninja -D__BUILD_FOR_PYTHON:BOOL=ON\n",
      "  \u001b[31m   \u001b[0m loading initial cache file /var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/tmpqtvcbe_0/build/CMakeInit.txt\n",
      "  \u001b[31m   \u001b[0m -- The C compiler identification is AppleClang 15.0.0.15000100\n",
      "  \u001b[31m   \u001b[0m -- The CXX compiler identification is AppleClang 15.0.0.15000100\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND)\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_C: -Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_CXX: -Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP: TRUE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_PREFETCH\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_PREFETCH - Failed\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_MALLOC\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_MALLOC - Success\n",
      "  \u001b[31m   \u001b[0m -- Using _mm_malloc\n",
      "  \u001b[31m   \u001b[0m -- Configuring done (1.3s)\n",
      "  \u001b[31m   \u001b[0m -- Generating done (0.0s)\n",
      "  \u001b[31m   \u001b[0m -- Build files have been written to: /var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/tmpqtvcbe_0/build\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2024-02-27 10:41:17,348 - scikit_build_core - INFO - RUN: /private/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/pip-build-env-q9h9owmx/normal/lib/python3.10/site-packages/cmake/data/bin/cmake --build /var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/tmpqtvcbe_0/build\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/opt/homebrew/opt/libomp/lib/libomp.dylib', needed by '/private/var/folders/bz/jsngdqs55xxdjx6nm7z430l00000gn/T/pip-install-653lq3ox/lightgbm_413ba50a41074ed68e41646aee2453da/lib_lightgbm.so', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[91m\u001b[1m*** CMake build failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for lightgbm\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build lightgbm\n",
      "\u001b[31mERROR: Could not build wheels for lightgbm, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: group_lasso in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from group_lasso) (1.24.4)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from group_lasso) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->group_lasso) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->group_lasso) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn->group_lasso) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ISLP in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.3.22)\n",
      "Requirement already satisfied: numpy<1.25,>=1.7.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (1.24.4)\n",
      "Requirement already satisfied: scipy>=0.9 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (1.12.0)\n",
      "Requirement already satisfied: pandas<=1.9,>=0.20 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (1.5.3)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (5.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (1.3.2)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (0.14.1)\n",
      "Requirement already satisfied: lifelines in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (0.28.0)\n",
      "Requirement already satisfied: pygam in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (0.9.0)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (2.2.1)\n",
      "Requirement already satisfied: pytorch-lightning in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (2.2.0.post0)\n",
      "Requirement already satisfied: torchmetrics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ISLP) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/marco/Library/Python/3.10/lib/python/site-packages (from pandas<=1.9,>=0.20->ISLP) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas<=1.9,>=0.20->ISLP) (2024.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=1.2->ISLP) (3.3.0)\n",
      "Requirement already satisfied: patsy>=0.5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from statsmodels>=0.13->ISLP) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/marco/Library/Python/3.10/lib/python/site-packages (from statsmodels>=0.13->ISLP) (23.2)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lifelines->ISLP) (3.8.3)\n",
      "Requirement already satisfied: autograd>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lifelines->ISLP) (1.6.2)\n",
      "Requirement already satisfied: autograd-gamma>=0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lifelines->ISLP) (0.5.0)\n",
      "Requirement already satisfied: formulaic>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lifelines->ISLP) (1.0.1)\n",
      "Requirement already satisfied: progressbar2<5.0.0,>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pygam->ISLP) (4.3.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning->ISLP) (4.66.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning->ISLP) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning->ISLP) (4.9.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pytorch-lightning->ISLP) (0.10.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->ISLP) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->ISLP) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->ISLP) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch->ISLP) (3.1.3)\n",
      "Requirement already satisfied: future>=0.15.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from autograd>=1.5->lifelines->ISLP) (1.0.0)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from formulaic>=0.2.2->lifelines->ISLP) (1.16.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.9.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning->ISLP) (65.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines->ISLP) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines->ISLP) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines->ISLP) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines->ISLP) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines->ISLP) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.0->lifelines->ISLP) (3.1.1)\n",
      "Requirement already satisfied: six in /Users/marco/Library/Python/3.10/lib/python/site-packages (from patsy>=0.5.4->statsmodels>=0.13->ISLP) (1.16.0)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from progressbar2<5.0.0,>=4.2.0->pygam->ISLP) (3.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch->ISLP) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch->ISLP) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (4.0.3)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->ISLP) (3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from xgboost) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lightgbm\n",
    "%pip install torch\n",
    "%pip install group_lasso\n",
    "%pip install ISLP\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.linear_model import (LinearRegression, \n",
    "                                  HuberRegressor,\n",
    "                                  ElasticNet)\n",
    "from sklearn.metrics import (mean_squared_error, \n",
    "                             r2_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (TimeSeriesSplit, \n",
    "                                     ParameterGrid)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import (GradientBoostingRegressor,\n",
    "                              RandomForestRegressor as RF)\n",
    "from group_lasso import GroupLasso\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and changing it to be usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dependent_y.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/marco/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/University - OD/EUR/Seminars/Data Science in Finance/Group Project/DS-Project/Main_Modelling_file.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marco/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/University%20-%20OD/EUR/Seminars/Data%20Science%20in%20Finance/Group%20Project/DS-Project/Main_Modelling_file.ipynb#Y163sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Read datafiles into dataframes.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marco/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/University%20-%20OD/EUR/Seminars/Data%20Science%20in%20Finance/Group%20Project/DS-Project/Main_Modelling_file.ipynb#Y163sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# y = the excess returns. X has the predictors.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/marco/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/University%20-%20OD/EUR/Seminars/Data%20Science%20in%20Finance/Group%20Project/DS-Project/Main_Modelling_file.ipynb#Y163sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mDependent_y.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, index_col\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marco/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/University%20-%20OD/EUR/Seminars/Data%20Science%20in%20Finance/Group%20Project/DS-Project/Main_Modelling_file.ipynb#Y163sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mFeatures_X.csv\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marco/Library/CloudStorage/OneDrive-ErasmusUniversityRotterdam/University%20-%20OD/EUR/Seminars/Data%20Science%20in%20Finance/Group%20Project/DS-Project/Main_Modelling_file.ipynb#Y163sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m y\u001b[39m.\u001b[39mfillna(\u001b[39m0\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m# Do we need this? Isn't it already done in Pre-Processing?\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dependent_y.csv'"
     ]
    }
   ],
   "source": [
    "# Read datafiles into dataframes.\n",
    "# y = the excess returns. X has the predictors.\n",
    "y = pd.read_csv('Dependent_y.csv', header=0, index_col=0)\n",
    "X = pd.read_csv('Features_X.csv', header=0, index_col=0)\n",
    "\n",
    "\n",
    "y.fillna(0, inplace=True) # Do we need this? Isn't it already done in Pre-Processing?\n",
    "\n",
    "# Converting data to Dates as index\n",
    "y.index = pd.to_datetime(y.index, format=\"%Y-%m\").to_period('M')\n",
    "X.index = pd.to_datetime(X.index, format=\"%Y-%m\").to_period('M')\n",
    "\n",
    "# Creating the weights of the stocks compared to the portfolio\n",
    "weights = pd.read_csv('Stocks_weights.csv', header=0)\n",
    "weights.index = weights['Date']\n",
    "weights = weights.drop('Date', axis=1)\n",
    "weights.index = pd.to_datetime(weights.index, format=\"%Y-%m\").to_period('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def R_oos(num, den):\n",
    "    \"\"\"\n",
    "Calculates the Out Of Sample R-squared\n",
    "Input: \n",
    "    - num: Numerator\n",
    "    - den: Denomenator\n",
    "\n",
    "    Output: Out of sample R-squared\n",
    "    \"\"\"\n",
    "    R_oos_val = 1 - (np.sum(num)/np.sum(den))\n",
    "    return R_oos_val\n",
    "\n",
    "\n",
    "\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld, max_iter=10, tol=1e-4):\n",
    "    \"\"\"\n",
    "Validates a model to get the best parameters\n",
    "Input: \n",
    "    - model: The model we are validating.\n",
    "    - params: A dictionary of parameters.\n",
    "    - X_trn: Predictors training set.\n",
    "    - y_trn: Dependent variable training set.\n",
    "    - X_vld: Predictors validation set.\n",
    "    - y_vld:Dependent variable validation set.\n",
    "    - max_iter: ...\n",
    "    - tol: ...\n",
    "\n",
    "    Output: Best parameters.\n",
    "    \"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    no_improvement_count = 0\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "        else:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_param = param\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= max_iter:\n",
    "                    break\n",
    "            if abs(ros - best_ros) < tol:\n",
    "                break\n",
    "    return best_param\n",
    "\n",
    "\n",
    "def Sharpe_gain(Sharpe_val, Roo2_val):\n",
    "    \"\"\"\n",
    "Here: what this function does.\n",
    "Input: \n",
    "    - Sharpe_val: \n",
    "    - Roo2_Val:\n",
    "\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    SR_star = np.sqrt(((Sharpe_val**2)+Roo2_val)/(1-Roo2_val))\n",
    "    return SR_star - Sharpe_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fun_NN(model, params: dict, X_trn, y_trn, X_vld, y_vld, illustration=True):\n",
    "    \"\"\"\n",
    "Validates a Neural Network to return the best mode.\n",
    "Input: \n",
    "    - model: The model we are validating.\n",
    "    - params: A dictionary of parameters.\n",
    "    - X_trn: Predictors training set.\n",
    "    - y_trn: Dependent variable training set.\n",
    "    - X_vld: Predictors validation set.\n",
    "    - y_vld: Dependent variable validation set.\n",
    "\n",
    "    Output: The best Neural Network model.\n",
    "    \"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros is None:\n",
    "            mod = model(n_layers=param['n_layers'], loss=param['loss'], l1=param['l1'], \n",
    "                            learning_rate=param['learning_rate'], batch_size=param['batch_size'], \n",
    "                            epochs=param['epochs'], random_state=param['random_state'], \n",
    "                            batch_norm=param['batch_norm'], patience=param['patience'], \n",
    "                            verbose=param['verbose'], monitor=param['monitor'])\n",
    "            mod.fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            best_mod = mod\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {best_ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "        else:\n",
    "            mod = model(n_layers=param['n_layers'], loss=param['loss'], l1=param['l1'], \n",
    "                            learning_rate=param['learning_rate'], batch_size=param['batch_size'], \n",
    "                            epochs=param['epochs'], random_state=param['random_state'], \n",
    "                            batch_norm=param['batch_norm'], patience=param['patience'], \n",
    "                            verbose=param['verbose'], monitor=param['monitor'])\n",
    "            mod.fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_mod = mod\n",
    "                best_param = param\n",
    "    if illustration:\n",
    "        print('\\n'+'#'*60)\n",
    "        print('Tuning process finished!!!')\n",
    "        print(f'The best setting is: {best_param}')\n",
    "        print(f'with R2oos {best_ros*100:.2f}% on validation set.')\n",
    "        print('#'*60)\n",
    "    return best_mod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - OLS(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics of OLS-3\n",
    "OLS-3 includes size = 'mvel1', Book-to-Market = 'bm', momentum = 'mom1m','mom6m','mom12m','mom36m'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictors needed for the OLS-3 in a seperate file\n",
    "X_3pred = X[['mvel1', 'bm', 'mom1m', 'mom6m', 'mom12m', 'mom36m']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with expanding window and with and without Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_regression_OLS(Dependent, Predictors, stock_weights, loss = 'OLS', initial_train_years = 18, validation_years = 12, test_years = 1):\n",
    "    \"\"\"\n",
    "Function that runs OLS with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio.\n",
    "    - loss: Specify if you want to use 'OLS' loss, or 'Huber' loss\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store values.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list =[]\n",
    "    iterations_Roo2 = []\n",
    "\n",
    "    # List of 1957-2016\n",
    "    years = Dependent.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "\n",
    "    # Loop for expanding window.\n",
    "    for i in range(len(years) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = years[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "\n",
    "        # Specifies which loss function to use.    \n",
    "        if loss == 'OLS':\n",
    "            model = LinearRegression()\n",
    "        elif loss == 'Huber':\n",
    "            model = HuberRegressor(epsilon = 99.9) # Set the epsilon to 99.9%.\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function. Use OLS or Huber.\")\n",
    "        \n",
    "        # Training the model\n",
    "        OLS3 = model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict returns at stock level\n",
    "        r_stock_pred = OLS3.predict(X_test).reshape(-1)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "            \n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "    \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.001969218131502082"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUT-OF-SAMPLE R^2 for OLS-3\n",
    "OLS_Roos, OLS_iteravg_Roos = expanding_regression_OLS(y,X_3pred,stock_weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004975755819653926"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUT-OF-SAMPLE R^2 for OLS-3 with Huber Loss function\n",
    "OLS_3pred_Roos_H, OLS_iteravg_Roos = expanding_regression_OLS(y, X_3pred,stock_weights=weights, loss = 'Huber')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dimension Reduction: PCR and PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcr(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs PCR with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # Lists to save outcomes.\n",
    "    component_counts = []\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "\n",
    "    yrs = Dependent.index.year.unique() # List with all the years. (1957-2016)\n",
    "    start_gen = time.time()\n",
    "    n_components_list = range(1, 7) # To determine the number of components that are tested.\n",
    "    best_components = None # Initialize and later save the best amount of components.\n",
    "    best_r2 = -np.inf  # Initialize with negative infinity to find the maximum R-squared\n",
    "\n",
    "    # Expanding window.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # Testing the different components in PCA.\n",
    "        for n_component in n_components_list:\n",
    "\n",
    "            pca = PCA(n_components=n_component)\n",
    "            X_train_pca = pca.fit_transform(X_train) \n",
    "            X_val_pca = pca.transform(X_val)\n",
    "\n",
    "            # Fit Linear Regression on the training set\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_pca, y_train)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_val_pred = model.predict(X_val_pca)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "            # Update best components if the current number of components yields a higher R-squared\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_components = n_component\n",
    "\n",
    "        # Save the best number of components to the list\n",
    "        component_counts.append(best_components)\n",
    "\n",
    "        # Use the best number of components to fit the final model on the combined training and validation sets\n",
    "        best_pca = PCA(n_components=best_components)\n",
    "        X_train_pca = best_pca.fit_transform(X_train)\n",
    "        X_test_pca = best_pca.transform(X_test)\n",
    "        \n",
    "        # Best Model\n",
    "        PCASP500 = LinearRegression()\n",
    "        PCASP500.fit(X_train_pca, y_train)\n",
    "\n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = PCASP500.predict(X_test_pca)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Out Of Sample R_squared\n",
    "PCR_Roos, PCR_iteravg_Roos = pcr(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pls(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs PLS with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\" \n",
    "    # # Initalize to store r-squared for portfolio..\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list =[]\n",
    "    component_counts = [] # Initialize to save number of components.\n",
    "    iterations_Roo2 = []\n",
    "\n",
    "    years = Predictors.index.year.unique()#List of years (1957-2016)\n",
    "    start_gen = time.time()\n",
    "    n_components_list = range(1, 7) # To determine the number of components that are tested.\n",
    "    best_components = None # Initialize and later save the best amount of components.\n",
    "    best_r2 = -np.inf  # Initialize with negative infinity to find the maximum R-squared\n",
    "\n",
    "    for i in range(len(years) - initial_train_years - validation_years): \n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = years[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # Testing the different components in PLS.\n",
    "        for n_component in n_components_list:\n",
    "\n",
    "            # Train the model once on the training set\n",
    "            pls = PLSRegression(n_components=n_component)\n",
    "            pls.fit(X_train, y_train) \n",
    "\n",
    "           # Predict on the validation set\n",
    "            y_val_pred = pls.predict(X_val)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "            # Update best components if the current number of components yields a higher R-squared\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_components = n_component\n",
    "\n",
    "        # Save the best number of components to the list\n",
    "        component_counts.append(best_components)\n",
    "\n",
    "        # Use the best number of components to fit the final model \n",
    "        best_pls = PLSRegression(n_components=best_components)\n",
    "        best_pls.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the final model on the test set\n",
    "        r_stock_pred = best_pls.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs PLS and saves the results\n",
    "pls_Roos, pls_iteravg_Roos = pls(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Elastic Net & Lasso & Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net -> l1_ratio=0.5\n",
    "Ridge -> l1_ratio=0\n",
    "Lasso -> l1_ratio=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENet(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Elastic Net with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0.5], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        ENet_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = ENet_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Runs PLS and saves the results\n",
    "ENet_Roos, ENet_iteravg_Roos = ENet(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (not adjusted to newest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lasso(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Lasso Regression with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [1], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        LAS_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = LAS_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso results\n",
    "Lasso_score = Lasso(Dependent=y,Predictors=X,stock_weights=weights) \n",
    "Lasso_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge (not adjusted to newest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        RID_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = RID_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Ridge regression\n",
    "Ridge_score = Ridge(Dependent=y,Predictors=X,stock_weights=weights) \n",
    "Ridge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Huber-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber-Loss-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_val, y_pred, delta):\n",
    "    \"\"\"\n",
    "Function that ...\n",
    "Input: \n",
    "    - y_val: ...\n",
    "    - y_pred: ...\n",
    "    - delta: ...\n",
    "\n",
    "    Output: ...\n",
    "\"\"\"\n",
    "    error = y_val - y_pred\n",
    "    is_small_error = np.abs(error) <= delta\n",
    "    squared_loss = 0.5 * (error ** 2)\n",
    "    linear_loss = delta * (np.abs(error) - 0.5 * delta)\n",
    "    return np.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fun_with_huber(model, params: dict, X_trn, y_trn, X_vld, y_vld):\n",
    "    \"\"\"\n",
    "Function that ...\n",
    "Input: \n",
    "    - model: ...\n",
    "    - params: ...\n",
    "    - X_trn: ...\n",
    "    - y_trn: ...\n",
    "    - X_vld ...\n",
    "    - y_vld ...\n",
    "\n",
    "    Output: ...\n",
    "\"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            smallest_loss = huber(y_vld, y_pred, xi=.999)\n",
    "            best_param = param\n",
    "        else:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            loss = huber(y_vld, y_pred, xi=0.999)\n",
    "            if loss < smallest_loss:\n",
    "                smallest_loss = loss\n",
    "                best_param = param\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Huber loss with the accelarated proximal algorithm (APG), as explained in the appendix of the paper. \n",
    "The Accelerated Proximal Gradient Method should be iniciated with theta=0 however we didn't managed to make it work therefore we use a random state to set inicial values for theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mse(actual, predicted):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    return np.mean(resid**2)\n",
    "\n",
    "# huber objective function\n",
    "def huber(actual, predicted, xi):\n",
    "    actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "    resid = actual - predicted\n",
    "    huber_loss = np.where(np.abs(resid)<=xi, resid**2, 2*xi*np.abs(resid)-xi**2)\n",
    "    return np.mean(huber_loss)\n",
    "\n",
    "# gradient of mse\n",
    "def grad_mse(X, y, theta):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    return (X.T @ (y - X@theta))/N\n",
    "\n",
    "# gradient of huber loss\n",
    "def grad_huber(X, y, theta, xi):\n",
    "    K = X.shape[1]\n",
    "    X = np.array(X)\n",
    "    N = len(y)\n",
    "    y = np.array(y).reshape((N,1))\n",
    "    theta = np.array(theta).reshape((K,1))\n",
    "    resid = y - X@theta\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    try:\n",
    "        grad_m = X[ind_m].T @ (y[ind_m] - X[ind_m]@theta)\n",
    "    except:\n",
    "        grad_m = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_u = 2*xi* X[ind_u].T@np.ones((len(ind_u[0]),1))\n",
    "    except:\n",
    "        grad_u = np.zeros((K,1))\n",
    "    try:\n",
    "        grad_l = -2*xi* X[ind_l].T@np.ones((len(ind_l[0]),1))\n",
    "    except:\n",
    "        grad_l = np.zeros((K,1))\n",
    "    return (grad_m+grad_u+grad_l)/N\n",
    "\n",
    "# proximal operator\n",
    "def prox(theta,lmd,rho,gamma):\n",
    "    return (1/(1+lmd*gamma*rho))*softhred(theta,(1-rho)*gamma*lmd)\n",
    "\n",
    "# soft-thresholding operator\n",
    "def softhred(x,mu):\n",
    "    x = np.where(np.abs(x)<=mu, 0, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x>0), x-mu, x)\n",
    "    x = np.where((np.abs(x)>mu) & (x<0), x+mu, x)\n",
    "    return x\n",
    "\n",
    "# Elastic Net\n",
    "class ENet:\n",
    "    \n",
    "    def __init__(\n",
    "        self, lmd=1, rho=0.5, L=1, verbose=False,\n",
    "        xi=0.99, max_iter=3000, tol=1e-4, loss='huber', random_state=None\n",
    "    ):\n",
    "        self.lmd = lmd\n",
    "        self.rho = rho\n",
    "        self.L = L\n",
    "        self.verbose = verbose\n",
    "        self.xi = xi\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        if loss not in ['huber','mse']:\n",
    "            raise AttributeError(\n",
    "            f\"The loss should be either 'huber' or 'mse', but {loss} is given\"\n",
    "            )\n",
    "        else:\n",
    "            self.loss = loss\n",
    "            \n",
    "    def set_params(self, **params):\n",
    "        for param in params.keys():\n",
    "            setattr(self, param, params[param])\n",
    "        return self\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        K = X.shape[1]\n",
    "        X = np.array(X)\n",
    "        N = len(y)\n",
    "        y = np.array(y).reshape((N, 1))\n",
    "        gamma = self.L #gamma is the learning rate\n",
    "        # initialize theta\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            theta = np.random.uniform(size=(K, 1))\n",
    "        else:\n",
    "            theta = np.zeros((K, 1))\n",
    "\n",
    "        for m in range(self.max_iter):\n",
    "            theta_old = theta\n",
    "\n",
    "            if self.loss == 'mse':\n",
    "                theta_bar = theta - gamma * grad_mse(X, y, theta)\n",
    "            else:\n",
    "                theta_bar = theta - gamma * grad_huber(X, y, theta, self.xi)\n",
    "\n",
    "            theta_til = prox(theta_bar, self.lmd, self.rho, gamma)\n",
    "            theta = theta_til + m / (m + 3) * (theta_til - theta)\n",
    "            gamma = gamma\n",
    "\n",
    "            # Check for convergence and print progress\n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {m + 1}:\")\n",
    "                print(f\"Theta: {theta.T}\")\n",
    "                print(f\"Change in theta: {np.sum((theta - theta_old)**2)}\")\n",
    "                print(f\"---------------------------------------------\")\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.sum((theta - theta_old)**2) < np.sum(theta_old**2 * self.tol) or np.sum(np.abs(theta - theta_old)) == 0:\n",
    "                break\n",
    "\n",
    "        self.theta = theta_old\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return X@self.theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-Net with Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENet_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Elastic Net using Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "         \"lmd\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"rho\": [0.5], \"tol\":[1e-2],'L': np.linspace(1e-1, 1e-4, num=10),'random state':[12308]\n",
    "    \n",
    "    } \n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ENet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        ENetH_SP500 = ENet(lmd=best_par['lmd'], rho=best_par['rho'], L=best_par['L'],random_state=best_par['random state'],verbose=True).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = ENetH_SP500.predict(X_test).reshape(-1)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Theta: [[6.82546091e-01 2.04494329e-01 3.44599912e-01 5.75089776e-01\n",
      "  1.67771507e-01 3.75621073e-01 1.16328455e-01 8.83560494e-01\n",
      "  5.22015054e-01 3.47313854e-01 8.63858729e-01 1.62075518e-01\n",
      "  1.26195004e-01 5.95378915e-01 9.41455693e-01 7.64137992e-01\n",
      "  5.77468028e-01 7.73427596e-01 1.63585342e-01 7.90815616e-01\n",
      "  9.35403940e-02 2.68769743e-01 3.84650536e-01 4.97059792e-01\n",
      "  4.94962760e-01 6.36548022e-01 3.65330878e-01 2.17156231e-01\n",
      "  4.22662450e-01 8.48193642e-01 8.36608021e-01 9.86229025e-01\n",
      "  4.69561492e-01 5.17740754e-01 1.62318153e-02 8.68910836e-01\n",
      "  6.89840425e-01 6.26047179e-01 4.87901929e-01 5.22096114e-01\n",
      "  4.13577269e-01 4.41728401e-01 7.98409453e-01 4.11062525e-01\n",
      "  7.66457748e-01 2.83422652e-01 6.01795833e-01 7.80164019e-02\n",
      "  9.99825774e-01 2.20873515e-01 7.66491924e-01 6.18820540e-01\n",
      "  6.52093394e-01 9.73251904e-01 3.28878056e-01 2.16509981e-02\n",
      "  3.20318401e-01 1.12494254e-01 5.44214993e-02 9.88027558e-01\n",
      "  6.05529458e-02 8.64963412e-01 4.53045820e-02 4.81449571e-01\n",
      "  7.91181545e-01 3.88656426e-01 4.32152310e-01 7.85438160e-02\n",
      "  1.92364758e-01 9.14138154e-01 7.07698649e-01 5.38811130e-02\n",
      "  8.76066338e-01 5.85022917e-01 5.29671461e-01 9.12758537e-01\n",
      "  7.89495882e-01 6.30161494e-01 9.55809541e-01 4.74061916e-01\n",
      "  4.29723698e-01 9.38200552e-01 6.08011979e-01 7.48794778e-01\n",
      "  5.09935876e-01 8.82801343e-01 9.07707813e-01 5.37782142e-01\n",
      "  5.87220251e-01 1.60244926e-01 8.63023913e-01 9.98818640e-02\n",
      "  9.15896868e-01 1.18076585e-01 5.78989833e-01 3.95690176e-01\n",
      "  2.48592754e-01 9.28283813e-01 2.19416317e-01 6.00560758e-02\n",
      "  9.26494856e-01 7.02443410e-01 4.80468337e-01 4.96181817e-01\n",
      "  6.61564662e-03 1.45489719e-01 4.15090689e-01 3.64876925e-02\n",
      "  7.29412636e-01 5.36854353e-01 3.39475318e-01 6.57772218e-01\n",
      "  1.25001261e-01 4.34462781e-01 8.53639988e-02 9.71337480e-01\n",
      "  2.27598537e-01 7.88484427e-01 8.18903485e-01 6.22079891e-01\n",
      "  9.20584111e-01 3.11760891e-01 1.55983607e-01 9.24285120e-01\n",
      "  5.82759639e-01 9.49769676e-01 3.92711674e-02 7.11892838e-01\n",
      "  5.12321352e-01 7.59381328e-01 8.43571279e-01 1.22646258e-01\n",
      "  4.76970426e-01 4.99899189e-01 3.46092917e-01 6.76101689e-01\n",
      "  1.99198560e-01 3.46397634e-01 7.67261882e-01 6.21400687e-02\n",
      "  4.65639544e-01 1.35861723e-01 2.74845665e-02 8.14419820e-01\n",
      "  5.12259492e-01 2.89697040e-01 2.26138022e-01 8.59405091e-01\n",
      "  8.31745188e-01 7.21004764e-01 2.36510329e-01 3.34386153e-01\n",
      "  5.16304553e-03 2.74498872e-01 9.88101266e-02 8.88417703e-01\n",
      "  6.15280994e-01 3.40329263e-01 8.85004745e-01 4.82957745e-01\n",
      "  6.52114050e-01 5.35444301e-01 8.15407849e-01 4.69705600e-01\n",
      "  9.84646138e-01 4.40853632e-02 8.85699344e-01 6.94543027e-01\n",
      "  8.33622884e-01 4.62178180e-01 6.11399421e-01 2.26835002e-01\n",
      "  7.93856494e-01 2.29889005e-01 4.08234049e-01 4.87981553e-01\n",
      "  3.36755458e-01 3.71346231e-01 6.56541533e-03 3.56325380e-01\n",
      "  7.18367954e-01 1.38357382e-01 9.48240315e-01 1.24251559e-01\n",
      "  8.18070780e-01 7.58704448e-01 5.16462705e-01 3.77466829e-01\n",
      "  1.32088427e-01 8.55614775e-01 9.48033571e-01 2.25112458e-01\n",
      "  5.16172305e-01 7.49082555e-01 6.40082583e-01 8.66469665e-01\n",
      "  1.47610073e-01 7.42580219e-01 6.45715049e-01 7.57996656e-01\n",
      "  8.00769988e-01 2.21013711e-01 6.02025961e-03 6.27623286e-01\n",
      "  2.51490155e-01 7.70814072e-01 1.97869128e-01 9.04945692e-01\n",
      "  8.38333424e-01 4.17752936e-01 7.56689745e-01 5.77354545e-01\n",
      "  9.48693839e-01 6.54331650e-01 5.91180479e-01 9.88736773e-01\n",
      "  5.36334209e-01 9.00687072e-01 1.28340309e-01 3.92113853e-01\n",
      "  9.80273693e-01 1.78478094e-01 5.28744279e-01 2.79097429e-01\n",
      "  7.88958296e-04 4.24782955e-01 3.87194918e-01 8.86460469e-01\n",
      "  3.16522266e-01 2.40235268e-01 1.78124666e-01 5.45023171e-01\n",
      "  9.51792323e-02 1.16774620e-01 4.73073380e-01 2.60644139e-02\n",
      "  9.36566193e-01 4.60431472e-01 9.32256206e-01 8.83162912e-01\n",
      "  6.57670221e-02 3.07737521e-01 1.89220059e-01 8.87220895e-01\n",
      "  6.49836662e-01 2.15815052e-01 1.20406108e-01 8.95791528e-01\n",
      "  9.54998290e-01 5.13442683e-01 2.84466470e-01 2.51468798e-01\n",
      "  6.40138843e-01 7.51343353e-01 3.00886746e-01 5.88121976e-01\n",
      "  4.87393932e-01 8.58436077e-01 1.42257145e-01 7.98669359e-01\n",
      "  2.73825013e-01 3.83813112e-01 8.75970202e-01 5.68003505e-01\n",
      "  9.93517542e-01 2.36348757e-01 1.41560935e-01 1.24047101e-01\n",
      "  5.27885588e-01 3.77242702e-01 1.40296632e-02 5.92201145e-01\n",
      "  2.26205591e-01 2.20709957e-01 9.22136206e-01 3.00980732e-01\n",
      "  8.47012101e-01 4.54737486e-01 7.00648097e-01 4.32750756e-01\n",
      "  2.82756582e-01 1.21017406e-02 9.12941835e-01 9.16615408e-02\n",
      "  4.80160094e-01 7.40640523e-01 1.07360434e-01 5.77256162e-01\n",
      "  2.73524769e-01 8.93696734e-01 8.18026005e-01 1.64478287e-01\n",
      "  3.99011659e-01 2.98293851e-01 1.90600288e-02 3.37429469e-01\n",
      "  8.73736701e-01 5.34769089e-01 1.41067000e-02 1.63409949e-01\n",
      "  7.28844675e-01 6.17531446e-01 5.02920601e-01 4.73803714e-01\n",
      "  9.99507747e-01 4.57240515e-02 1.02430624e-03 8.25554906e-01\n",
      "  3.13195707e-01 2.34008955e-02 5.74171943e-01 3.39214785e-01\n",
      "  2.72242412e-01 6.21501757e-01 7.02675094e-01 1.13259248e-01\n",
      "  4.60222649e-01 1.75200979e-01 8.24596491e-01 7.05312559e-02\n",
      "  6.04545609e-01 8.05284794e-01 9.78867320e-01 5.73979645e-01\n",
      "  5.04703552e-01 2.05028179e-01 2.04838696e-01 9.94103816e-01\n",
      "  4.66774317e-02 9.70091363e-01 9.72976266e-01 1.93432057e-01\n",
      "  9.06003214e-01 1.03389492e-01 2.73891847e-01 1.36003819e-01\n",
      "  2.00696076e-01 5.62348707e-01 6.69006689e-01 8.94421441e-01\n",
      "  1.83262498e-01 6.23291578e-01 3.17568625e-01 6.02565507e-01\n",
      "  1.42312496e-01 4.40877229e-01 1.35359947e-01 4.46392385e-01\n",
      "  8.82966194e-01 7.66551095e-01 8.33528752e-01 6.51891790e-01\n",
      "  4.96767847e-01 1.96686364e-01 4.71291717e-01 8.09250557e-01\n",
      "  9.68928483e-01 1.04440930e-01 9.82480724e-02 5.16279300e-02\n",
      "  5.56970082e-01 3.29704951e-01 4.34705881e-01 7.55804357e-01\n",
      "  8.99310738e-01 7.09499130e-01 2.51611327e-01 7.89557658e-01\n",
      "  1.51646373e-01 2.22321493e-01 2.34403613e-01 5.26774897e-01\n",
      "  7.36691414e-01 4.32926760e-01 2.72321997e-02 7.41931437e-01\n",
      "  5.81463026e-01 6.78572784e-01 3.52199601e-01 3.95172118e-01\n",
      "  2.57994824e-01 9.22826176e-01 5.25816522e-01 8.64659369e-01\n",
      "  8.70013702e-02 6.75824531e-01 1.13490976e-01 8.76514348e-01\n",
      "  8.34063190e-01 2.01367794e-01 1.08197646e-01 4.38332118e-01\n",
      "  2.80570441e-01 4.26051062e-01 7.02946857e-01 6.32334829e-01\n",
      "  3.66913262e-01 3.11638831e-01 3.82048256e-01 7.29449930e-01\n",
      "  9.67013097e-03 6.19592535e-02 3.53962188e-01 2.69937046e-01\n",
      "  9.66121648e-01 7.30300551e-01 9.43719245e-01 7.51369881e-01\n",
      "  5.49203386e-01 5.10224851e-01 6.98448760e-01 5.46651076e-01\n",
      "  4.81214373e-01 5.08615789e-01 1.50927471e-01 3.67495551e-01\n",
      "  8.24270620e-01 9.09505326e-01 9.44356886e-02 8.25079874e-01\n",
      "  2.40368382e-01 9.05150459e-01 3.45875820e-01 2.32379562e-01\n",
      "  2.50689067e-01 4.80423014e-02 9.59448415e-01 1.43105851e-01\n",
      "  9.60542844e-01 6.71603631e-01 3.57114993e-01 9.39018882e-01\n",
      "  3.73563770e-01 2.37498039e-01 1.69418011e-01 7.89345852e-01\n",
      "  1.27655999e-01 6.36366221e-01 2.12369158e-01 7.46938762e-01\n",
      "  6.92325830e-01 9.55221456e-01 1.11630294e-01 5.90085245e-01\n",
      "  7.51634248e-02 6.89069516e-01 1.23302593e-01 8.00777876e-01\n",
      "  4.69678404e-01 3.68725215e-01 5.12998233e-01 9.94882774e-01\n",
      "  9.26140805e-01 5.79536398e-01 5.40828519e-01 9.24848165e-01\n",
      "  4.96906988e-01 3.09711114e-01 9.76897026e-01 8.82752836e-01\n",
      "  8.42183325e-01 5.27393859e-02 5.77213226e-01 9.42846138e-01\n",
      "  9.52934768e-01 8.25687425e-02 9.22179357e-01 9.68922741e-02\n",
      "  6.48927698e-01 9.51210990e-01 1.90804235e-01 5.94327566e-01\n",
      "  8.19238324e-01 8.25559323e-01 9.12792104e-01 7.80909034e-01\n",
      "  1.40295577e-01 3.04238347e-01 1.01040051e-01 2.08787450e-01\n",
      "  8.76132519e-01 5.92016329e-01 6.43525902e-02 6.71810016e-02\n",
      "  8.62833495e-02 2.24976405e-01 6.52580447e-01 6.14405771e-01\n",
      "  8.17639458e-01 4.13445010e-01 5.53434249e-01 9.86131416e-01\n",
      "  9.98177329e-01 7.21707158e-01 6.89853099e-01 1.79559684e-01\n",
      "  3.67386767e-01 5.29374125e-01 9.16674783e-01 2.23330880e-01\n",
      "  1.03049570e-01 7.59457253e-01 5.64269286e-01 4.23030725e-01\n",
      "  6.15063303e-01 2.35885647e-01 8.00307466e-01 9.25078101e-01\n",
      "  8.50836346e-01 5.93741470e-01 1.46178650e-02 9.47486824e-01\n",
      "  1.23513471e-01 6.62978331e-01 5.01363768e-01 1.08894649e-01\n",
      "  1.02213109e-01 7.68023606e-01 4.78157135e-01 7.49245626e-02\n",
      "  6.96260981e-01 8.01770447e-02 4.29890393e-01 5.64878396e-01\n",
      "  3.91912247e-01 5.87232618e-01 4.26936857e-01 4.31581948e-01\n",
      "  6.36550815e-02 3.29808447e-01 1.59495309e-01 7.22752088e-01\n",
      "  1.45291374e-01 3.38361736e-01 8.64565070e-01 7.93349500e-02\n",
      "  8.82619869e-01 1.26532361e-01 4.54593183e-01 7.47045649e-01\n",
      "  8.38620627e-01 7.88212708e-01 4.87953413e-01 1.02005956e-01\n",
      "  7.53796609e-01 8.80475488e-01 5.26639800e-01 1.70804055e-01\n",
      "  4.59551032e-01 9.68463104e-01 3.58453386e-01 5.99266874e-01\n",
      "  9.17952844e-01 7.24689070e-01 1.48512216e-01 5.61800083e-01\n",
      "  8.79315290e-01 3.57871361e-01 9.23114036e-01 3.06248462e-01\n",
      "  1.99867359e-01 5.77173786e-01 5.64148733e-01 6.50648126e-01\n",
      "  8.98995834e-01 9.87188580e-01 1.40534860e-01 4.20641084e-01\n",
      "  8.65542485e-01 4.93299472e-01 8.35765225e-02 7.62206656e-01\n",
      "  8.09666898e-01 7.33123508e-01 8.53749307e-01 7.27762454e-01\n",
      "  7.21568247e-01 5.75447592e-01 1.85606524e-01 7.60913054e-01\n",
      "  8.68960311e-01 9.52498523e-01 1.98931917e-01 7.96110360e-01\n",
      "  7.70561382e-02 9.90853972e-01 1.97807653e-01 4.74168902e-02\n",
      "  4.47442255e-01 7.70357852e-01 3.76032255e-01 8.45104104e-01\n",
      "  3.03871688e-01 2.27190126e-01 7.00114516e-02 3.80327246e-01\n",
      "  7.32869058e-02 7.45871467e-01 9.68548519e-01 1.31793565e-01\n",
      "  1.18275316e-03 3.14175839e-01 4.05147131e-01 8.80271257e-01\n",
      "  6.47511382e-01 1.54598003e-01 1.79589439e-01 4.07170892e-01\n",
      "  6.54951513e-01 5.78985186e-01 2.89448556e-01 9.09049543e-01\n",
      "  9.27767421e-01 9.15086341e-01 1.25416169e-01 1.68061180e-01\n",
      "  5.24852194e-01 6.45049500e-01 5.73956355e-02 8.80075491e-01\n",
      "  4.66082403e-01 4.75766747e-01 9.24919544e-01 5.30023665e-01\n",
      "  3.95242915e-01 7.50690187e-01 2.51684802e-01 9.60960329e-01\n",
      "  6.68670964e-01 8.39493491e-01 9.94613784e-01 7.65481126e-01\n",
      "  5.10761165e-01 3.75936347e-01 4.98971402e-01 5.76999667e-02\n",
      "  7.28704719e-01 7.74299986e-01 7.94824236e-01 4.15549492e-01\n",
      "  4.42357747e-01 2.55184676e-01 6.40334632e-01 3.30201473e-01\n",
      "  1.65375878e-01 3.51058666e-01 6.43706115e-01 4.72063173e-01\n",
      "  2.43983234e-01 4.43172872e-01 5.10116780e-01 4.09434781e-01\n",
      "  2.07530470e-01 8.90938608e-01 9.23877000e-01 6.80887546e-01\n",
      "  1.56192767e-01 3.89093338e-01 7.27493337e-01 6.75569770e-01\n",
      "  8.96672019e-01 2.88782475e-01 5.00021151e-01 7.30091125e-01\n",
      "  1.90227200e-01 5.63479915e-01 2.45751245e-01 3.42261410e-01\n",
      "  4.26147932e-01 8.07050950e-01 4.93428709e-01 8.56355395e-02\n",
      "  7.99042358e-01 3.11778925e-01 7.64784713e-01 7.40946923e-01\n",
      "  4.86162327e-01 1.69074011e-01 4.74817638e-01 3.51375361e-01\n",
      "  9.34592520e-01 9.79388631e-01 3.23939183e-01 4.71688247e-01\n",
      "  6.93078234e-02 9.35364866e-01 5.14878676e-01 1.31673201e-01\n",
      "  2.70912073e-01 1.35200672e-01 9.95808793e-01 1.72676476e-01\n",
      "  4.46653048e-01 4.99640314e-01 8.38684409e-01 2.13968211e-01\n",
      "  9.03449809e-01 9.88357560e-01 8.00210334e-01 5.48031924e-01\n",
      "  5.23996147e-01 5.01565677e-01 1.10453124e-01 8.55194746e-01\n",
      "  8.68547071e-01 9.84704806e-01 7.90487015e-01 5.71441206e-01\n",
      "  2.88308298e-01 9.33072522e-01 8.21206814e-01 6.92260130e-01\n",
      "  2.08944998e-01 5.16243471e-01 7.85966362e-02 2.31554915e-01\n",
      "  9.92575109e-01 6.36619422e-01 2.47800250e-01 8.49959555e-01\n",
      "  3.60284099e-01 1.71858463e-01 1.25612907e-01 4.33493877e-01\n",
      "  3.68674406e-01 9.46457345e-01 7.36762023e-01 3.60574799e-01\n",
      "  8.07330026e-01 3.95619335e-01 4.06474487e-01 7.38361038e-01\n",
      "  8.54065333e-01 6.67317340e-01 7.23870512e-01 1.74502498e-01\n",
      "  9.23180587e-02 8.46725646e-01 3.91457654e-01 8.22133911e-01\n",
      "  2.63911095e-01 4.67571368e-01 2.50107425e-01 5.23038466e-01\n",
      "  2.45248121e-01 5.58040572e-01 1.28862637e-01 2.39066781e-02\n",
      "  3.29868541e-01 8.42621934e-01 4.12433360e-01 7.62052785e-01\n",
      "  1.83365862e-01 3.04300589e-01 5.46328888e-01 7.63128886e-01\n",
      "  8.09604116e-01 2.56430274e-01 9.98495441e-01 7.98028660e-02\n",
      "  7.79680888e-01 8.78977478e-01 6.58025808e-01 9.70711495e-01\n",
      "  1.45832505e-01 9.45126202e-01 5.79244833e-02 4.42603337e-01\n",
      "  7.66979347e-01 7.57105421e-02 1.64832446e-01 6.43907492e-01\n",
      "  5.57326924e-01 8.70959131e-01 2.54154647e-01 3.17291098e-02\n",
      "  3.30269548e-01 4.70472605e-02 8.03704926e-01 2.99250936e-01\n",
      "  7.07802118e-01 3.52608727e-01 3.39132393e-01 2.17969016e-01\n",
      "  3.35969217e-01 1.38833712e-02 7.24088824e-01 8.14926899e-01\n",
      "  2.41657395e-01 1.75006767e-01 2.95774427e-02 9.32496215e-01\n",
      "  4.65646136e-01 9.79143188e-01 8.04175042e-01 1.61041485e-01\n",
      "  3.98120330e-01 1.59906666e-01 4.77502052e-01 2.23804603e-02\n",
      "  7.50446093e-01 8.18703831e-01 8.06190299e-01 5.60788103e-01\n",
      "  9.13293882e-01 8.93183704e-01 2.04359277e-01 3.99947432e-01\n",
      "  6.82185322e-01 4.78395003e-01 2.04195939e-01 8.36628118e-01\n",
      "  7.14345637e-01 2.42722182e-01 6.69369487e-01 4.88074914e-01\n",
      "  9.52245360e-01 2.82833001e-01 9.38221126e-01 7.98656662e-01\n",
      "  8.46946463e-01 8.50845099e-01 2.84086345e-03 2.91979189e-01\n",
      "  7.72421965e-01 7.34933299e-01 4.99500537e-01 9.60698765e-01\n",
      "  2.69270991e-01 7.67158745e-02 8.13067373e-01 9.14824231e-01\n",
      "  8.52954420e-01 1.87436158e-01 9.12932092e-01 8.93443944e-01\n",
      "  1.53578158e-01 8.52511553e-01 4.12764988e-01 6.47735345e-01\n",
      "  4.37643140e-01 6.53439985e-01 8.43405904e-01 2.65381681e-01\n",
      "  2.97407678e-01 7.01250267e-01 6.26192975e-01 1.01201253e-01\n",
      "  2.28336828e-01 5.24506118e-01 6.52681106e-01 7.56401610e-01\n",
      "  6.24604849e-01 4.88456311e-01 7.96402926e-01 9.54895061e-01\n",
      "  3.26695734e-01 2.17731280e-01 8.58635722e-01 2.18599412e-01\n",
      "  9.66904421e-01 8.53091385e-01 3.38944189e-01 7.39839541e-01\n",
      "  9.03334989e-01 7.61464154e-02 9.34524308e-01 5.81047338e-02\n",
      "  5.65178287e-01 5.48694886e-01 2.58686222e-01 4.96627350e-01\n",
      "  9.55168550e-01 6.37749910e-01 4.51701547e-01 8.25696118e-01\n",
      "  6.62365651e-01 7.54686753e-01 8.38804100e-01 4.09187642e-01\n",
      "  6.80204384e-01 2.08220221e-01 7.07641616e-02 2.82812755e-01\n",
      "  3.36587140e-01 5.06677749e-01 2.40664494e-01 9.35593847e-01\n",
      "  3.39616718e-02 4.14570834e-01 6.14970064e-01 5.42592111e-01\n",
      "  7.55437712e-01 2.23736024e-01 4.93254759e-01 9.54474641e-01\n",
      "  8.64222523e-02 6.86272182e-01 7.55450765e-01 1.94813443e-01\n",
      "  2.51671831e-01 2.73412999e-01 4.60005578e-01 3.83404112e-01\n",
      "  5.09543318e-01 6.18519333e-01 8.51075064e-01 2.35285517e-01\n",
      "  4.01849681e-01 6.53845141e-01 6.34504303e-01]]\n",
      "Change in theta: 2.7014957543010095e-07\n",
      "---------------------------------------------\n",
      "Iteration number 1 finished, Running time 253.37209296226501, Roo2 iteration 1 = -1371817.2993387945\n",
      "Iteration 1:\n",
      "Theta: [[6.82548965e-01 2.04497202e-01 3.44602785e-01 5.75092650e-01\n",
      "  1.67774381e-01 3.75623947e-01 1.16331329e-01 8.83563367e-01\n",
      "  5.22017927e-01 3.47316728e-01 8.63861602e-01 1.62078392e-01\n",
      "  1.26197878e-01 5.95381789e-01 9.41458566e-01 7.64140865e-01\n",
      "  5.77470902e-01 7.73430469e-01 1.63588215e-01 7.90818490e-01\n",
      "  9.35432675e-02 2.68772617e-01 3.84653409e-01 4.97062666e-01\n",
      "  4.94965634e-01 6.36550896e-01 3.65333751e-01 2.17159105e-01\n",
      "  4.22665323e-01 8.48196515e-01 8.36610894e-01 9.86231899e-01\n",
      "  4.69564366e-01 5.17743628e-01 1.62346888e-02 8.68913709e-01\n",
      "  6.89843298e-01 6.26050052e-01 4.87904802e-01 5.22098987e-01\n",
      "  4.13580143e-01 4.41731275e-01 7.98412326e-01 4.11065398e-01\n",
      "  7.66460621e-01 2.83425525e-01 6.01798707e-01 7.80192754e-02\n",
      "  9.99828648e-01 2.20876388e-01 7.66494798e-01 6.18823414e-01\n",
      "  6.52096268e-01 9.73254778e-01 3.28880930e-01 2.16538716e-02\n",
      "  3.20321274e-01 1.12497128e-01 5.44243728e-02 9.88030431e-01\n",
      "  6.05558193e-02 8.64966286e-01 4.53074554e-02 4.81452445e-01\n",
      "  7.91184418e-01 3.88659300e-01 4.32155183e-01 7.85466895e-02\n",
      "  1.92367632e-01 9.14141027e-01 7.07701523e-01 5.38839864e-02\n",
      "  8.76069211e-01 5.85025791e-01 5.29674335e-01 9.12761411e-01\n",
      "  7.89498755e-01 6.30164368e-01 9.55812415e-01 4.74064789e-01\n",
      "  4.29726571e-01 9.38203425e-01 6.08014852e-01 7.48797652e-01\n",
      "  5.09938749e-01 8.82804217e-01 9.07710687e-01 5.37785016e-01\n",
      "  5.87223124e-01 1.60247799e-01 8.63026786e-01 9.98847375e-02\n",
      "  9.15899741e-01 1.18079459e-01 5.78992706e-01 3.95693050e-01\n",
      "  2.48595628e-01 9.28286686e-01 2.19419190e-01 6.00589493e-02\n",
      "  9.26497729e-01 7.02446283e-01 4.80471211e-01 4.96184690e-01\n",
      "  6.61852011e-03 1.45492593e-01 4.15093562e-01 3.64905660e-02\n",
      "  7.29415509e-01 5.36857226e-01 3.39478191e-01 6.57775092e-01\n",
      "  1.25004135e-01 4.34465655e-01 8.53668722e-02 9.71340353e-01\n",
      "  2.27601410e-01 7.88487301e-01 8.18906359e-01 6.22082764e-01\n",
      "  9.20586985e-01 3.11763765e-01 1.55986480e-01 9.24287994e-01\n",
      "  5.82762512e-01 9.49772549e-01 3.92740409e-02 7.11895712e-01\n",
      "  5.12324226e-01 7.59384202e-01 8.43574152e-01 1.22649132e-01\n",
      "  4.76973299e-01 4.99902063e-01 3.46095790e-01 6.76104562e-01\n",
      "  1.99201434e-01 3.46400507e-01 7.67264756e-01 6.21429422e-02\n",
      "  4.65642418e-01 1.35864596e-01 2.74874400e-02 8.14422693e-01\n",
      "  5.12262365e-01 2.89699913e-01 2.26140896e-01 8.59407965e-01\n",
      "  8.31748062e-01 7.21007637e-01 2.36513202e-01 3.34389027e-01\n",
      "  5.16591901e-03 2.74501746e-01 9.88130001e-02 8.88420576e-01\n",
      "  6.15283867e-01 3.40332136e-01 8.85007618e-01 4.82960618e-01\n",
      "  6.52116923e-01 5.35447174e-01 8.15410722e-01 4.69708474e-01\n",
      "  9.84649011e-01 4.40882367e-02 8.85702217e-01 6.94545901e-01\n",
      "  8.33625758e-01 4.62181054e-01 6.11402294e-01 2.26837875e-01\n",
      "  7.93859368e-01 2.29891879e-01 4.08236922e-01 4.87984427e-01\n",
      "  3.36758332e-01 3.71349104e-01 6.56828881e-03 3.56328253e-01\n",
      "  7.18370827e-01 1.38360256e-01 9.48243188e-01 1.24254432e-01\n",
      "  8.18073654e-01 7.58707321e-01 5.16465579e-01 3.77469702e-01\n",
      "  1.32091301e-01 8.55617648e-01 9.48036445e-01 2.25115331e-01\n",
      "  5.16175178e-01 7.49085429e-01 6.40085457e-01 8.66472539e-01\n",
      "  1.47612947e-01 7.42583093e-01 6.45717922e-01 7.57999529e-01\n",
      "  8.00772862e-01 2.21016584e-01 6.02313309e-03 6.27626160e-01\n",
      "  2.51493028e-01 7.70816945e-01 1.97872002e-01 9.04948566e-01\n",
      "  8.38336297e-01 4.17755810e-01 7.56692618e-01 5.77357419e-01\n",
      "  9.48696713e-01 6.54334524e-01 5.91183353e-01 9.88739646e-01\n",
      "  5.36337083e-01 9.00689945e-01 1.28343182e-01 3.92116727e-01\n",
      "  9.80276567e-01 1.78480967e-01 5.28747152e-01 2.79100302e-01\n",
      "  7.91831781e-04 4.24785829e-01 3.87197792e-01 8.86463343e-01\n",
      "  3.16525139e-01 2.40238141e-01 1.78127540e-01 5.45026045e-01\n",
      "  9.51821058e-02 1.16777494e-01 4.73076254e-01 2.60672874e-02\n",
      "  9.36569066e-01 4.60434345e-01 9.32259079e-01 8.83165785e-01\n",
      "  6.57698956e-02 3.07740395e-01 1.89222932e-01 8.87223768e-01\n",
      "  6.49839535e-01 2.15817925e-01 1.20408981e-01 8.95794401e-01\n",
      "  9.55001164e-01 5.13445556e-01 2.84469343e-01 2.51471672e-01\n",
      "  6.40141717e-01 7.51346226e-01 3.00889620e-01 5.88124850e-01\n",
      "  4.87396806e-01 8.58438950e-01 1.42260019e-01 7.98672232e-01\n",
      "  2.73827886e-01 3.83815986e-01 8.75973075e-01 5.68006378e-01\n",
      "  9.93520415e-01 2.36351630e-01 1.41563809e-01 1.24049975e-01\n",
      "  5.27888462e-01 3.77245576e-01 1.40325367e-02 5.92204018e-01\n",
      "  2.26208464e-01 2.20712831e-01 9.22139080e-01 3.00983606e-01\n",
      "  8.47014974e-01 4.54740359e-01 7.00650971e-01 4.32753629e-01\n",
      "  2.82759455e-01 1.21046141e-02 9.12944709e-01 9.16644143e-02\n",
      "  4.80162967e-01 7.40643396e-01 1.07363308e-01 5.77259035e-01\n",
      "  2.73527642e-01 8.93699607e-01 8.18028878e-01 1.64481161e-01\n",
      "  3.99014532e-01 2.98296725e-01 1.90629023e-02 3.37432342e-01\n",
      "  8.73739574e-01 5.34771963e-01 1.41095734e-02 1.63412822e-01\n",
      "  7.28847549e-01 6.17534320e-01 5.02923474e-01 4.73806587e-01\n",
      "  9.99510621e-01 4.57269250e-02 1.02717973e-03 8.25557780e-01\n",
      "  3.13198580e-01 2.34037690e-02 5.74174816e-01 3.39217658e-01\n",
      "  2.72245285e-01 6.21504631e-01 7.02677968e-01 1.13262122e-01\n",
      "  4.60225523e-01 1.75203853e-01 8.24599365e-01 7.05341294e-02\n",
      "  6.04548482e-01 8.05287667e-01 9.78870194e-01 5.73982519e-01\n",
      "  5.04706425e-01 2.05031053e-01 2.04841569e-01 9.94106689e-01\n",
      "  4.66803052e-02 9.70094236e-01 9.72979139e-01 1.93434931e-01\n",
      "  9.06006088e-01 1.03392365e-01 2.73894720e-01 1.36006693e-01\n",
      "  2.00698949e-01 5.62351581e-01 6.69009563e-01 8.94424314e-01\n",
      "  1.83265371e-01 6.23294451e-01 3.17571499e-01 6.02568381e-01\n",
      "  1.42315369e-01 4.40880103e-01 1.35362821e-01 4.46395258e-01\n",
      "  8.82969068e-01 7.66553968e-01 8.33531626e-01 6.51894664e-01\n",
      "  4.96770720e-01 1.96689238e-01 4.71294590e-01 8.09253431e-01\n",
      "  9.68931356e-01 1.04443803e-01 9.82509458e-02 5.16308035e-02\n",
      "  5.56972955e-01 3.29707824e-01 4.34708754e-01 7.55807230e-01\n",
      "  8.99313612e-01 7.09502003e-01 2.51614200e-01 7.89560531e-01\n",
      "  1.51649246e-01 2.22324366e-01 2.34406487e-01 5.26777770e-01\n",
      "  7.36694288e-01 4.32929633e-01 2.72350732e-02 7.41934311e-01\n",
      "  5.81465899e-01 6.78575658e-01 3.52202474e-01 3.95174992e-01\n",
      "  2.57997698e-01 9.22829050e-01 5.25819395e-01 8.64662242e-01\n",
      "  8.70042437e-02 6.75827404e-01 1.13493849e-01 8.76517221e-01\n",
      "  8.34066064e-01 2.01370667e-01 1.08200520e-01 4.38334992e-01\n",
      "  2.80573315e-01 4.26053936e-01 7.02949731e-01 6.32337702e-01\n",
      "  3.66916135e-01 3.11641704e-01 3.82051130e-01 7.29452803e-01\n",
      "  9.67300445e-03 6.19621270e-02 3.53965061e-01 2.69939919e-01\n",
      "  9.66124521e-01 7.30303424e-01 9.43722119e-01 7.51372755e-01\n",
      "  5.49206260e-01 5.10227724e-01 6.98451634e-01 5.46653950e-01\n",
      "  4.81217246e-01 5.08618662e-01 1.50930345e-01 3.67498425e-01\n",
      "  8.24273493e-01 9.09508200e-01 9.44385621e-02 8.25082747e-01\n",
      "  2.40371255e-01 9.05153332e-01 3.45878693e-01 2.32382435e-01\n",
      "  2.50691941e-01 4.80451749e-02 9.59451289e-01 1.43108724e-01\n",
      "  9.60545717e-01 6.71606505e-01 3.57117866e-01 9.39021755e-01\n",
      "  3.73566644e-01 2.37500912e-01 1.69420885e-01 7.89348725e-01\n",
      "  1.27658872e-01 6.36369095e-01 2.12372031e-01 7.46941635e-01\n",
      "  6.92328704e-01 9.55224330e-01 1.11633167e-01 5.90088119e-01\n",
      "  7.51662983e-02 6.89072390e-01 1.23305467e-01 8.00780749e-01\n",
      "  4.69681277e-01 3.68728089e-01 5.13001106e-01 9.94885648e-01\n",
      "  9.26143679e-01 5.79539271e-01 5.40831393e-01 9.24851038e-01\n",
      "  4.96909862e-01 3.09713988e-01 9.76899900e-01 8.82755710e-01\n",
      "  8.42186198e-01 5.27422594e-02 5.77216100e-01 9.42849012e-01\n",
      "  9.52937641e-01 8.25716159e-02 9.22182230e-01 9.68951476e-02\n",
      "  6.48930571e-01 9.51213863e-01 1.90807109e-01 5.94330439e-01\n",
      "  8.19241198e-01 8.25562197e-01 9.12794978e-01 7.80911907e-01\n",
      "  1.40298451e-01 3.04241220e-01 1.01042925e-01 2.08790323e-01\n",
      "  8.76135393e-01 5.92019202e-01 6.43554636e-02 6.71838751e-02\n",
      "  8.62862230e-02 2.24979278e-01 6.52583320e-01 6.14408645e-01\n",
      "  8.17642332e-01 4.13447884e-01 5.53437123e-01 9.86134289e-01\n",
      "  9.98180202e-01 7.21710032e-01 6.89855972e-01 1.79562557e-01\n",
      "  3.67389640e-01 5.29376998e-01 9.16677656e-01 2.23333753e-01\n",
      "  1.03052444e-01 7.59460126e-01 5.64272160e-01 4.23033598e-01\n",
      "  6.15066176e-01 2.35888520e-01 8.00310339e-01 9.25080975e-01\n",
      "  8.50839220e-01 5.93744344e-01 1.46207384e-02 9.47489697e-01\n",
      "  1.23516344e-01 6.62981204e-01 5.01366642e-01 1.08897523e-01\n",
      "  1.02215982e-01 7.68026480e-01 4.78160008e-01 7.49274361e-02\n",
      "  6.96263854e-01 8.01799182e-02 4.29893266e-01 5.64881270e-01\n",
      "  3.91915120e-01 5.87235491e-01 4.26939731e-01 4.31584822e-01\n",
      "  6.36579550e-02 3.29811320e-01 1.59498182e-01 7.22754961e-01\n",
      "  1.45294247e-01 3.38364610e-01 8.64567944e-01 7.93378235e-02\n",
      "  8.82622743e-01 1.26535234e-01 4.54596056e-01 7.47048523e-01\n",
      "  8.38623500e-01 7.88215581e-01 4.87956286e-01 1.02008829e-01\n",
      "  7.53799483e-01 8.80478362e-01 5.26642673e-01 1.70806929e-01\n",
      "  4.59553905e-01 9.68465978e-01 3.58456259e-01 5.99269748e-01\n",
      "  9.17955718e-01 7.24691944e-01 1.48515090e-01 5.61802956e-01\n",
      "  8.79318164e-01 3.57874235e-01 9.23116909e-01 3.06251336e-01\n",
      "  1.99870233e-01 5.77176660e-01 5.64151606e-01 6.50650999e-01\n",
      "  8.98998707e-01 9.87191453e-01 1.40537733e-01 4.20643957e-01\n",
      "  8.65545358e-01 4.93302345e-01 8.35793960e-02 7.62209530e-01\n",
      "  8.09669771e-01 7.33126382e-01 8.53752181e-01 7.27765327e-01\n",
      "  7.21571121e-01 5.75450465e-01 1.85609397e-01 7.60915928e-01\n",
      "  8.68963185e-01 9.52501396e-01 1.98934790e-01 7.96113234e-01\n",
      "  7.70590116e-02 9.90856846e-01 1.97810526e-01 4.74197637e-02\n",
      "  4.47445128e-01 7.70360725e-01 3.76035129e-01 8.45106978e-01\n",
      "  3.03874561e-01 2.27193000e-01 7.00143251e-02 3.80330119e-01\n",
      "  7.32897792e-02 7.45874340e-01 9.68551392e-01 1.31796439e-01\n",
      "  1.18562665e-03 3.14178713e-01 4.05150004e-01 8.80274130e-01\n",
      "  6.47514255e-01 1.54600877e-01 1.79592313e-01 4.07173765e-01\n",
      "  6.54954387e-01 5.78988059e-01 2.89451429e-01 9.09052416e-01\n",
      "  9.27770295e-01 9.15089214e-01 1.25419042e-01 1.68064054e-01\n",
      "  5.24855068e-01 6.45052374e-01 5.73985089e-02 8.80078365e-01\n",
      "  4.66085276e-01 4.75769621e-01 9.24922417e-01 5.30026539e-01\n",
      "  3.95245789e-01 7.50693061e-01 2.51687675e-01 9.60963202e-01\n",
      "  6.68673837e-01 8.39496365e-01 9.94616657e-01 7.65484000e-01\n",
      "  5.10764038e-01 3.75939220e-01 4.98974275e-01 5.77028401e-02\n",
      "  7.28707592e-01 7.74302859e-01 7.94827110e-01 4.15552365e-01\n",
      "  4.42360621e-01 2.55187550e-01 6.40337505e-01 3.30204347e-01\n",
      "  1.65378751e-01 3.51061540e-01 6.43708989e-01 4.72066046e-01\n",
      "  2.43986108e-01 4.43175746e-01 5.10119654e-01 4.09437655e-01\n",
      "  2.07533344e-01 8.90941481e-01 9.23879873e-01 6.80890420e-01\n",
      "  1.56195641e-01 3.89096211e-01 7.27496211e-01 6.75572644e-01\n",
      "  8.96674893e-01 2.88785348e-01 5.00024025e-01 7.30093999e-01\n",
      "  1.90230073e-01 5.63482788e-01 2.45754119e-01 3.42264283e-01\n",
      "  4.26150806e-01 8.07053824e-01 4.93431582e-01 8.56384129e-02\n",
      "  7.99045232e-01 3.11781799e-01 7.64787587e-01 7.40949796e-01\n",
      "  4.86165201e-01 1.69076885e-01 4.74820512e-01 3.51378235e-01\n",
      "  9.34595393e-01 9.79391505e-01 3.23942057e-01 4.71691121e-01\n",
      "  6.93106969e-02 9.35367739e-01 5.14881549e-01 1.31676075e-01\n",
      "  2.70914947e-01 1.35203545e-01 9.95811666e-01 1.72679350e-01\n",
      "  4.46655922e-01 4.99643188e-01 8.38687283e-01 2.13971084e-01\n",
      "  9.03452682e-01 9.88360434e-01 8.00213207e-01 5.48034797e-01\n",
      "  5.23999020e-01 5.01568551e-01 1.10455998e-01 8.55197620e-01\n",
      "  8.68549944e-01 9.84707679e-01 7.90489889e-01 5.71444080e-01\n",
      "  2.88311172e-01 9.33075395e-01 8.21209688e-01 6.92263004e-01\n",
      "  2.08947872e-01 5.16246344e-01 7.85995097e-02 2.31557789e-01\n",
      "  9.92577982e-01 6.36622295e-01 2.47803123e-01 8.49962428e-01\n",
      "  3.60286973e-01 1.71861337e-01 1.25615780e-01 4.33496751e-01\n",
      "  3.68677280e-01 9.46460219e-01 7.36764896e-01 3.60577672e-01\n",
      "  8.07332900e-01 3.95622209e-01 4.06477360e-01 7.38363912e-01\n",
      "  8.54068207e-01 6.67320214e-01 7.23873385e-01 1.74505371e-01\n",
      "  9.23209322e-02 8.46728519e-01 3.91460527e-01 8.22136784e-01\n",
      "  2.63913968e-01 4.67574242e-01 2.50110298e-01 5.23041339e-01\n",
      "  2.45250995e-01 5.58043446e-01 1.28865511e-01 2.39095515e-02\n",
      "  3.29871414e-01 8.42624807e-01 4.12436234e-01 7.62055658e-01\n",
      "  1.83368735e-01 3.04303463e-01 5.46331762e-01 7.63131759e-01\n",
      "  8.09606990e-01 2.56433147e-01 9.98498315e-01 7.98057395e-02\n",
      "  7.79683761e-01 8.78980352e-01 6.58028682e-01 9.70714368e-01\n",
      "  1.45835379e-01 9.45129075e-01 5.79273568e-02 4.42606210e-01\n",
      "  7.66982220e-01 7.57134156e-02 1.64835319e-01 6.43910366e-01\n",
      "  5.57329798e-01 8.70962004e-01 2.54157521e-01 3.17319832e-02\n",
      "  3.30272422e-01 4.70501340e-02 8.03707800e-01 2.99253810e-01\n",
      "  7.07804991e-01 3.52611601e-01 3.39135267e-01 2.17971890e-01\n",
      "  3.35972090e-01 1.38862447e-02 7.24091697e-01 8.14929772e-01\n",
      "  2.41660268e-01 1.75009640e-01 2.95803162e-02 9.32499089e-01\n",
      "  4.65649009e-01 9.79146061e-01 8.04177915e-01 1.61044359e-01\n",
      "  3.98123204e-01 1.59909540e-01 4.77504926e-01 2.23833338e-02\n",
      "  7.50448967e-01 8.18706705e-01 8.06193172e-01 5.60790977e-01\n",
      "  9.13296756e-01 8.93186578e-01 2.04362151e-01 3.99950306e-01\n",
      "  6.82188195e-01 4.78397876e-01 2.04198812e-01 8.36630991e-01\n",
      "  7.14348510e-01 2.42725056e-01 6.69372361e-01 4.88077788e-01\n",
      "  9.52248233e-01 2.82835875e-01 9.38223999e-01 7.98659536e-01\n",
      "  8.46949336e-01 8.50847972e-01 2.84373694e-03 2.91982062e-01\n",
      "  7.72424838e-01 7.34936172e-01 4.99503410e-01 9.60701638e-01\n",
      "  2.69273865e-01 7.67187480e-02 8.13070246e-01 9.14827104e-01\n",
      "  8.52957294e-01 1.87439032e-01 9.12934965e-01 8.93446818e-01\n",
      "  1.53581032e-01 8.52514427e-01 4.12767862e-01 6.47738218e-01\n",
      "  4.37646014e-01 6.53442859e-01 8.43408777e-01 2.65384555e-01\n",
      "  2.97410552e-01 7.01253141e-01 6.26195849e-01 1.01204127e-01\n",
      "  2.28339701e-01 5.24508992e-01 6.52683979e-01 7.56404483e-01\n",
      "  6.24607723e-01 4.88459185e-01 7.96405799e-01 9.54897934e-01\n",
      "  3.26698608e-01 2.17734153e-01 8.58638596e-01 2.18602286e-01\n",
      "  9.66907295e-01 8.53094259e-01 3.38947062e-01 7.39842415e-01\n",
      "  9.03337863e-01 7.61492889e-02 9.34527181e-01 5.81076073e-02\n",
      "  5.65181161e-01 5.48697759e-01 2.58689095e-01 4.96630223e-01\n",
      "  9.55171424e-01 6.37752783e-01 4.51704421e-01 8.25698992e-01\n",
      "  6.62368524e-01 7.54689627e-01 8.38806974e-01 4.09190516e-01\n",
      "  6.80207257e-01 2.08223095e-01 7.07670350e-02 2.82815628e-01\n",
      "  3.36590014e-01 5.06680623e-01 2.40667367e-01 9.35596721e-01\n",
      "  3.39645453e-02 4.14573707e-01 6.14972938e-01 5.42594984e-01\n",
      "  7.55440585e-01 2.23738897e-01 4.93257632e-01 9.54477515e-01\n",
      "  8.64251258e-02 6.86275055e-01 7.55453639e-01 1.94816317e-01\n",
      "  2.51674705e-01 2.73415872e-01 4.60008452e-01 3.83406986e-01\n",
      "  5.09546192e-01 6.18522206e-01 8.51077937e-01 2.35288390e-01\n",
      "  4.01852554e-01 6.53848014e-01 6.34507177e-01]]\n",
      "Change in theta: 3.6782879456748947e-07\n",
      "---------------------------------------------\n",
      "Iteration number 2 finished, Running time 213.38142585754395, Roo2 iteration 2 = -3884634.670916148\n",
      "Iteration 1:\n",
      "Theta: [[6.82551713e-01 2.04499951e-01 3.44605534e-01 5.75095399e-01\n",
      "  1.67777129e-01 3.75626696e-01 1.16334078e-01 8.83566116e-01\n",
      "  5.22020676e-01 3.47319477e-01 8.63864351e-01 1.62081140e-01\n",
      "  1.26200626e-01 5.95384537e-01 9.41461315e-01 7.64143614e-01\n",
      "  5.77473650e-01 7.73433218e-01 1.63590964e-01 7.90821238e-01\n",
      "  9.35460163e-02 2.68775366e-01 3.84656158e-01 4.97065415e-01\n",
      "  4.94968382e-01 6.36553644e-01 3.65336500e-01 2.17161854e-01\n",
      "  4.22668072e-01 8.48199264e-01 8.36613643e-01 9.86234647e-01\n",
      "  4.69567115e-01 5.17746376e-01 1.62374376e-02 8.68916458e-01\n",
      "  6.89846047e-01 6.26052801e-01 4.87907551e-01 5.22101736e-01\n",
      "  4.13582892e-01 4.41734023e-01 7.98415075e-01 4.11068147e-01\n",
      "  7.66463370e-01 2.83428274e-01 6.01801456e-01 7.80220241e-02\n",
      "  9.99831396e-01 2.20879137e-01 7.66497547e-01 6.18826162e-01\n",
      "  6.52099017e-01 9.73257527e-01 3.28883679e-01 2.16566204e-02\n",
      "  3.20324023e-01 1.12499877e-01 5.44271216e-02 9.88033180e-01\n",
      "  6.05585681e-02 8.64969035e-01 4.53102042e-02 4.81455194e-01\n",
      "  7.91187167e-01 3.88662049e-01 4.32157932e-01 7.85494383e-02\n",
      "  1.92370381e-01 9.14143776e-01 7.07704272e-01 5.38867352e-02\n",
      "  8.76071960e-01 5.85028540e-01 5.29677083e-01 9.12764159e-01\n",
      "  7.89501504e-01 6.30167116e-01 9.55815164e-01 4.74067538e-01\n",
      "  4.29729320e-01 9.38206174e-01 6.08017601e-01 7.48800400e-01\n",
      "  5.09941498e-01 8.82806966e-01 9.07713436e-01 5.37787764e-01\n",
      "  5.87225873e-01 1.60250548e-01 8.63029535e-01 9.98874863e-02\n",
      "  9.15902490e-01 1.18082208e-01 5.78995455e-01 3.95695799e-01\n",
      "  2.48598377e-01 9.28289435e-01 2.19421939e-01 6.00616981e-02\n",
      "  9.26500478e-01 7.02449032e-01 4.80473959e-01 4.96187439e-01\n",
      "  6.62126889e-03 1.45495341e-01 4.15096311e-01 3.64933148e-02\n",
      "  7.29418258e-01 5.36859975e-01 3.39480940e-01 6.57777841e-01\n",
      "  1.25006884e-01 4.34468404e-01 8.53696210e-02 9.71343102e-01\n",
      "  2.27604159e-01 7.88490050e-01 8.18909107e-01 6.22085513e-01\n",
      "  9.20589733e-01 3.11766514e-01 1.55989229e-01 9.24290743e-01\n",
      "  5.82765261e-01 9.49775298e-01 3.92767897e-02 7.11898460e-01\n",
      "  5.12326975e-01 7.59386951e-01 8.43576901e-01 1.22651881e-01\n",
      "  4.76976048e-01 4.99904812e-01 3.46098539e-01 6.76107311e-01\n",
      "  1.99204182e-01 3.46403256e-01 7.67267505e-01 6.21456910e-02\n",
      "  4.65645167e-01 1.35867345e-01 2.74901887e-02 8.14425442e-01\n",
      "  5.12265114e-01 2.89702662e-01 2.26143645e-01 8.59410713e-01\n",
      "  8.31750810e-01 7.21010386e-01 2.36515951e-01 3.34391775e-01\n",
      "  5.16866780e-03 2.74504495e-01 9.88157489e-02 8.88423325e-01\n",
      "  6.15286616e-01 3.40334885e-01 8.85010367e-01 4.82963367e-01\n",
      "  6.52119672e-01 5.35449923e-01 8.15413471e-01 4.69711223e-01\n",
      "  9.84651760e-01 4.40909854e-02 8.85704966e-01 6.94548649e-01\n",
      "  8.33628507e-01 4.62183803e-01 6.11405043e-01 2.26840624e-01\n",
      "  7.93862116e-01 2.29894628e-01 4.08239671e-01 4.87987176e-01\n",
      "  3.36761080e-01 3.71351853e-01 6.57103760e-03 3.56331002e-01\n",
      "  7.18373576e-01 1.38363004e-01 9.48245937e-01 1.24257181e-01\n",
      "  8.18076403e-01 7.58710070e-01 5.16468327e-01 3.77472451e-01\n",
      "  1.32094049e-01 8.55620397e-01 9.48039194e-01 2.25118080e-01\n",
      "  5.16177927e-01 7.49088177e-01 6.40088205e-01 8.66475288e-01\n",
      "  1.47615696e-01 7.42585841e-01 6.45720671e-01 7.58002278e-01\n",
      "  8.00775610e-01 2.21019333e-01 6.02588188e-03 6.27628908e-01\n",
      "  2.51495777e-01 7.70819694e-01 1.97874751e-01 9.04951314e-01\n",
      "  8.38339046e-01 4.17758558e-01 7.56695367e-01 5.77360168e-01\n",
      "  9.48699462e-01 6.54337273e-01 5.91186101e-01 9.88742395e-01\n",
      "  5.36339831e-01 9.00692694e-01 1.28345931e-01 3.92119476e-01\n",
      "  9.80279315e-01 1.78483716e-01 5.28749901e-01 2.79103051e-01\n",
      "  7.94580566e-04 4.24788578e-01 3.87200540e-01 8.86466091e-01\n",
      "  3.16527888e-01 2.40240890e-01 1.78130288e-01 5.45028793e-01\n",
      "  9.51848546e-02 1.16780243e-01 4.73079002e-01 2.60700362e-02\n",
      "  9.36571815e-01 4.60437094e-01 9.32261828e-01 8.83168534e-01\n",
      "  6.57726444e-02 3.07743144e-01 1.89225681e-01 8.87226517e-01\n",
      "  6.49842284e-01 2.15820674e-01 1.20411730e-01 8.95797150e-01\n",
      "  9.55003913e-01 5.13448305e-01 2.84472092e-01 2.51474420e-01\n",
      "  6.40144465e-01 7.51348975e-01 3.00892369e-01 5.88127599e-01\n",
      "  4.87399555e-01 8.58441699e-01 1.42262767e-01 7.98674981e-01\n",
      "  2.73830635e-01 3.83818734e-01 8.75975824e-01 5.68009127e-01\n",
      "  9.93523164e-01 2.36354379e-01 1.41566558e-01 1.24052724e-01\n",
      "  5.27891211e-01 3.77248324e-01 1.40352855e-02 5.92206767e-01\n",
      "  2.26211213e-01 2.20715580e-01 9.22141829e-01 3.00986355e-01\n",
      "  8.47017723e-01 4.54743108e-01 7.00653719e-01 4.32756378e-01\n",
      "  2.82762204e-01 1.21073628e-02 9.12947458e-01 9.16671631e-02\n",
      "  4.80165716e-01 7.40646145e-01 1.07366056e-01 5.77261784e-01\n",
      "  2.73530391e-01 8.93702356e-01 8.18031627e-01 1.64483910e-01\n",
      "  3.99017281e-01 2.98299474e-01 1.90656511e-02 3.37435091e-01\n",
      "  8.73742323e-01 5.34774711e-01 1.41123222e-02 1.63415571e-01\n",
      "  7.28850298e-01 6.17537069e-01 5.02926223e-01 4.73809336e-01\n",
      "  9.99513369e-01 4.57296737e-02 1.02992851e-03 8.25560528e-01\n",
      "  3.13201329e-01 2.34065178e-02 5.74177565e-01 3.39220407e-01\n",
      "  2.72248034e-01 6.21507380e-01 7.02680717e-01 1.13264871e-01\n",
      "  4.60228272e-01 1.75206602e-01 8.24602114e-01 7.05368782e-02\n",
      "  6.04551231e-01 8.05290416e-01 9.78872943e-01 5.73985268e-01\n",
      "  5.04709174e-01 2.05033801e-01 2.04844318e-01 9.94109438e-01\n",
      "  4.66830540e-02 9.70096985e-01 9.72981888e-01 1.93437679e-01\n",
      "  9.06008836e-01 1.03395114e-01 2.73897469e-01 1.36009442e-01\n",
      "  2.00701698e-01 5.62354329e-01 6.69012312e-01 8.94427063e-01\n",
      "  1.83268120e-01 6.23297200e-01 3.17574248e-01 6.02571130e-01\n",
      "  1.42318118e-01 4.40882852e-01 1.35365569e-01 4.46398007e-01\n",
      "  8.82971816e-01 7.66556717e-01 8.33534374e-01 6.51897412e-01\n",
      "  4.96773469e-01 1.96691986e-01 4.71297339e-01 8.09256179e-01\n",
      "  9.68934105e-01 1.04446552e-01 9.82536946e-02 5.16335523e-02\n",
      "  5.56975704e-01 3.29710573e-01 4.34711503e-01 7.55809979e-01\n",
      "  8.99316361e-01 7.09504752e-01 2.51616949e-01 7.89563280e-01\n",
      "  1.51651995e-01 2.22327115e-01 2.34409236e-01 5.26780519e-01\n",
      "  7.36697036e-01 4.32932382e-01 2.72378220e-02 7.41937059e-01\n",
      "  5.81468648e-01 6.78578406e-01 3.52205223e-01 3.95177740e-01\n",
      "  2.58000446e-01 9.22831799e-01 5.25822144e-01 8.64664991e-01\n",
      "  8.70069924e-02 6.75830153e-01 1.13496598e-01 8.76519970e-01\n",
      "  8.34068812e-01 2.01373416e-01 1.08203269e-01 4.38337741e-01\n",
      "  2.80576064e-01 4.26056685e-01 7.02952479e-01 6.32340451e-01\n",
      "  3.66918884e-01 3.11644453e-01 3.82053879e-01 7.29455552e-01\n",
      "  9.67575324e-03 6.19648757e-02 3.53967810e-01 2.69942668e-01\n",
      "  9.66127270e-01 7.30306173e-01 9.43724868e-01 7.51375503e-01\n",
      "  5.49209009e-01 5.10230473e-01 6.98454382e-01 5.46656698e-01\n",
      "  4.81219995e-01 5.08621411e-01 1.50933094e-01 3.67501173e-01\n",
      "  8.24276242e-01 9.09510948e-01 9.44413109e-02 8.25085496e-01\n",
      "  2.40374004e-01 9.05156081e-01 3.45881442e-01 2.32385184e-01\n",
      "  2.50694690e-01 4.80479237e-02 9.59454038e-01 1.43111473e-01\n",
      "  9.60548466e-01 6.71609254e-01 3.57120615e-01 9.39024504e-01\n",
      "  3.73569392e-01 2.37503661e-01 1.69423633e-01 7.89351474e-01\n",
      "  1.27661621e-01 6.36371843e-01 2.12374780e-01 7.46944384e-01\n",
      "  6.92331453e-01 9.55227079e-01 1.11635916e-01 5.90090867e-01\n",
      "  7.51690471e-02 6.89075139e-01 1.23308216e-01 8.00783498e-01\n",
      "  4.69684026e-01 3.68730838e-01 5.13003855e-01 9.94888397e-01\n",
      "  9.26146427e-01 5.79542020e-01 5.40834142e-01 9.24853787e-01\n",
      "  4.96912610e-01 3.09716737e-01 9.76902649e-01 8.82758459e-01\n",
      "  8.42188947e-01 5.27450082e-02 5.77218848e-01 9.42851761e-01\n",
      "  9.52940390e-01 8.25743647e-02 9.22184979e-01 9.68978964e-02\n",
      "  6.48933320e-01 9.51216612e-01 1.90809857e-01 5.94333188e-01\n",
      "  8.19243946e-01 8.25564946e-01 9.12797727e-01 7.80914656e-01\n",
      "  1.40301200e-01 3.04243969e-01 1.01045673e-01 2.08793072e-01\n",
      "  8.76138142e-01 5.92021951e-01 6.43582124e-02 6.71866239e-02\n",
      "  8.62889718e-02 2.24982027e-01 6.52586069e-01 6.14411394e-01\n",
      "  8.17645081e-01 4.13450633e-01 5.53439871e-01 9.86137038e-01\n",
      "  9.98182951e-01 7.21712780e-01 6.89858721e-01 1.79565306e-01\n",
      "  3.67392389e-01 5.29379747e-01 9.16680405e-01 2.23336502e-01\n",
      "  1.03055193e-01 7.59462875e-01 5.64274909e-01 4.23036347e-01\n",
      "  6.15068925e-01 2.35891269e-01 8.00313088e-01 9.25083724e-01\n",
      "  8.50841969e-01 5.93747092e-01 1.46234872e-02 9.47492446e-01\n",
      "  1.23519093e-01 6.62983953e-01 5.01369391e-01 1.08900271e-01\n",
      "  1.02218731e-01 7.68029228e-01 4.78162757e-01 7.49301848e-02\n",
      "  6.96266603e-01 8.01826670e-02 4.29896015e-01 5.64884019e-01\n",
      "  3.91917869e-01 5.87238240e-01 4.26942480e-01 4.31587571e-01\n",
      "  6.36607038e-02 3.29814069e-01 1.59500931e-01 7.22757710e-01\n",
      "  1.45296996e-01 3.38367358e-01 8.64570692e-01 7.93405723e-02\n",
      "  8.82625491e-01 1.26537983e-01 4.54598805e-01 7.47051271e-01\n",
      "  8.38626249e-01 7.88218330e-01 4.87959035e-01 1.02011578e-01\n",
      "  7.53802232e-01 8.80481110e-01 5.26645422e-01 1.70809678e-01\n",
      "  4.59556654e-01 9.68468726e-01 3.58459008e-01 5.99272497e-01\n",
      "  9.17958467e-01 7.24694692e-01 1.48517838e-01 5.61805705e-01\n",
      "  8.79320913e-01 3.57876983e-01 9.23119658e-01 3.06254085e-01\n",
      "  1.99872982e-01 5.77179409e-01 5.64154355e-01 6.50653748e-01\n",
      "  8.99001456e-01 9.87194202e-01 1.40540482e-01 4.20646706e-01\n",
      "  8.65548107e-01 4.93305094e-01 8.35821447e-02 7.62212279e-01\n",
      "  8.09672520e-01 7.33129131e-01 8.53754930e-01 7.27768076e-01\n",
      "  7.21573870e-01 5.75453214e-01 1.85612146e-01 7.60918676e-01\n",
      "  8.68965934e-01 9.52504145e-01 1.98937539e-01 7.96115983e-01\n",
      "  7.70617604e-02 9.90859595e-01 1.97813275e-01 4.74225125e-02\n",
      "  4.47447877e-01 7.70363474e-01 3.76037878e-01 8.45109727e-01\n",
      "  3.03877310e-01 2.27195749e-01 7.00170739e-02 3.80332868e-01\n",
      "  7.32925280e-02 7.45877089e-01 9.68554141e-01 1.31799187e-01\n",
      "  1.18837543e-03 3.14181462e-01 4.05152753e-01 8.80276879e-01\n",
      "  6.47517004e-01 1.54603626e-01 1.79595061e-01 4.07176514e-01\n",
      "  6.54957136e-01 5.78990808e-01 2.89454178e-01 9.09055165e-01\n",
      "  9.27773043e-01 9.15091963e-01 1.25421791e-01 1.68066802e-01\n",
      "  5.24857816e-01 6.45055123e-01 5.74012577e-02 8.80081114e-01\n",
      "  4.66088025e-01 4.75772369e-01 9.24925166e-01 5.30029287e-01\n",
      "  3.95248538e-01 7.50695810e-01 2.51690424e-01 9.60965951e-01\n",
      "  6.68676586e-01 8.39499114e-01 9.94619406e-01 7.65486749e-01\n",
      "  5.10766787e-01 3.75941969e-01 4.98977024e-01 5.77055889e-02\n",
      "  7.28710341e-01 7.74305608e-01 7.94829859e-01 4.15555114e-01\n",
      "  4.42363369e-01 2.55190299e-01 6.40340254e-01 3.30207095e-01\n",
      "  1.65381500e-01 3.51064289e-01 6.43711737e-01 4.72068795e-01\n",
      "  2.43988857e-01 4.43178495e-01 5.10122403e-01 4.09440404e-01\n",
      "  2.07536092e-01 8.90944230e-01 9.23882622e-01 6.80893169e-01\n",
      "  1.56198389e-01 3.89098960e-01 7.27498960e-01 6.75575393e-01\n",
      "  8.96677642e-01 2.88788097e-01 5.00026773e-01 7.30096748e-01\n",
      "  1.90232822e-01 5.63485537e-01 2.45756868e-01 3.42267032e-01\n",
      "  4.26153555e-01 8.07056572e-01 4.93434331e-01 8.56411617e-02\n",
      "  7.99047981e-01 3.11784547e-01 7.64790336e-01 7.40952545e-01\n",
      "  4.86167949e-01 1.69079634e-01 4.74823261e-01 3.51380984e-01\n",
      "  9.34598142e-01 9.79394254e-01 3.23944805e-01 4.71693870e-01\n",
      "  6.93134457e-02 9.35370488e-01 5.14884298e-01 1.31678824e-01\n",
      "  2.70917696e-01 1.35206294e-01 9.95814415e-01 1.72682098e-01\n",
      "  4.46658671e-01 4.99645937e-01 8.38690032e-01 2.13973833e-01\n",
      "  9.03455431e-01 9.88363183e-01 8.00215956e-01 5.48037546e-01\n",
      "  5.24001769e-01 5.01571300e-01 1.10458746e-01 8.55200368e-01\n",
      "  8.68552693e-01 9.84710428e-01 7.90492638e-01 5.71446829e-01\n",
      "  2.88313921e-01 9.33078144e-01 8.21212437e-01 6.92265752e-01\n",
      "  2.08950621e-01 5.16249093e-01 7.86022585e-02 2.31560538e-01\n",
      "  9.92580731e-01 6.36625044e-01 2.47805872e-01 8.49965177e-01\n",
      "  3.60289722e-01 1.71864085e-01 1.25618529e-01 4.33499499e-01\n",
      "  3.68680028e-01 9.46462967e-01 7.36767645e-01 3.60580421e-01\n",
      "  8.07335648e-01 3.95624958e-01 4.06480109e-01 7.38366661e-01\n",
      "  8.54070956e-01 6.67322963e-01 7.23876134e-01 1.74508120e-01\n",
      "  9.23236810e-02 8.46731268e-01 3.91463276e-01 8.22139533e-01\n",
      "  2.63916717e-01 4.67576991e-01 2.50113047e-01 5.23044088e-01\n",
      "  2.45253744e-01 5.58046195e-01 1.28868259e-01 2.39123003e-02\n",
      "  3.29874163e-01 8.42627556e-01 4.12438983e-01 7.62058407e-01\n",
      "  1.83371484e-01 3.04306211e-01 5.46334511e-01 7.63134508e-01\n",
      "  8.09609739e-01 2.56435896e-01 9.98501064e-01 7.98084883e-02\n",
      "  7.79686510e-01 8.78983101e-01 6.58031430e-01 9.70717117e-01\n",
      "  1.45838128e-01 9.45131824e-01 5.79301056e-02 4.42608959e-01\n",
      "  7.66984969e-01 7.57161644e-02 1.64838068e-01 6.43913115e-01\n",
      "  5.57332546e-01 8.70964753e-01 2.54160270e-01 3.17347320e-02\n",
      "  3.30275170e-01 4.70528828e-02 8.03710548e-01 2.99256559e-01\n",
      "  7.07807740e-01 3.52614350e-01 3.39138015e-01 2.17974638e-01\n",
      "  3.35974839e-01 1.38889935e-02 7.24094446e-01 8.14932521e-01\n",
      "  2.41663017e-01 1.75012389e-01 2.95830650e-02 9.32501837e-01\n",
      "  4.65651758e-01 9.79148810e-01 8.04180664e-01 1.61047107e-01\n",
      "  3.98125952e-01 1.59912289e-01 4.77507674e-01 2.23860826e-02\n",
      "  7.50451715e-01 8.18709453e-01 8.06195921e-01 5.60793725e-01\n",
      "  9.13299504e-01 8.93189326e-01 2.04364900e-01 3.99953055e-01\n",
      "  6.82190944e-01 4.78400625e-01 2.04201561e-01 8.36633740e-01\n",
      "  7.14351259e-01 2.42727804e-01 6.69375110e-01 4.88080536e-01\n",
      "  9.52250982e-01 2.82838623e-01 9.38226748e-01 7.98662285e-01\n",
      "  8.46952085e-01 8.50850721e-01 2.84648572e-03 2.91984811e-01\n",
      "  7.72427587e-01 7.34938921e-01 4.99506159e-01 9.60704387e-01\n",
      "  2.69276613e-01 7.67214967e-02 8.13072995e-01 9.14829853e-01\n",
      "  8.52960042e-01 1.87441781e-01 9.12937714e-01 8.93449566e-01\n",
      "  1.53583781e-01 8.52517176e-01 4.12770610e-01 6.47740967e-01\n",
      "  4.37648762e-01 6.53445607e-01 8.43411526e-01 2.65387303e-01\n",
      "  2.97413301e-01 7.01255890e-01 6.26198597e-01 1.01206876e-01\n",
      "  2.28342450e-01 5.24511741e-01 6.52686728e-01 7.56407232e-01\n",
      "  6.24610472e-01 4.88461934e-01 7.96408548e-01 9.54900683e-01\n",
      "  3.26701356e-01 2.17736902e-01 8.58641344e-01 2.18605034e-01\n",
      "  9.66910044e-01 8.53097007e-01 3.38949811e-01 7.39845164e-01\n",
      "  9.03340612e-01 7.61520376e-02 9.34529930e-01 5.81103561e-02\n",
      "  5.65183910e-01 5.48700508e-01 2.58691844e-01 4.96632972e-01\n",
      "  9.55174172e-01 6.37755532e-01 4.51707170e-01 8.25701741e-01\n",
      "  6.62371273e-01 7.54692375e-01 8.38809723e-01 4.09193264e-01\n",
      "  6.80210006e-01 2.08225844e-01 7.07697838e-02 2.82818377e-01\n",
      "  3.36592762e-01 5.06683371e-01 2.40670116e-01 9.35599470e-01\n",
      "  3.39672941e-02 4.14576456e-01 6.14975687e-01 5.42597733e-01\n",
      "  7.55443334e-01 2.23741646e-01 4.93260381e-01 9.54480263e-01\n",
      "  8.64278746e-02 6.86277804e-01 7.55456387e-01 1.94819066e-01\n",
      "  2.51677454e-01 2.73418621e-01 4.60011201e-01 3.83409735e-01\n",
      "  5.09548941e-01 6.18524955e-01 8.51080686e-01 2.35291139e-01\n",
      "  4.01855303e-01 6.53850763e-01 6.34509926e-01]]\n",
      "Change in theta: 4.753480337470256e-07\n",
      "---------------------------------------------\n",
      "Iteration number 3 finished, Running time 304.1105239391327, Roo2 iteration 3 = -3368689.4670471405\n",
      "Iteration 1:\n",
      "Theta: [[6.82554003e-01 2.04502241e-01 3.44607824e-01 5.75097688e-01\n",
      "  1.67779419e-01 3.75628985e-01 1.16336367e-01 8.83568406e-01\n",
      "  5.22022966e-01 3.47321766e-01 8.63866641e-01 1.62083430e-01\n",
      "  1.26202916e-01 5.95386827e-01 9.41463605e-01 7.64145904e-01\n",
      "  5.77475940e-01 7.73435508e-01 1.63593254e-01 7.90823528e-01\n",
      "  9.35483061e-02 2.68777655e-01 3.84658448e-01 4.97067704e-01\n",
      "  4.94970672e-01 6.36555934e-01 3.65338790e-01 2.17164143e-01\n",
      "  4.22670362e-01 8.48201554e-01 8.36615933e-01 9.86236937e-01\n",
      "  4.69569405e-01 5.17748666e-01 1.62397275e-02 8.68918748e-01\n",
      "  6.89848337e-01 6.26055091e-01 4.87909841e-01 5.22104026e-01\n",
      "  4.13585182e-01 4.41736313e-01 7.98417365e-01 4.11070437e-01\n",
      "  7.66465660e-01 2.83430564e-01 6.01803746e-01 7.80243140e-02\n",
      "  9.99833686e-01 2.20881427e-01 7.66499837e-01 6.18828452e-01\n",
      "  6.52101307e-01 9.73259816e-01 3.28885969e-01 2.16589102e-02\n",
      "  3.20326313e-01 1.12502166e-01 5.44294114e-02 9.88035470e-01\n",
      "  6.05608579e-02 8.64971324e-01 4.53124941e-02 4.81457484e-01\n",
      "  7.91189457e-01 3.88664338e-01 4.32160222e-01 7.85517281e-02\n",
      "  1.92372671e-01 9.14146066e-01 7.07706561e-01 5.38890251e-02\n",
      "  8.76074250e-01 5.85030829e-01 5.29679373e-01 9.12766449e-01\n",
      "  7.89503794e-01 6.30169406e-01 9.55817453e-01 4.74069828e-01\n",
      "  4.29731610e-01 9.38208464e-01 6.08019891e-01 7.48802690e-01\n",
      "  5.09943788e-01 8.82809256e-01 9.07715725e-01 5.37790054e-01\n",
      "  5.87228163e-01 1.60252838e-01 8.63031825e-01 9.98897761e-02\n",
      "  9.15904780e-01 1.18084498e-01 5.78997745e-01 3.95698088e-01\n",
      "  2.48600667e-01 9.28291725e-01 2.19424229e-01 6.00639879e-02\n",
      "  9.26502768e-01 7.02451322e-01 4.80476249e-01 4.96189729e-01\n",
      "  6.62355872e-03 1.45497631e-01 4.15098601e-01 3.64956046e-02\n",
      "  7.29420548e-01 5.36862265e-01 3.39483230e-01 6.57780131e-01\n",
      "  1.25009174e-01 4.34470694e-01 8.53719109e-02 9.71345392e-01\n",
      "  2.27606449e-01 7.88492339e-01 8.18911397e-01 6.22087803e-01\n",
      "  9.20592023e-01 3.11768804e-01 1.55991519e-01 9.24293032e-01\n",
      "  5.82767551e-01 9.49777588e-01 3.92790795e-02 7.11900750e-01\n",
      "  5.12329265e-01 7.59389241e-01 8.43579191e-01 1.22654170e-01\n",
      "  4.76978338e-01 4.99907101e-01 3.46100829e-01 6.76109601e-01\n",
      "  1.99206472e-01 3.46405546e-01 7.67269795e-01 6.21479808e-02\n",
      "  4.65647456e-01 1.35869635e-01 2.74924786e-02 8.14427732e-01\n",
      "  5.12267404e-01 2.89704952e-01 2.26145935e-01 8.59413003e-01\n",
      "  8.31753100e-01 7.21012676e-01 2.36518241e-01 3.34394065e-01\n",
      "  5.17095763e-03 2.74506784e-01 9.88180387e-02 8.88425615e-01\n",
      "  6.15288906e-01 3.40337175e-01 8.85012657e-01 4.82965657e-01\n",
      "  6.52121962e-01 5.35452213e-01 8.15415761e-01 4.69713512e-01\n",
      "  9.84654050e-01 4.40932753e-02 8.85707256e-01 6.94550939e-01\n",
      "  8.33630796e-01 4.62186092e-01 6.11407333e-01 2.26842914e-01\n",
      "  7.93864406e-01 2.29896918e-01 4.08241961e-01 4.87989466e-01\n",
      "  3.36763370e-01 3.71354143e-01 6.57332743e-03 3.56333292e-01\n",
      "  7.18375866e-01 1.38365294e-01 9.48248227e-01 1.24259471e-01\n",
      "  8.18078692e-01 7.58712360e-01 5.16470617e-01 3.77474741e-01\n",
      "  1.32096339e-01 8.55622687e-01 9.48041483e-01 2.25120370e-01\n",
      "  5.16180217e-01 7.49090467e-01 6.40090495e-01 8.66477577e-01\n",
      "  1.47617985e-01 7.42588131e-01 6.45722961e-01 7.58004568e-01\n",
      "  8.00777900e-01 2.21021623e-01 6.02817171e-03 6.27631198e-01\n",
      "  2.51498067e-01 7.70821984e-01 1.97877040e-01 9.04953604e-01\n",
      "  8.38341336e-01 4.17760848e-01 7.56697657e-01 5.77362457e-01\n",
      "  9.48701751e-01 6.54339562e-01 5.91188391e-01 9.88744685e-01\n",
      "  5.36342121e-01 9.00694984e-01 1.28348221e-01 3.92121766e-01\n",
      "  9.80281605e-01 1.78486006e-01 5.28752191e-01 2.79105341e-01\n",
      "  7.96870399e-04 4.24790868e-01 3.87202830e-01 8.86468381e-01\n",
      "  3.16530178e-01 2.40243180e-01 1.78132578e-01 5.45031083e-01\n",
      "  9.51871444e-02 1.16782533e-01 4.73081292e-01 2.60723260e-02\n",
      "  9.36574105e-01 4.60439384e-01 9.32264118e-01 8.83170824e-01\n",
      "  6.57749342e-02 3.07745433e-01 1.89227971e-01 8.87228807e-01\n",
      "  6.49844574e-01 2.15822964e-01 1.20414020e-01 8.95799440e-01\n",
      "  9.55006202e-01 5.13450595e-01 2.84474382e-01 2.51476710e-01\n",
      "  6.40146755e-01 7.51351265e-01 3.00894658e-01 5.88129889e-01\n",
      "  4.87401845e-01 8.58443989e-01 1.42265057e-01 7.98677271e-01\n",
      "  2.73832925e-01 3.83821024e-01 8.75978114e-01 5.68011417e-01\n",
      "  9.93525454e-01 2.36356669e-01 1.41568847e-01 1.24055013e-01\n",
      "  5.27893501e-01 3.77250614e-01 1.40375753e-02 5.92209057e-01\n",
      "  2.26213503e-01 2.20717869e-01 9.22144118e-01 3.00988644e-01\n",
      "  8.47020013e-01 4.54745398e-01 7.00656009e-01 4.32758668e-01\n",
      "  2.82764494e-01 1.21096527e-02 9.12949747e-01 9.16694529e-02\n",
      "  4.80168006e-01 7.40648435e-01 1.07368346e-01 5.77264074e-01\n",
      "  2.73532681e-01 8.93704646e-01 8.18033917e-01 1.64486199e-01\n",
      "  3.99019571e-01 2.98301764e-01 1.90679409e-02 3.37437381e-01\n",
      "  8.73744613e-01 5.34777001e-01 1.41146121e-02 1.63417861e-01\n",
      "  7.28852588e-01 6.17539358e-01 5.02928513e-01 4.73811626e-01\n",
      "  9.99515659e-01 4.57319636e-02 1.03221834e-03 8.25562818e-01\n",
      "  3.13203619e-01 2.34088076e-02 5.74179855e-01 3.39222697e-01\n",
      "  2.72250324e-01 6.21509669e-01 7.02683006e-01 1.13267161e-01\n",
      "  4.60230561e-01 1.75208891e-01 8.24604403e-01 7.05391680e-02\n",
      "  6.04553521e-01 8.05292706e-01 9.78875233e-01 5.73987558e-01\n",
      "  5.04711464e-01 2.05036091e-01 2.04846608e-01 9.94111728e-01\n",
      "  4.66853438e-02 9.70099275e-01 9.72984178e-01 1.93439969e-01\n",
      "  9.06011126e-01 1.03397404e-01 2.73899759e-01 1.36011732e-01\n",
      "  2.00703988e-01 5.62356619e-01 6.69014601e-01 8.94429353e-01\n",
      "  1.83270410e-01 6.23299490e-01 3.17576537e-01 6.02573419e-01\n",
      "  1.42320408e-01 4.40885142e-01 1.35367859e-01 4.46400297e-01\n",
      "  8.82974106e-01 7.66559007e-01 8.33536664e-01 6.51899702e-01\n",
      "  4.96775759e-01 1.96694276e-01 4.71299629e-01 8.09258469e-01\n",
      "  9.68936395e-01 1.04448842e-01 9.82559845e-02 5.16358421e-02\n",
      "  5.56977994e-01 3.29712863e-01 4.34713793e-01 7.55812269e-01\n",
      "  8.99318651e-01 7.09507042e-01 2.51619239e-01 7.89565570e-01\n",
      "  1.51654285e-01 2.22329405e-01 2.34411526e-01 5.26782809e-01\n",
      "  7.36699326e-01 4.32934672e-01 2.72401118e-02 7.41939349e-01\n",
      "  5.81470938e-01 6.78580696e-01 3.52207513e-01 3.95180030e-01\n",
      "  2.58002736e-01 9.22834089e-01 5.25824434e-01 8.64667281e-01\n",
      "  8.70092823e-02 6.75832443e-01 1.13498888e-01 8.76522260e-01\n",
      "  8.34071102e-01 2.01375706e-01 1.08205558e-01 4.38340030e-01\n",
      "  2.80578353e-01 4.26058974e-01 7.02954769e-01 6.32342741e-01\n",
      "  3.66921174e-01 3.11646743e-01 3.82056168e-01 7.29457842e-01\n",
      "  9.67804307e-03 6.19671656e-02 3.53970100e-01 2.69944958e-01\n",
      "  9.66129560e-01 7.30308463e-01 9.43727157e-01 7.51377793e-01\n",
      "  5.49211299e-01 5.10232763e-01 6.98456672e-01 5.46658988e-01\n",
      "  4.81222285e-01 5.08623701e-01 1.50935383e-01 3.67503463e-01\n",
      "  8.24278532e-01 9.09513238e-01 9.44436007e-02 8.25087786e-01\n",
      "  2.40376294e-01 9.05158371e-01 3.45883732e-01 2.32387474e-01\n",
      "  2.50696979e-01 4.80502135e-02 9.59456327e-01 1.43113763e-01\n",
      "  9.60550756e-01 6.71611544e-01 3.57122905e-01 9.39026794e-01\n",
      "  3.73571682e-01 2.37505951e-01 1.69425923e-01 7.89353764e-01\n",
      "  1.27663911e-01 6.36374133e-01 2.12377070e-01 7.46946674e-01\n",
      "  6.92333743e-01 9.55229368e-01 1.11638206e-01 5.90093157e-01\n",
      "  7.51713369e-02 6.89077429e-01 1.23310506e-01 8.00785788e-01\n",
      "  4.69686316e-01 3.68733127e-01 5.13006145e-01 9.94890686e-01\n",
      "  9.26148717e-01 5.79544310e-01 5.40836431e-01 9.24856077e-01\n",
      "  4.96914900e-01 3.09719026e-01 9.76904938e-01 8.82760749e-01\n",
      "  8.42191237e-01 5.27472980e-02 5.77221138e-01 9.42854051e-01\n",
      "  9.52942680e-01 8.25766546e-02 9.22187269e-01 9.69001862e-02\n",
      "  6.48935610e-01 9.51218902e-01 1.90812147e-01 5.94335478e-01\n",
      "  8.19246236e-01 8.25567236e-01 9.12800017e-01 7.80916946e-01\n",
      "  1.40303489e-01 3.04246259e-01 1.01047963e-01 2.08795362e-01\n",
      "  8.76140431e-01 5.92024241e-01 6.43605023e-02 6.71889137e-02\n",
      "  8.62912616e-02 2.24984317e-01 6.52588359e-01 6.14413684e-01\n",
      "  8.17647370e-01 4.13452923e-01 5.53442161e-01 9.86139328e-01\n",
      "  9.98185241e-01 7.21715070e-01 6.89861011e-01 1.79567596e-01\n",
      "  3.67394679e-01 5.29382037e-01 9.16682695e-01 2.23338792e-01\n",
      "  1.03057482e-01 7.59465165e-01 5.64277199e-01 4.23038637e-01\n",
      "  6.15071215e-01 2.35893559e-01 8.00315378e-01 9.25086013e-01\n",
      "  8.50844259e-01 5.93749382e-01 1.46257771e-02 9.47494736e-01\n",
      "  1.23521383e-01 6.62986243e-01 5.01371680e-01 1.08902561e-01\n",
      "  1.02221021e-01 7.68031518e-01 4.78165047e-01 7.49324747e-02\n",
      "  6.96268893e-01 8.01849568e-02 4.29898305e-01 5.64886308e-01\n",
      "  3.91920159e-01 5.87240530e-01 4.26944769e-01 4.31589860e-01\n",
      "  6.36629936e-02 3.29816359e-01 1.59503221e-01 7.22760000e-01\n",
      "  1.45299286e-01 3.38369648e-01 8.64572982e-01 7.93428621e-02\n",
      "  8.82627781e-01 1.26540273e-01 4.54601095e-01 7.47053561e-01\n",
      "  8.38628539e-01 7.88220620e-01 4.87961325e-01 1.02013868e-01\n",
      "  7.53804521e-01 8.80483400e-01 5.26647712e-01 1.70811968e-01\n",
      "  4.59558944e-01 9.68471016e-01 3.58461298e-01 5.99274787e-01\n",
      "  9.17960756e-01 7.24696982e-01 1.48520128e-01 5.61807995e-01\n",
      "  8.79323202e-01 3.57879273e-01 9.23121948e-01 3.06256374e-01\n",
      "  1.99875271e-01 5.77181699e-01 5.64156645e-01 6.50656038e-01\n",
      "  8.99003746e-01 9.87196492e-01 1.40542772e-01 4.20648996e-01\n",
      "  8.65550397e-01 4.93307384e-01 8.35844346e-02 7.62214568e-01\n",
      "  8.09674810e-01 7.33131420e-01 8.53757219e-01 7.27770366e-01\n",
      "  7.21576159e-01 5.75455504e-01 1.85614436e-01 7.60920966e-01\n",
      "  8.68968223e-01 9.52506435e-01 1.98939829e-01 7.96118272e-01\n",
      "  7.70640503e-02 9.90861884e-01 1.97815565e-01 4.74248023e-02\n",
      "  4.47450167e-01 7.70365764e-01 3.76040168e-01 8.45112017e-01\n",
      "  3.03879600e-01 2.27198038e-01 7.00193637e-02 3.80335158e-01\n",
      "  7.32948179e-02 7.45879379e-01 9.68556431e-01 1.31801477e-01\n",
      "  1.19066526e-03 3.14183751e-01 4.05155043e-01 8.80279169e-01\n",
      "  6.47519294e-01 1.54605915e-01 1.79597351e-01 4.07178804e-01\n",
      "  6.54959425e-01 5.78993098e-01 2.89456468e-01 9.09057455e-01\n",
      "  9.27775333e-01 9.15094253e-01 1.25424081e-01 1.68069092e-01\n",
      "  5.24860106e-01 6.45057413e-01 5.74035476e-02 8.80083404e-01\n",
      "  4.66090315e-01 4.75774659e-01 9.24927456e-01 5.30031577e-01\n",
      "  3.95250827e-01 7.50698100e-01 2.51692714e-01 9.60968241e-01\n",
      "  6.68678876e-01 8.39501404e-01 9.94621696e-01 7.65489038e-01\n",
      "  5.10769077e-01 3.75944259e-01 4.98979314e-01 5.77078788e-02\n",
      "  7.28712631e-01 7.74307898e-01 7.94832149e-01 4.15557404e-01\n",
      "  4.42365659e-01 2.55192588e-01 6.40342544e-01 3.30209385e-01\n",
      "  1.65383790e-01 3.51066578e-01 6.43714027e-01 4.72071085e-01\n",
      "  2.43991147e-01 4.43180784e-01 5.10124693e-01 4.09442694e-01\n",
      "  2.07538382e-01 8.90946520e-01 9.23884912e-01 6.80895458e-01\n",
      "  1.56200679e-01 3.89101250e-01 7.27501250e-01 6.75577682e-01\n",
      "  8.96679931e-01 2.88790387e-01 5.00029063e-01 7.30099037e-01\n",
      "  1.90235112e-01 5.63487827e-01 2.45759158e-01 3.42269322e-01\n",
      "  4.26155845e-01 8.07058862e-01 4.93436621e-01 8.56434516e-02\n",
      "  7.99050270e-01 3.11786837e-01 7.64792626e-01 7.40954835e-01\n",
      "  4.86170239e-01 1.69081923e-01 4.74825551e-01 3.51383273e-01\n",
      "  9.34600432e-01 9.79396543e-01 3.23947095e-01 4.71696159e-01\n",
      "  6.93157355e-02 9.35372778e-01 5.14886588e-01 1.31681113e-01\n",
      "  2.70919986e-01 1.35208584e-01 9.95816705e-01 1.72684388e-01\n",
      "  4.46660961e-01 4.99648226e-01 8.38692321e-01 2.13976123e-01\n",
      "  9.03457721e-01 9.88365472e-01 8.00218246e-01 5.48039836e-01\n",
      "  5.24004059e-01 5.01573589e-01 1.10461036e-01 8.55202658e-01\n",
      "  8.68554983e-01 9.84712718e-01 7.90494927e-01 5.71449118e-01\n",
      "  2.88316210e-01 9.33080434e-01 8.21214727e-01 6.92268042e-01\n",
      "  2.08952910e-01 5.16251383e-01 7.86045483e-02 2.31562828e-01\n",
      "  9.92583021e-01 6.36627334e-01 2.47808162e-01 8.49967467e-01\n",
      "  3.60292011e-01 1.71866375e-01 1.25620819e-01 4.33501789e-01\n",
      "  3.68682318e-01 9.46465257e-01 7.36769935e-01 3.60582711e-01\n",
      "  8.07337938e-01 3.95627248e-01 4.06482399e-01 7.38368951e-01\n",
      "  8.54073245e-01 6.67325253e-01 7.23878424e-01 1.74510410e-01\n",
      "  9.23259708e-02 8.46733558e-01 3.91465566e-01 8.22141823e-01\n",
      "  2.63919007e-01 4.67579280e-01 2.50115337e-01 5.23046378e-01\n",
      "  2.45256034e-01 5.58048484e-01 1.28870549e-01 2.39145902e-02\n",
      "  3.29876453e-01 8.42629846e-01 4.12441273e-01 7.62060697e-01\n",
      "  1.83373774e-01 3.04308501e-01 5.46336801e-01 7.63136798e-01\n",
      "  8.09612028e-01 2.56438186e-01 9.98503353e-01 7.98107781e-02\n",
      "  7.79688800e-01 8.78985391e-01 6.58033720e-01 9.70719407e-01\n",
      "  1.45840417e-01 9.45134114e-01 5.79323954e-02 4.42611249e-01\n",
      "  7.66987259e-01 7.57184542e-02 1.64840358e-01 6.43915405e-01\n",
      "  5.57334836e-01 8.70967043e-01 2.54162559e-01 3.17370219e-02\n",
      "  3.30277460e-01 4.70551726e-02 8.03712838e-01 2.99258849e-01\n",
      "  7.07810030e-01 3.52616639e-01 3.39140305e-01 2.17976928e-01\n",
      "  3.35977129e-01 1.38912833e-02 7.24096736e-01 8.14934811e-01\n",
      "  2.41665307e-01 1.75014679e-01 2.95853548e-02 9.32504127e-01\n",
      "  4.65654048e-01 9.79151100e-01 8.04182954e-01 1.61049397e-01\n",
      "  3.98128242e-01 1.59914578e-01 4.77509964e-01 2.23883724e-02\n",
      "  7.50454005e-01 8.18711743e-01 8.06198211e-01 5.60796015e-01\n",
      "  9.13301794e-01 8.93191616e-01 2.04367189e-01 3.99955344e-01\n",
      "  6.82193234e-01 4.78402915e-01 2.04203851e-01 8.36636030e-01\n",
      "  7.14353549e-01 2.42730094e-01 6.69377399e-01 4.88082826e-01\n",
      "  9.52253272e-01 2.82840913e-01 9.38229038e-01 7.98664574e-01\n",
      "  8.46954375e-01 8.50853011e-01 2.84877556e-03 2.91987101e-01\n",
      "  7.72429877e-01 7.34941211e-01 4.99508449e-01 9.60706677e-01\n",
      "  2.69278903e-01 7.67237866e-02 8.13075285e-01 9.14832143e-01\n",
      "  8.52962332e-01 1.87444070e-01 9.12940004e-01 8.93451856e-01\n",
      "  1.53586070e-01 8.52519465e-01 4.12772900e-01 6.47743257e-01\n",
      "  4.37651052e-01 6.53447897e-01 8.43413816e-01 2.65389593e-01\n",
      "  2.97415591e-01 7.01258179e-01 6.26200887e-01 1.01209166e-01\n",
      "  2.28344740e-01 5.24514030e-01 6.52689018e-01 7.56409522e-01\n",
      "  6.24612761e-01 4.88464224e-01 7.96410838e-01 9.54902973e-01\n",
      "  3.26703646e-01 2.17739192e-01 8.58643634e-01 2.18607324e-01\n",
      "  9.66912333e-01 8.53099297e-01 3.38952101e-01 7.39847453e-01\n",
      "  9.03342901e-01 7.61543275e-02 9.34532220e-01 5.81126459e-02\n",
      "  5.65186199e-01 5.48702798e-01 2.58694134e-01 4.96635262e-01\n",
      "  9.55176462e-01 6.37757822e-01 4.51709460e-01 8.25704030e-01\n",
      "  6.62373563e-01 7.54694665e-01 8.38812013e-01 4.09195554e-01\n",
      "  6.80212296e-01 2.08228133e-01 7.07720737e-02 2.82820667e-01\n",
      "  3.36595052e-01 5.06685661e-01 2.40672406e-01 9.35601759e-01\n",
      "  3.39695839e-02 4.14578746e-01 6.14977977e-01 5.42600023e-01\n",
      "  7.55445624e-01 2.23743936e-01 4.93262671e-01 9.54482553e-01\n",
      "  8.64301644e-02 6.86280094e-01 7.55458677e-01 1.94821355e-01\n",
      "  2.51679743e-01 2.73420911e-01 4.60013490e-01 3.83412024e-01\n",
      "  5.09551230e-01 6.18527245e-01 8.51082976e-01 2.35293429e-01\n",
      "  4.01857593e-01 6.53853053e-01 6.34512215e-01]]\n",
      "Change in theta: 5.75425998939859e-07\n",
      "---------------------------------------------\n",
      "Iteration number 4 finished, Running time 365.5182797908783, Roo2 iteration 4 = -1661767.6573155269\n",
      "Iteration 1:\n",
      "Theta: [[6.82556126e-01 2.04504363e-01 3.44609946e-01 5.75099811e-01\n",
      "  1.67781542e-01 3.75631108e-01 1.16338490e-01 8.83570528e-01\n",
      "  5.22025088e-01 3.47323889e-01 8.63868763e-01 1.62085553e-01\n",
      "  1.26205039e-01 5.95388950e-01 9.41465727e-01 7.64148026e-01\n",
      "  5.77478063e-01 7.73437630e-01 1.63595376e-01 7.90825651e-01\n",
      "  9.35504285e-02 2.68779778e-01 3.84660570e-01 4.97069827e-01\n",
      "  4.94972795e-01 6.36558057e-01 3.65340912e-01 2.17166266e-01\n",
      "  4.22672484e-01 8.48203676e-01 8.36618055e-01 9.86239060e-01\n",
      "  4.69571527e-01 5.17750789e-01 1.62418498e-02 8.68920870e-01\n",
      "  6.89850459e-01 6.26057213e-01 4.87911963e-01 5.22106148e-01\n",
      "  4.13587304e-01 4.41738436e-01 7.98419487e-01 4.11072559e-01\n",
      "  7.66467782e-01 2.83432686e-01 6.01805868e-01 7.80264364e-02\n",
      "  9.99835809e-01 2.20883549e-01 7.66501959e-01 6.18830575e-01\n",
      "  6.52103429e-01 9.73261939e-01 3.28888091e-01 2.16610326e-02\n",
      "  3.20328435e-01 1.12504289e-01 5.44315338e-02 9.88037592e-01\n",
      "  6.05629803e-02 8.64973447e-01 4.53146164e-02 4.81459606e-01\n",
      "  7.91191579e-01 3.88666461e-01 4.32162344e-01 7.85538505e-02\n",
      "  1.92374793e-01 9.14148188e-01 7.07708684e-01 5.38911474e-02\n",
      "  8.76076372e-01 5.85032952e-01 5.29681496e-01 9.12768572e-01\n",
      "  7.89505916e-01 6.30171529e-01 9.55819576e-01 4.74071950e-01\n",
      "  4.29733732e-01 9.38210586e-01 6.08022013e-01 7.48804813e-01\n",
      "  5.09945910e-01 8.82811378e-01 9.07717848e-01 5.37792177e-01\n",
      "  5.87230285e-01 1.60254960e-01 8.63033947e-01 9.98918985e-02\n",
      "  9.15906902e-01 1.18086620e-01 5.78999867e-01 3.95700211e-01\n",
      "  2.48602789e-01 9.28293847e-01 2.19426351e-01 6.00661103e-02\n",
      "  9.26504890e-01 7.02453444e-01 4.80478372e-01 4.96191851e-01\n",
      "  6.62568110e-03 1.45499754e-01 4.15100723e-01 3.64977270e-02\n",
      "  7.29422670e-01 5.36864387e-01 3.39485352e-01 6.57782253e-01\n",
      "  1.25011296e-01 4.34472816e-01 8.53740332e-02 9.71347514e-01\n",
      "  2.27608571e-01 7.88494462e-01 8.18913520e-01 6.22089925e-01\n",
      "  9.20594146e-01 3.11770926e-01 1.55993641e-01 9.24295155e-01\n",
      "  5.82769673e-01 9.49779710e-01 3.92812019e-02 7.11902873e-01\n",
      "  5.12331387e-01 7.59391363e-01 8.43581313e-01 1.22656293e-01\n",
      "  4.76980460e-01 4.99909224e-01 3.46102951e-01 6.76111723e-01\n",
      "  1.99208595e-01 3.46407668e-01 7.67271917e-01 6.21501032e-02\n",
      "  4.65649579e-01 1.35871757e-01 2.74946010e-02 8.14429854e-01\n",
      "  5.12269526e-01 2.89707074e-01 2.26148057e-01 8.59415126e-01\n",
      "  8.31755223e-01 7.21014798e-01 2.36520363e-01 3.34396188e-01\n",
      "  5.17308001e-03 2.74508907e-01 9.88201611e-02 8.88427737e-01\n",
      "  6.15291028e-01 3.40339297e-01 8.85014779e-01 4.82967779e-01\n",
      "  6.52124084e-01 5.35454335e-01 8.15417883e-01 4.69715635e-01\n",
      "  9.84656172e-01 4.40953976e-02 8.85709378e-01 6.94553062e-01\n",
      "  8.33632919e-01 4.62188215e-01 6.11409455e-01 2.26845036e-01\n",
      "  7.93866529e-01 2.29899040e-01 4.08244083e-01 4.87991588e-01\n",
      "  3.36765493e-01 3.71356265e-01 6.57544981e-03 3.56335414e-01\n",
      "  7.18377988e-01 1.38367417e-01 9.48250349e-01 1.24261593e-01\n",
      "  8.18080815e-01 7.58714482e-01 5.16472740e-01 3.77476863e-01\n",
      "  1.32098462e-01 8.55624809e-01 9.48043606e-01 2.25122492e-01\n",
      "  5.16182339e-01 7.49092590e-01 6.40092618e-01 8.66479700e-01\n",
      "  1.47620108e-01 7.42590254e-01 6.45725083e-01 7.58006690e-01\n",
      "  8.00780023e-01 2.21023745e-01 6.03029409e-03 6.27633321e-01\n",
      "  2.51500189e-01 7.70824106e-01 1.97879163e-01 9.04955727e-01\n",
      "  8.38343458e-01 4.17762971e-01 7.56699779e-01 5.77364580e-01\n",
      "  9.48703874e-01 6.54341685e-01 5.91190514e-01 9.88746807e-01\n",
      "  5.36344244e-01 9.00697106e-01 1.28350343e-01 3.92123888e-01\n",
      "  9.80283728e-01 1.78488128e-01 5.28754313e-01 2.79107463e-01\n",
      "  7.98992774e-04 4.24792990e-01 3.87204953e-01 8.86470504e-01\n",
      "  3.16532300e-01 2.40245302e-01 1.78134701e-01 5.45033206e-01\n",
      "  9.51892668e-02 1.16784655e-01 4.73083414e-01 2.60744484e-02\n",
      "  9.36576227e-01 4.60441506e-01 9.32266240e-01 8.83172946e-01\n",
      "  6.57770566e-02 3.07747556e-01 1.89230093e-01 8.87230929e-01\n",
      "  6.49846696e-01 2.15825086e-01 1.20416142e-01 8.95801562e-01\n",
      "  9.55008325e-01 5.13452717e-01 2.84476504e-01 2.51478833e-01\n",
      "  6.40148878e-01 7.51353387e-01 3.00896781e-01 5.88132011e-01\n",
      "  4.87403967e-01 8.58446111e-01 1.42267180e-01 7.98679393e-01\n",
      "  2.73835047e-01 3.83823147e-01 8.75980236e-01 5.68013539e-01\n",
      "  9.93527576e-01 2.36358791e-01 1.41570970e-01 1.24057136e-01\n",
      "  5.27895623e-01 3.77252737e-01 1.40396977e-02 5.92211179e-01\n",
      "  2.26215625e-01 2.20719992e-01 9.22146241e-01 3.00990767e-01\n",
      "  8.47022135e-01 4.54747520e-01 7.00658132e-01 4.32760790e-01\n",
      "  2.82766616e-01 1.21117751e-02 9.12951870e-01 9.16715753e-02\n",
      "  4.80170128e-01 7.40650557e-01 1.07370469e-01 5.77266196e-01\n",
      "  2.73534803e-01 8.93706768e-01 8.18036039e-01 1.64488322e-01\n",
      "  3.99021693e-01 2.98303886e-01 1.90700633e-02 3.37439503e-01\n",
      "  8.73746735e-01 5.34779124e-01 1.41167344e-02 1.63419983e-01\n",
      "  7.28854710e-01 6.17541481e-01 5.02930635e-01 4.73813748e-01\n",
      "  9.99517782e-01 4.57340859e-02 1.03434072e-03 8.25564941e-01\n",
      "  3.13205741e-01 2.34109300e-02 5.74181977e-01 3.39224819e-01\n",
      "  2.72252446e-01 6.21511792e-01 7.02685129e-01 1.13269283e-01\n",
      "  4.60232684e-01 1.75211014e-01 8.24606526e-01 7.05412904e-02\n",
      "  6.04555643e-01 8.05294828e-01 9.78877355e-01 5.73989680e-01\n",
      "  5.04713586e-01 2.05038213e-01 2.04848730e-01 9.94113850e-01\n",
      "  4.66874662e-02 9.70101397e-01 9.72986300e-01 1.93442092e-01\n",
      "  9.06013249e-01 1.03399526e-01 2.73901881e-01 1.36013854e-01\n",
      "  2.00706110e-01 5.62358742e-01 6.69016724e-01 8.94431475e-01\n",
      "  1.83272532e-01 6.23301612e-01 3.17578660e-01 6.02575542e-01\n",
      "  1.42322530e-01 4.40887264e-01 1.35369982e-01 4.46402419e-01\n",
      "  8.82976228e-01 7.66561129e-01 8.33538787e-01 6.51901825e-01\n",
      "  4.96777881e-01 1.96696399e-01 4.71301751e-01 8.09260592e-01\n",
      "  9.68938517e-01 1.04450964e-01 9.82581068e-02 5.16379645e-02\n",
      "  5.56980116e-01 3.29714985e-01 4.34715915e-01 7.55814391e-01\n",
      "  8.99320773e-01 7.09509164e-01 2.51621361e-01 7.89567692e-01\n",
      "  1.51656407e-01 2.22331527e-01 2.34413648e-01 5.26784931e-01\n",
      "  7.36701449e-01 4.32936794e-01 2.72422342e-02 7.41941472e-01\n",
      "  5.81473060e-01 6.78582819e-01 3.52209635e-01 3.95182153e-01\n",
      "  2.58004858e-01 9.22836211e-01 5.25826556e-01 8.64669403e-01\n",
      "  8.70114047e-02 6.75834565e-01 1.13501010e-01 8.76524382e-01\n",
      "  8.34073225e-01 2.01377828e-01 1.08207681e-01 4.38342153e-01\n",
      "  2.80580476e-01 4.26061097e-01 7.02956892e-01 6.32344863e-01\n",
      "  3.66923296e-01 3.11648865e-01 3.82058291e-01 7.29459964e-01\n",
      "  9.68016545e-03 6.19692880e-02 3.53972222e-01 2.69947080e-01\n",
      "  9.66131682e-01 7.30310585e-01 9.43729280e-01 7.51379916e-01\n",
      "  5.49213421e-01 5.10234885e-01 6.98458795e-01 5.46661111e-01\n",
      "  4.81224407e-01 5.08625823e-01 1.50937506e-01 3.67505586e-01\n",
      "  8.24280654e-01 9.09515361e-01 9.44457231e-02 8.25089908e-01\n",
      "  2.40378416e-01 9.05160493e-01 3.45885854e-01 2.32389596e-01\n",
      "  2.50699102e-01 4.80523359e-02 9.59458450e-01 1.43115885e-01\n",
      "  9.60552878e-01 6.71613666e-01 3.57125027e-01 9.39028916e-01\n",
      "  3.73573805e-01 2.37508073e-01 1.69428046e-01 7.89355886e-01\n",
      "  1.27666033e-01 6.36376256e-01 2.12379192e-01 7.46948796e-01\n",
      "  6.92335865e-01 9.55231491e-01 1.11640328e-01 5.90095280e-01\n",
      "  7.51734593e-02 6.89079551e-01 1.23312628e-01 8.00787910e-01\n",
      "  4.69688438e-01 3.68735250e-01 5.13008267e-01 9.94892809e-01\n",
      "  9.26150840e-01 5.79546432e-01 5.40838554e-01 9.24858199e-01\n",
      "  4.96917023e-01 3.09721149e-01 9.76907061e-01 8.82762871e-01\n",
      "  8.42193359e-01 5.27494204e-02 5.77223261e-01 9.42856173e-01\n",
      "  9.52944802e-01 8.25787769e-02 9.22189391e-01 9.69023086e-02\n",
      "  6.48937732e-01 9.51221024e-01 1.90814270e-01 5.94337600e-01\n",
      "  8.19248359e-01 8.25569358e-01 9.12802139e-01 7.80919068e-01\n",
      "  1.40305612e-01 3.04248381e-01 1.01050086e-01 2.08797484e-01\n",
      "  8.76142554e-01 5.92026363e-01 6.43626246e-02 6.71910361e-02\n",
      "  8.62933840e-02 2.24986439e-01 6.52590481e-01 6.14415806e-01\n",
      "  8.17649493e-01 4.13455045e-01 5.53444284e-01 9.86141450e-01\n",
      "  9.98187363e-01 7.21717193e-01 6.89863133e-01 1.79569718e-01\n",
      "  3.67396801e-01 5.29384159e-01 9.16684817e-01 2.23340914e-01\n",
      "  1.03059605e-01 7.59467287e-01 5.64279321e-01 4.23040759e-01\n",
      "  6.15073337e-01 2.35895681e-01 8.00317500e-01 9.25088136e-01\n",
      "  8.50846381e-01 5.93751505e-01 1.46278994e-02 9.47496858e-01\n",
      "  1.23523505e-01 6.62988365e-01 5.01373803e-01 1.08904684e-01\n",
      "  1.02223143e-01 7.68033641e-01 4.78167169e-01 7.49345971e-02\n",
      "  6.96271015e-01 8.01870792e-02 4.29900427e-01 5.64888431e-01\n",
      "  3.91922281e-01 5.87242652e-01 4.26946892e-01 4.31591983e-01\n",
      "  6.36651160e-02 3.29818481e-01 1.59505343e-01 7.22762122e-01\n",
      "  1.45301408e-01 3.38371771e-01 8.64575105e-01 7.93449845e-02\n",
      "  8.82629904e-01 1.26542395e-01 4.54603217e-01 7.47055684e-01\n",
      "  8.38630661e-01 7.88222742e-01 4.87963447e-01 1.02015990e-01\n",
      "  7.53806644e-01 8.80485523e-01 5.26649834e-01 1.70814090e-01\n",
      "  4.59561066e-01 9.68473139e-01 3.58463420e-01 5.99276909e-01\n",
      "  9.17962879e-01 7.24699105e-01 1.48522251e-01 5.61810117e-01\n",
      "  8.79325325e-01 3.57881396e-01 9.23124070e-01 3.06258497e-01\n",
      "  1.99877394e-01 5.77183821e-01 5.64158767e-01 6.50658160e-01\n",
      "  8.99005868e-01 9.87198614e-01 1.40544894e-01 4.20651118e-01\n",
      "  8.65552519e-01 4.93309506e-01 8.35865569e-02 7.62216691e-01\n",
      "  8.09676932e-01 7.33133543e-01 8.53759342e-01 7.27772488e-01\n",
      "  7.21578282e-01 5.75457626e-01 1.85616558e-01 7.60923089e-01\n",
      "  8.68970346e-01 9.52508557e-01 1.98941951e-01 7.96120395e-01\n",
      "  7.70661726e-02 9.90864007e-01 1.97817687e-01 4.74269247e-02\n",
      "  4.47452289e-01 7.70367886e-01 3.76042290e-01 8.45114139e-01\n",
      "  3.03881722e-01 2.27200161e-01 7.00214861e-02 3.80337280e-01\n",
      "  7.32969402e-02 7.45881501e-01 9.68558553e-01 1.31803600e-01\n",
      "  1.19278764e-03 3.14185874e-01 4.05157165e-01 8.80281291e-01\n",
      "  6.47521416e-01 1.54608038e-01 1.79599474e-01 4.07180926e-01\n",
      "  6.54961548e-01 5.78995220e-01 2.89458590e-01 9.09059577e-01\n",
      "  9.27777456e-01 9.15096375e-01 1.25426203e-01 1.68071215e-01\n",
      "  5.24862229e-01 6.45059535e-01 5.74056699e-02 8.80085526e-01\n",
      "  4.66092437e-01 4.75776782e-01 9.24929578e-01 5.30033700e-01\n",
      "  3.95252950e-01 7.50700222e-01 2.51694836e-01 9.60970363e-01\n",
      "  6.68680998e-01 8.39503526e-01 9.94623818e-01 7.65491161e-01\n",
      "  5.10771199e-01 3.75946381e-01 4.98981436e-01 5.77100011e-02\n",
      "  7.28714753e-01 7.74310020e-01 7.94834271e-01 4.15559526e-01\n",
      "  4.42367781e-01 2.55194711e-01 6.40344666e-01 3.30211508e-01\n",
      "  1.65385912e-01 3.51068701e-01 6.43716150e-01 4.72073207e-01\n",
      "  2.43993269e-01 4.43182907e-01 5.10126815e-01 4.09444816e-01\n",
      "  2.07540505e-01 8.90948642e-01 9.23887034e-01 6.80897581e-01\n",
      "  1.56202802e-01 3.89103372e-01 7.27503372e-01 6.75579805e-01\n",
      "  8.96682054e-01 2.88792509e-01 5.00031186e-01 7.30101160e-01\n",
      "  1.90237234e-01 5.63489949e-01 2.45761280e-01 3.42271444e-01\n",
      "  4.26157967e-01 8.07060985e-01 4.93438743e-01 8.56455739e-02\n",
      "  7.99052393e-01 3.11788960e-01 7.64794748e-01 7.40956957e-01\n",
      "  4.86172362e-01 1.69084046e-01 4.74827673e-01 3.51385396e-01\n",
      "  9.34602554e-01 9.79398666e-01 3.23949218e-01 4.71698282e-01\n",
      "  6.93178579e-02 9.35374900e-01 5.14888710e-01 1.31683236e-01\n",
      "  2.70922108e-01 1.35210706e-01 9.95818827e-01 1.72686511e-01\n",
      "  4.46663083e-01 4.99650349e-01 8.38694444e-01 2.13978245e-01\n",
      "  9.03459843e-01 9.88367595e-01 8.00220368e-01 5.48041958e-01\n",
      "  5.24006181e-01 5.01575712e-01 1.10463159e-01 8.55204781e-01\n",
      "  8.68557105e-01 9.84714840e-01 7.90497050e-01 5.71451241e-01\n",
      "  2.88318333e-01 9.33082556e-01 8.21216849e-01 6.92270165e-01\n",
      "  2.08955033e-01 5.16253505e-01 7.86066707e-02 2.31564950e-01\n",
      "  9.92585143e-01 6.36629456e-01 2.47810284e-01 8.49969589e-01\n",
      "  3.60294134e-01 1.71868498e-01 1.25622941e-01 4.33503912e-01\n",
      "  3.68684441e-01 9.46467380e-01 7.36772057e-01 3.60584833e-01\n",
      "  8.07340061e-01 3.95629370e-01 4.06484521e-01 7.38371073e-01\n",
      "  8.54075368e-01 6.67327375e-01 7.23880546e-01 1.74512532e-01\n",
      "  9.23280932e-02 8.46735680e-01 3.91467688e-01 8.22143945e-01\n",
      "  2.63921129e-01 4.67581403e-01 2.50117459e-01 5.23048500e-01\n",
      "  2.45258156e-01 5.58050607e-01 1.28872672e-01 2.39167125e-02\n",
      "  3.29878575e-01 8.42631968e-01 4.12443395e-01 7.62062819e-01\n",
      "  1.83375896e-01 3.04310624e-01 5.46338923e-01 7.63138920e-01\n",
      "  8.09614151e-01 2.56440308e-01 9.98505476e-01 7.98129005e-02\n",
      "  7.79690922e-01 8.78987513e-01 6.58035843e-01 9.70721529e-01\n",
      "  1.45842540e-01 9.45136236e-01 5.79345178e-02 4.42613371e-01\n",
      "  7.66989381e-01 7.57205766e-02 1.64842480e-01 6.43917527e-01\n",
      "  5.57336959e-01 8.70969165e-01 2.54164682e-01 3.17391442e-02\n",
      "  3.30279583e-01 4.70572950e-02 8.03714961e-01 2.99260971e-01\n",
      "  7.07812152e-01 3.52618762e-01 3.39142428e-01 2.17979051e-01\n",
      "  3.35979251e-01 1.38934057e-02 7.24098858e-01 8.14936933e-01\n",
      "  2.41667429e-01 1.75016801e-01 2.95874772e-02 9.32506250e-01\n",
      "  4.65656170e-01 9.79153222e-01 8.04185076e-01 1.61051520e-01\n",
      "  3.98130365e-01 1.59916701e-01 4.77512087e-01 2.23904948e-02\n",
      "  7.50456128e-01 8.18713866e-01 8.06200333e-01 5.60798138e-01\n",
      "  9.13303917e-01 8.93193739e-01 2.04369312e-01 3.99957467e-01\n",
      "  6.82195356e-01 4.78405037e-01 2.04205973e-01 8.36638152e-01\n",
      "  7.14355671e-01 2.42732217e-01 6.69379522e-01 4.88084949e-01\n",
      "  9.52255394e-01 2.82843036e-01 9.38231160e-01 7.98666697e-01\n",
      "  8.46956497e-01 8.50855133e-01 2.85089793e-03 2.91989223e-01\n",
      "  7.72431999e-01 7.34943333e-01 4.99510571e-01 9.60708799e-01\n",
      "  2.69281025e-01 7.67259089e-02 8.13077407e-01 9.14834265e-01\n",
      "  8.52964455e-01 1.87446193e-01 9.12942126e-01 8.93453979e-01\n",
      "  1.53588193e-01 8.52521588e-01 4.12775023e-01 6.47745379e-01\n",
      "  4.37653175e-01 6.53450020e-01 8.43415938e-01 2.65391716e-01\n",
      "  2.97417713e-01 7.01260302e-01 6.26203010e-01 1.01211288e-01\n",
      "  2.28346862e-01 5.24516153e-01 6.52691140e-01 7.56411644e-01\n",
      "  6.24614884e-01 4.88466346e-01 7.96412960e-01 9.54905095e-01\n",
      "  3.26705769e-01 2.17741314e-01 8.58645757e-01 2.18609447e-01\n",
      "  9.66914456e-01 8.53101420e-01 3.38954223e-01 7.39849576e-01\n",
      "  9.03345024e-01 7.61564498e-02 9.34534342e-01 5.81147683e-02\n",
      "  5.65188322e-01 5.48704920e-01 2.58696256e-01 4.96637384e-01\n",
      "  9.55178585e-01 6.37759944e-01 4.51711582e-01 8.25706153e-01\n",
      "  6.62375685e-01 7.54696788e-01 8.38814135e-01 4.09197677e-01\n",
      "  6.80214418e-01 2.08230256e-01 7.07741960e-02 2.82822789e-01\n",
      "  3.36597175e-01 5.06687784e-01 2.40674528e-01 9.35603882e-01\n",
      "  3.39717063e-02 4.14580868e-01 6.14980099e-01 5.42602145e-01\n",
      "  7.55447746e-01 2.23746058e-01 4.93264793e-01 9.54484676e-01\n",
      "  8.64322868e-02 6.86282216e-01 7.55460800e-01 1.94823478e-01\n",
      "  2.51681866e-01 2.73423033e-01 4.60015613e-01 3.83414147e-01\n",
      "  5.09553353e-01 6.18529367e-01 8.51085098e-01 2.35295551e-01\n",
      "  4.01859715e-01 6.53855175e-01 6.34514338e-01]]\n",
      "Change in theta: 6.767161082446081e-07\n",
      "---------------------------------------------\n",
      "Iteration number 5 finished, Running time 290.0001368522644, Roo2 iteration 5 = -4810824.925136909\n",
      "Iteration 1:\n",
      "Theta: [[6.82558044e-01 2.04506282e-01 3.44611865e-01 5.75101730e-01\n",
      "  1.67783460e-01 3.75633027e-01 1.16340409e-01 8.83572447e-01\n",
      "  5.22027007e-01 3.47325808e-01 8.63870682e-01 1.62087472e-01\n",
      "  1.26206958e-01 5.95390869e-01 9.41467646e-01 7.64149945e-01\n",
      "  5.77479982e-01 7.73439549e-01 1.63597295e-01 7.90827570e-01\n",
      "  9.35523475e-02 2.68781697e-01 3.84662489e-01 4.97071746e-01\n",
      "  4.94974713e-01 6.36559976e-01 3.65342831e-01 2.17168185e-01\n",
      "  4.22674403e-01 8.48205595e-01 8.36619974e-01 9.86240978e-01\n",
      "  4.69573446e-01 5.17752707e-01 1.62437688e-02 8.68922789e-01\n",
      "  6.89852378e-01 6.26059132e-01 4.87913882e-01 5.22108067e-01\n",
      "  4.13589223e-01 4.41740355e-01 7.98421406e-01 4.11074478e-01\n",
      "  7.66469701e-01 2.83434605e-01 6.01807787e-01 7.80283553e-02\n",
      "  9.99837728e-01 2.20885468e-01 7.66503878e-01 6.18832493e-01\n",
      "  6.52105348e-01 9.73263858e-01 3.28890010e-01 2.16629515e-02\n",
      "  3.20330354e-01 1.12506208e-01 5.44334528e-02 9.88039511e-01\n",
      "  6.05648993e-02 8.64975366e-01 4.53165354e-02 4.81461525e-01\n",
      "  7.91193498e-01 3.88668380e-01 4.32164263e-01 7.85557694e-02\n",
      "  1.92376712e-01 9.14150107e-01 7.07710603e-01 5.38930664e-02\n",
      "  8.76078291e-01 5.85034871e-01 5.29683415e-01 9.12770491e-01\n",
      "  7.89507835e-01 6.30173447e-01 9.55821495e-01 4.74073869e-01\n",
      "  4.29735651e-01 9.38212505e-01 6.08023932e-01 7.48806732e-01\n",
      "  5.09947829e-01 8.82813297e-01 9.07719767e-01 5.37794096e-01\n",
      "  5.87232204e-01 1.60256879e-01 8.63035866e-01 9.98938174e-02\n",
      "  9.15908821e-01 1.18088539e-01 5.79001786e-01 3.95702130e-01\n",
      "  2.48604708e-01 9.28295766e-01 2.19428270e-01 6.00680292e-02\n",
      "  9.26506809e-01 7.02455363e-01 4.80480291e-01 4.96193770e-01\n",
      "  6.62760005e-03 1.45501673e-01 4.15102642e-01 3.64996460e-02\n",
      "  7.29424589e-01 5.36866306e-01 3.39487271e-01 6.57784172e-01\n",
      "  1.25013215e-01 4.34474735e-01 8.53759522e-02 9.71349433e-01\n",
      "  2.27610490e-01 7.88496381e-01 8.18915439e-01 6.22091844e-01\n",
      "  9.20596065e-01 3.11772845e-01 1.55995560e-01 9.24297074e-01\n",
      "  5.82771592e-01 9.49781629e-01 3.92831209e-02 7.11904791e-01\n",
      "  5.12333306e-01 7.59393282e-01 8.43583232e-01 1.22658212e-01\n",
      "  4.76982379e-01 4.99911143e-01 3.46104870e-01 6.76113642e-01\n",
      "  1.99210514e-01 3.46409587e-01 7.67273836e-01 6.21520222e-02\n",
      "  4.65651498e-01 1.35873676e-01 2.74965199e-02 8.14431773e-01\n",
      "  5.12271445e-01 2.89708993e-01 2.26149976e-01 8.59417045e-01\n",
      "  8.31757142e-01 7.21016717e-01 2.36522282e-01 3.34398106e-01\n",
      "  5.17499896e-03 2.74510826e-01 9.88220800e-02 8.88429656e-01\n",
      "  6.15292947e-01 3.40341216e-01 8.85016698e-01 4.82969698e-01\n",
      "  6.52126003e-01 5.35456254e-01 8.15419802e-01 4.69717554e-01\n",
      "  9.84658091e-01 4.40973166e-02 8.85711297e-01 6.94554981e-01\n",
      "  8.33634838e-01 4.62190134e-01 6.11411374e-01 2.26846955e-01\n",
      "  7.93868448e-01 2.29900959e-01 4.08246002e-01 4.87993507e-01\n",
      "  3.36767412e-01 3.71358184e-01 6.57736875e-03 3.56337333e-01\n",
      "  7.18379907e-01 1.38369336e-01 9.48252268e-01 1.24263512e-01\n",
      "  8.18082734e-01 7.58716401e-01 5.16474659e-01 3.77478782e-01\n",
      "  1.32100381e-01 8.55626728e-01 9.48045525e-01 2.25124411e-01\n",
      "  5.16184258e-01 7.49094509e-01 6.40094537e-01 8.66481619e-01\n",
      "  1.47622027e-01 7.42592173e-01 6.45727002e-01 7.58008609e-01\n",
      "  8.00781942e-01 2.21025664e-01 6.03221304e-03 6.27635240e-01\n",
      "  2.51502108e-01 7.70826025e-01 1.97881082e-01 9.04957646e-01\n",
      "  8.38345377e-01 4.17764890e-01 7.56701698e-01 5.77366499e-01\n",
      "  9.48705793e-01 6.54343604e-01 5.91192433e-01 9.88748726e-01\n",
      "  5.36346163e-01 9.00699025e-01 1.28352262e-01 3.92125807e-01\n",
      "  9.80285647e-01 1.78490047e-01 5.28756232e-01 2.79109382e-01\n",
      "  8.00911723e-04 4.24794909e-01 3.87206872e-01 8.86472423e-01\n",
      "  3.16534219e-01 2.40247221e-01 1.78136620e-01 5.45035124e-01\n",
      "  9.51911857e-02 1.16786574e-01 4.73085333e-01 2.60763674e-02\n",
      "  9.36578146e-01 4.60443425e-01 9.32268159e-01 8.83174865e-01\n",
      "  6.57789755e-02 3.07749475e-01 1.89232012e-01 8.87232848e-01\n",
      "  6.49848615e-01 2.15827005e-01 1.20418061e-01 8.95803481e-01\n",
      "  9.55010244e-01 5.13454636e-01 2.84478423e-01 2.51480752e-01\n",
      "  6.40150796e-01 7.51355306e-01 3.00898700e-01 5.88133930e-01\n",
      "  4.87405886e-01 8.58448030e-01 1.42269098e-01 7.98681312e-01\n",
      "  2.73836966e-01 3.83825065e-01 8.75982155e-01 5.68015458e-01\n",
      "  9.93529495e-01 2.36360710e-01 1.41572889e-01 1.24059055e-01\n",
      "  5.27897542e-01 3.77254656e-01 1.40416167e-02 5.92213098e-01\n",
      "  2.26217544e-01 2.20721911e-01 9.22148160e-01 3.00992686e-01\n",
      "  8.47024054e-01 4.54749439e-01 7.00660050e-01 4.32762709e-01\n",
      "  2.82768535e-01 1.21136940e-02 9.12953789e-01 9.16734943e-02\n",
      "  4.80172047e-01 7.40652476e-01 1.07372387e-01 5.77268115e-01\n",
      "  2.73536722e-01 8.93708687e-01 8.18037958e-01 1.64490241e-01\n",
      "  3.99023612e-01 2.98305805e-01 1.90719822e-02 3.37441422e-01\n",
      "  8.73748654e-01 5.34781043e-01 1.41186534e-02 1.63421902e-01\n",
      "  7.28856629e-01 6.17543400e-01 5.02932554e-01 4.73815667e-01\n",
      "  9.99519701e-01 4.57360049e-02 1.03625967e-03 8.25566860e-01\n",
      "  3.13207660e-01 2.34128490e-02 5.74183896e-01 3.39226738e-01\n",
      "  2.72254365e-01 6.21513711e-01 7.02687048e-01 1.13271202e-01\n",
      "  4.60234603e-01 1.75212933e-01 8.24608445e-01 7.05432093e-02\n",
      "  6.04557562e-01 8.05296747e-01 9.78879274e-01 5.73991599e-01\n",
      "  5.04715505e-01 2.05040132e-01 2.04850649e-01 9.94115769e-01\n",
      "  4.66893851e-02 9.70103316e-01 9.72988219e-01 1.93444011e-01\n",
      "  9.06015168e-01 1.03401445e-01 2.73903800e-01 1.36015773e-01\n",
      "  2.00708029e-01 5.62360660e-01 6.69018643e-01 8.94433394e-01\n",
      "  1.83274451e-01 6.23303531e-01 3.17580579e-01 6.02577461e-01\n",
      "  1.42324449e-01 4.40889183e-01 1.35371901e-01 4.46404338e-01\n",
      "  8.82978147e-01 7.66563048e-01 8.33540705e-01 6.51903744e-01\n",
      "  4.96779800e-01 1.96698318e-01 4.71303670e-01 8.09262511e-01\n",
      "  9.68940436e-01 1.04452883e-01 9.82600258e-02 5.16398834e-02\n",
      "  5.56982035e-01 3.29716904e-01 4.34717834e-01 7.55816310e-01\n",
      "  8.99322692e-01 7.09511083e-01 2.51623280e-01 7.89569611e-01\n",
      "  1.51658326e-01 2.22333446e-01 2.34415567e-01 5.26786850e-01\n",
      "  7.36703367e-01 4.32938713e-01 2.72441532e-02 7.41943391e-01\n",
      "  5.81474979e-01 6.78584737e-01 3.52211554e-01 3.95184072e-01\n",
      "  2.58006777e-01 9.22838130e-01 5.25828475e-01 8.64671322e-01\n",
      "  8.70133236e-02 6.75836484e-01 1.13502929e-01 8.76526301e-01\n",
      "  8.34075144e-01 2.01379747e-01 1.08209600e-01 4.38344072e-01\n",
      "  2.80582395e-01 4.26063016e-01 7.02958811e-01 6.32346782e-01\n",
      "  3.66925215e-01 3.11650784e-01 3.82060210e-01 7.29461883e-01\n",
      "  9.68208439e-03 6.19712069e-02 3.53974141e-01 2.69948999e-01\n",
      "  9.66133601e-01 7.30312504e-01 9.43731199e-01 7.51381835e-01\n",
      "  5.49215340e-01 5.10236804e-01 6.98460713e-01 5.46663030e-01\n",
      "  4.81226326e-01 5.08627742e-01 1.50939425e-01 3.67507504e-01\n",
      "  8.24282573e-01 9.09517280e-01 9.44476420e-02 8.25091827e-01\n",
      "  2.40380335e-01 9.05162412e-01 3.45887773e-01 2.32391515e-01\n",
      "  2.50701021e-01 4.80542548e-02 9.59460369e-01 1.43117804e-01\n",
      "  9.60554797e-01 6.71615585e-01 3.57126946e-01 9.39030835e-01\n",
      "  3.73575723e-01 2.37509992e-01 1.69429965e-01 7.89357805e-01\n",
      "  1.27667952e-01 6.36378174e-01 2.12381111e-01 7.46950715e-01\n",
      "  6.92337784e-01 9.55233410e-01 1.11642247e-01 5.90097199e-01\n",
      "  7.51753783e-02 6.89081470e-01 1.23314547e-01 8.00789829e-01\n",
      "  4.69690357e-01 3.68737169e-01 5.13010186e-01 9.94894728e-01\n",
      "  9.26152759e-01 5.79548351e-01 5.40840473e-01 9.24860118e-01\n",
      "  4.96918941e-01 3.09723068e-01 9.76908980e-01 8.82764790e-01\n",
      "  8.42195278e-01 5.27513393e-02 5.77225180e-01 9.42858092e-01\n",
      "  9.52946721e-01 8.25806959e-02 9.22191310e-01 9.69042275e-02\n",
      "  6.48939651e-01 9.51222943e-01 1.90816189e-01 5.94339519e-01\n",
      "  8.19250278e-01 8.25571277e-01 9.12804058e-01 7.80920987e-01\n",
      "  1.40307531e-01 3.04250300e-01 1.01052005e-01 2.08799403e-01\n",
      "  8.76144473e-01 5.92028282e-01 6.43645436e-02 6.71929550e-02\n",
      "  8.62953029e-02 2.24988358e-01 6.52592400e-01 6.14417725e-01\n",
      "  8.17651412e-01 4.13456964e-01 5.53446202e-01 9.86143369e-01\n",
      "  9.98189282e-01 7.21719112e-01 6.89865052e-01 1.79571637e-01\n",
      "  3.67398720e-01 5.29386078e-01 9.16686736e-01 2.23342833e-01\n",
      "  1.03061524e-01 7.59469206e-01 5.64281240e-01 4.23042678e-01\n",
      "  6.15075256e-01 2.35897600e-01 8.00319419e-01 9.25090055e-01\n",
      "  8.50848300e-01 5.93753423e-01 1.46298184e-02 9.47498777e-01\n",
      "  1.23525424e-01 6.62990284e-01 5.01375722e-01 1.08906603e-01\n",
      "  1.02225062e-01 7.68035560e-01 4.78169088e-01 7.49365160e-02\n",
      "  6.96272934e-01 8.01889982e-02 4.29902346e-01 5.64890350e-01\n",
      "  3.91924200e-01 5.87244571e-01 4.26948811e-01 4.31593902e-01\n",
      "  6.36670349e-02 3.29820400e-01 1.59507262e-01 7.22764041e-01\n",
      "  1.45303327e-01 3.38373690e-01 8.64577024e-01 7.93469034e-02\n",
      "  8.82631822e-01 1.26544314e-01 4.54605136e-01 7.47057602e-01\n",
      "  8.38632580e-01 7.88224661e-01 4.87965366e-01 1.02017909e-01\n",
      "  7.53808563e-01 8.80487442e-01 5.26651753e-01 1.70816009e-01\n",
      "  4.59562985e-01 9.68475058e-01 3.58465339e-01 5.99278828e-01\n",
      "  9.17964798e-01 7.24701023e-01 1.48524169e-01 5.61812036e-01\n",
      "  8.79327244e-01 3.57883315e-01 9.23125989e-01 3.06260416e-01\n",
      "  1.99879313e-01 5.77185740e-01 5.64160686e-01 6.50660079e-01\n",
      "  8.99007787e-01 9.87200533e-01 1.40546813e-01 4.20653037e-01\n",
      "  8.65554438e-01 4.93311425e-01 8.35884759e-02 7.62218610e-01\n",
      "  8.09678851e-01 7.33135462e-01 8.53761261e-01 7.27774407e-01\n",
      "  7.21580201e-01 5.75459545e-01 1.85618477e-01 7.60925008e-01\n",
      "  8.68972265e-01 9.52510476e-01 1.98943870e-01 7.96122314e-01\n",
      "  7.70680916e-02 9.90865926e-01 1.97819606e-01 4.74288436e-02\n",
      "  4.47454208e-01 7.70369805e-01 3.76044209e-01 8.45116058e-01\n",
      "  3.03883641e-01 2.27202080e-01 7.00234050e-02 3.80339199e-01\n",
      "  7.32988592e-02 7.45883420e-01 9.68560472e-01 1.31805519e-01\n",
      "  1.19470659e-03 3.14187793e-01 4.05159084e-01 8.80283210e-01\n",
      "  6.47523335e-01 1.54609957e-01 1.79601393e-01 4.07182845e-01\n",
      "  6.54963467e-01 5.78997139e-01 2.89460509e-01 9.09061496e-01\n",
      "  9.27779374e-01 9.15098294e-01 1.25428122e-01 1.68073133e-01\n",
      "  5.24864147e-01 6.45061454e-01 5.74075889e-02 8.80087445e-01\n",
      "  4.66094356e-01 4.75778701e-01 9.24931497e-01 5.30035618e-01\n",
      "  3.95254869e-01 7.50702141e-01 2.51696755e-01 9.60972282e-01\n",
      "  6.68682917e-01 8.39505445e-01 9.94625737e-01 7.65493080e-01\n",
      "  5.10773118e-01 3.75948300e-01 4.98983355e-01 5.77119201e-02\n",
      "  7.28716672e-01 7.74311939e-01 7.94836190e-01 4.15561445e-01\n",
      "  4.42369700e-01 2.55196630e-01 6.40346585e-01 3.30213427e-01\n",
      "  1.65387831e-01 3.51070620e-01 6.43718069e-01 4.72075126e-01\n",
      "  2.43995188e-01 4.43184826e-01 5.10128734e-01 4.09446735e-01\n",
      "  2.07542424e-01 8.90950561e-01 9.23888953e-01 6.80899500e-01\n",
      "  1.56204720e-01 3.89105291e-01 7.27505291e-01 6.75581724e-01\n",
      "  8.96683973e-01 2.88794428e-01 5.00033105e-01 7.30103079e-01\n",
      "  1.90239153e-01 5.63491868e-01 2.45763199e-01 3.42273363e-01\n",
      "  4.26159886e-01 8.07062904e-01 4.93440662e-01 8.56474929e-02\n",
      "  7.99054312e-01 3.11790879e-01 7.64796667e-01 7.40958876e-01\n",
      "  4.86174281e-01 1.69085965e-01 4.74829592e-01 3.51387315e-01\n",
      "  9.34604473e-01 9.79400585e-01 3.23951137e-01 4.71700201e-01\n",
      "  6.93197768e-02 9.35376819e-01 5.14890629e-01 1.31685155e-01\n",
      "  2.70924027e-01 1.35212625e-01 9.95820746e-01 1.72688430e-01\n",
      "  4.46665002e-01 4.99652268e-01 8.38696363e-01 2.13980164e-01\n",
      "  9.03461762e-01 9.88369514e-01 8.00222287e-01 5.48043877e-01\n",
      "  5.24008100e-01 5.01577631e-01 1.10465078e-01 8.55206699e-01\n",
      "  8.68559024e-01 9.84716759e-01 7.90498969e-01 5.71453160e-01\n",
      "  2.88320252e-01 9.33084475e-01 8.21218768e-01 6.92272083e-01\n",
      "  2.08956952e-01 5.16255424e-01 7.86085897e-02 2.31566869e-01\n",
      "  9.92587062e-01 6.36631375e-01 2.47812203e-01 8.49971508e-01\n",
      "  3.60296053e-01 1.71870417e-01 1.25624860e-01 4.33505831e-01\n",
      "  3.68686360e-01 9.46469299e-01 7.36773976e-01 3.60586752e-01\n",
      "  8.07341980e-01 3.95631289e-01 4.06486440e-01 7.38372992e-01\n",
      "  8.54077287e-01 6.67329294e-01 7.23882465e-01 1.74514451e-01\n",
      "  9.23300121e-02 8.46737599e-01 3.91469607e-01 8.22145864e-01\n",
      "  2.63923048e-01 4.67583322e-01 2.50119378e-01 5.23050419e-01\n",
      "  2.45260075e-01 5.58052526e-01 1.28874591e-01 2.39186315e-02\n",
      "  3.29880494e-01 8.42633887e-01 4.12445314e-01 7.62064738e-01\n",
      "  1.83377815e-01 3.04312542e-01 5.46340842e-01 7.63140839e-01\n",
      "  8.09616070e-01 2.56442227e-01 9.98507395e-01 7.98148195e-02\n",
      "  7.79692841e-01 8.78989432e-01 6.58037761e-01 9.70723448e-01\n",
      "  1.45844459e-01 9.45138155e-01 5.79364367e-02 4.42615290e-01\n",
      "  7.66991300e-01 7.57224955e-02 1.64844399e-01 6.43919446e-01\n",
      "  5.57338878e-01 8.70971084e-01 2.54166601e-01 3.17410632e-02\n",
      "  3.30281501e-01 4.70592139e-02 8.03716879e-01 2.99262890e-01\n",
      "  7.07814071e-01 3.52620681e-01 3.39144347e-01 2.17980970e-01\n",
      "  3.35981170e-01 1.38953247e-02 7.24100777e-01 8.14938852e-01\n",
      "  2.41669348e-01 1.75018720e-01 2.95893962e-02 9.32508169e-01\n",
      "  4.65658089e-01 9.79155141e-01 8.04186995e-01 1.61053439e-01\n",
      "  3.98132284e-01 1.59918620e-01 4.77514006e-01 2.23924137e-02\n",
      "  7.50458047e-01 8.18715785e-01 8.06202252e-01 5.60800056e-01\n",
      "  9.13305836e-01 8.93195658e-01 2.04371231e-01 3.99959386e-01\n",
      "  6.82197275e-01 4.78406956e-01 2.04207892e-01 8.36640071e-01\n",
      "  7.14357590e-01 2.42734135e-01 6.69381441e-01 4.88086868e-01\n",
      "  9.52257313e-01 2.82844954e-01 9.38233079e-01 7.98668616e-01\n",
      "  8.46958416e-01 8.50857052e-01 2.85281688e-03 2.91991142e-01\n",
      "  7.72433918e-01 7.34945252e-01 4.99512490e-01 9.60710718e-01\n",
      "  2.69282944e-01 7.67278279e-02 8.13079326e-01 9.14836184e-01\n",
      "  8.52966373e-01 1.87448112e-01 9.12944045e-01 8.93455897e-01\n",
      "  1.53590112e-01 8.52523507e-01 4.12776941e-01 6.47747298e-01\n",
      "  4.37655093e-01 6.53451938e-01 8.43417857e-01 2.65393635e-01\n",
      "  2.97419632e-01 7.01262221e-01 6.26204929e-01 1.01213207e-01\n",
      "  2.28348781e-01 5.24518072e-01 6.52693059e-01 7.56413563e-01\n",
      "  6.24616803e-01 4.88468265e-01 7.96414879e-01 9.54907014e-01\n",
      "  3.26707688e-01 2.17743233e-01 8.58647676e-01 2.18611365e-01\n",
      "  9.66916375e-01 8.53103339e-01 3.38956142e-01 7.39851495e-01\n",
      "  9.03346943e-01 7.61583688e-02 9.34536261e-01 5.81166872e-02\n",
      "  5.65190241e-01 5.48706839e-01 2.58698175e-01 4.96639303e-01\n",
      "  9.55180504e-01 6.37761863e-01 4.51713501e-01 8.25708072e-01\n",
      "  6.62377604e-01 7.54698707e-01 8.38816054e-01 4.09199596e-01\n",
      "  6.80216337e-01 2.08232175e-01 7.07761150e-02 2.82824708e-01\n",
      "  3.36599094e-01 5.06689702e-01 2.40676447e-01 9.35605801e-01\n",
      "  3.39736252e-02 4.14582787e-01 6.14982018e-01 5.42604064e-01\n",
      "  7.55449665e-01 2.23747977e-01 4.93266712e-01 9.54486595e-01\n",
      "  8.64342058e-02 6.86284135e-01 7.55462719e-01 1.94825397e-01\n",
      "  2.51683785e-01 2.73424952e-01 4.60017532e-01 3.83416066e-01\n",
      "  5.09555272e-01 6.18531286e-01 8.51087017e-01 2.35297470e-01\n",
      "  4.01861634e-01 6.53857094e-01 6.34516257e-01]]\n",
      "Change in theta: 7.753625898200386e-07\n",
      "---------------------------------------------\n",
      "Iteration number 6 finished, Running time 242.73944401741028, Roo2 iteration 6 = -10099076.77124807\n",
      "Iteration 1:\n",
      "Theta: [[6.82559883e-01 2.04508120e-01 3.44613703e-01 5.75103568e-01\n",
      "  1.67785299e-01 3.75634865e-01 1.16342247e-01 8.83574285e-01\n",
      "  5.22028845e-01 3.47327646e-01 8.63872521e-01 1.62089310e-01\n",
      "  1.26208796e-01 5.95392707e-01 9.41469485e-01 7.64151783e-01\n",
      "  5.77481820e-01 7.73441388e-01 1.63599134e-01 7.90829408e-01\n",
      "  9.35541857e-02 2.68783535e-01 3.84664328e-01 4.97073584e-01\n",
      "  4.94976552e-01 6.36561814e-01 3.65344669e-01 2.17170023e-01\n",
      "  4.22676242e-01 8.48207433e-01 8.36621812e-01 9.86242817e-01\n",
      "  4.69575284e-01 5.17754546e-01 1.62456070e-02 8.68924627e-01\n",
      "  6.89854216e-01 6.26060971e-01 4.87915721e-01 5.22109905e-01\n",
      "  4.13591061e-01 4.41742193e-01 7.98423244e-01 4.11076316e-01\n",
      "  7.66471539e-01 2.83436444e-01 6.01809625e-01 7.80301936e-02\n",
      "  9.99839566e-01 2.20887307e-01 7.66505716e-01 6.18834332e-01\n",
      "  6.52107186e-01 9.73265696e-01 3.28891848e-01 2.16647898e-02\n",
      "  3.20332193e-01 1.12508046e-01 5.44352910e-02 9.88041349e-01\n",
      "  6.05667375e-02 8.64977204e-01 4.53183736e-02 4.81463363e-01\n",
      "  7.91195336e-01 3.88670218e-01 4.32166102e-01 7.85576077e-02\n",
      "  1.92378550e-01 9.14151945e-01 7.07712441e-01 5.38949046e-02\n",
      "  8.76080129e-01 5.85036709e-01 5.29685253e-01 9.12772329e-01\n",
      "  7.89509674e-01 6.30175286e-01 9.55823333e-01 4.74075708e-01\n",
      "  4.29737489e-01 9.38214344e-01 6.08025771e-01 7.48808570e-01\n",
      "  5.09949668e-01 8.82815135e-01 9.07721605e-01 5.37795934e-01\n",
      "  5.87234043e-01 1.60258717e-01 8.63037705e-01 9.98956557e-02\n",
      "  9.15910659e-01 1.18090377e-01 5.79003625e-01 3.95703968e-01\n",
      "  2.48606546e-01 9.28297604e-01 2.19430108e-01 6.00698675e-02\n",
      "  9.26508647e-01 7.02457201e-01 4.80482129e-01 4.96195608e-01\n",
      "  6.62943831e-03 1.45503511e-01 4.15104480e-01 3.65014842e-02\n",
      "  7.29426427e-01 5.36868144e-01 3.39489110e-01 6.57786010e-01\n",
      "  1.25015053e-01 4.34476573e-01 8.53777905e-02 9.71351271e-01\n",
      "  2.27612328e-01 7.88498219e-01 8.18917277e-01 6.22093682e-01\n",
      "  9.20597903e-01 3.11774683e-01 1.55997398e-01 9.24298912e-01\n",
      "  5.82773430e-01 9.49783467e-01 3.92849591e-02 7.11906630e-01\n",
      "  5.12335144e-01 7.59395120e-01 8.43585071e-01 1.22660050e-01\n",
      "  4.76984217e-01 4.99912981e-01 3.46106708e-01 6.76115480e-01\n",
      "  1.99212352e-01 3.46411425e-01 7.67275674e-01 6.21538604e-02\n",
      "  4.65653336e-01 1.35875514e-01 2.74983582e-02 8.14433612e-01\n",
      "  5.12273284e-01 2.89710831e-01 2.26151814e-01 8.59418883e-01\n",
      "  8.31758980e-01 7.21018555e-01 2.36524120e-01 3.34399945e-01\n",
      "  5.17683722e-03 2.74512664e-01 9.88239183e-02 8.88431494e-01\n",
      "  6.15294785e-01 3.40343054e-01 8.85018537e-01 4.82971536e-01\n",
      "  6.52127842e-01 5.35458093e-01 8.15421641e-01 4.69719392e-01\n",
      "  9.84659929e-01 4.40991549e-02 8.85713136e-01 6.94556819e-01\n",
      "  8.33636676e-01 4.62191972e-01 6.11413213e-01 2.26848793e-01\n",
      "  7.93870286e-01 2.29902797e-01 4.08247841e-01 4.87995345e-01\n",
      "  3.36769250e-01 3.71360023e-01 6.57920701e-03 3.56339171e-01\n",
      "  7.18381745e-01 1.38371174e-01 9.48254107e-01 1.24265350e-01\n",
      "  8.18084572e-01 7.58718239e-01 5.16476497e-01 3.77480620e-01\n",
      "  1.32102219e-01 8.55628566e-01 9.48047363e-01 2.25126250e-01\n",
      "  5.16186096e-01 7.49096347e-01 6.40096375e-01 8.66483457e-01\n",
      "  1.47623865e-01 7.42594011e-01 6.45728840e-01 7.58010448e-01\n",
      "  8.00783780e-01 2.21027502e-01 6.03405130e-03 6.27637078e-01\n",
      "  2.51503946e-01 7.70827864e-01 1.97882920e-01 9.04959484e-01\n",
      "  8.38347215e-01 4.17766728e-01 7.56703536e-01 5.77368337e-01\n",
      "  9.48707631e-01 6.54345442e-01 5.91194271e-01 9.88750565e-01\n",
      "  5.36348001e-01 9.00700863e-01 1.28354101e-01 3.92127645e-01\n",
      "  9.80287485e-01 1.78491885e-01 5.28758071e-01 2.79111220e-01\n",
      "  8.02749982e-04 4.24796747e-01 3.87208710e-01 8.86474261e-01\n",
      "  3.16536057e-01 2.40249059e-01 1.78138458e-01 5.45036963e-01\n",
      "  9.51930240e-02 1.16788412e-01 4.73087172e-01 2.60782056e-02\n",
      "  9.36579984e-01 4.60445264e-01 9.32269997e-01 8.83176703e-01\n",
      "  6.57808138e-02 3.07751313e-01 1.89233850e-01 8.87234687e-01\n",
      "  6.49850454e-01 2.15828844e-01 1.20419899e-01 8.95805320e-01\n",
      "  9.55012082e-01 5.13456474e-01 2.84480261e-01 2.51482590e-01\n",
      "  6.40152635e-01 7.51357145e-01 3.00900538e-01 5.88135768e-01\n",
      "  4.87407724e-01 8.58449868e-01 1.42270937e-01 7.98683150e-01\n",
      "  2.73838805e-01 3.83826904e-01 8.75983994e-01 5.68017296e-01\n",
      "  9.93531333e-01 2.36362549e-01 1.41574727e-01 1.24060893e-01\n",
      "  5.27899380e-01 3.77256494e-01 1.40434549e-02 5.92214937e-01\n",
      "  2.26219382e-01 2.20723749e-01 9.22149998e-01 3.00994524e-01\n",
      "  8.47025892e-01 4.54751278e-01 7.00661889e-01 4.32764547e-01\n",
      "  2.82770373e-01 1.21155323e-02 9.12955627e-01 9.16753325e-02\n",
      "  4.80173885e-01 7.40654315e-01 1.07374226e-01 5.77269953e-01\n",
      "  2.73538561e-01 8.93710526e-01 8.18039796e-01 1.64492079e-01\n",
      "  3.99025451e-01 2.98307643e-01 1.90738205e-02 3.37443261e-01\n",
      "  8.73750492e-01 5.34782881e-01 1.41204916e-02 1.63423741e-01\n",
      "  7.28858467e-01 6.17545238e-01 5.02934392e-01 4.73817506e-01\n",
      "  9.99521539e-01 4.57378432e-02 1.03809793e-03 8.25568698e-01\n",
      "  3.13209498e-01 2.34146872e-02 5.74185734e-01 3.39228576e-01\n",
      "  2.72256203e-01 6.21515549e-01 7.02688886e-01 1.13273040e-01\n",
      "  4.60236441e-01 1.75214771e-01 8.24610283e-01 7.05450476e-02\n",
      "  6.04559400e-01 8.05298586e-01 9.78881112e-01 5.73993437e-01\n",
      "  5.04717343e-01 2.05041971e-01 2.04852487e-01 9.94117607e-01\n",
      "  4.66912234e-02 9.70105155e-01 9.72990058e-01 1.93445849e-01\n",
      "  9.06017006e-01 1.03403283e-01 2.73905638e-01 1.36017611e-01\n",
      "  2.00709867e-01 5.62362499e-01 6.69020481e-01 8.94435233e-01\n",
      "  1.83276289e-01 6.23305369e-01 3.17582417e-01 6.02579299e-01\n",
      "  1.42326288e-01 4.40891021e-01 1.35373739e-01 4.46406177e-01\n",
      "  8.82979986e-01 7.66564887e-01 8.33542544e-01 6.51905582e-01\n",
      "  4.96781639e-01 1.96700156e-01 4.71305508e-01 8.09264349e-01\n",
      "  9.68942275e-01 1.04454721e-01 9.82618640e-02 5.16417217e-02\n",
      "  5.56983874e-01 3.29718742e-01 4.34719673e-01 7.55818148e-01\n",
      "  8.99324530e-01 7.09512921e-01 2.51625119e-01 7.89571449e-01\n",
      "  1.51660164e-01 2.22335285e-01 2.34417405e-01 5.26788688e-01\n",
      "  7.36705206e-01 4.32940551e-01 2.72459914e-02 7.41945229e-01\n",
      "  5.81476817e-01 6.78586576e-01 3.52213392e-01 3.95185910e-01\n",
      "  2.58008616e-01 9.22839968e-01 5.25830314e-01 8.64673160e-01\n",
      "  8.70151619e-02 6.75838323e-01 1.13504768e-01 8.76528139e-01\n",
      "  8.34076982e-01 2.01381586e-01 1.08211438e-01 4.38345910e-01\n",
      "  2.80584233e-01 4.26064854e-01 7.02960649e-01 6.32348620e-01\n",
      "  3.66927054e-01 3.11652622e-01 3.82062048e-01 7.29463722e-01\n",
      "  9.68392265e-03 6.19730452e-02 3.53975979e-01 2.69950837e-01\n",
      "  9.66135439e-01 7.30314343e-01 9.43733037e-01 7.51383673e-01\n",
      "  5.49217178e-01 5.10238643e-01 6.98462552e-01 5.46664868e-01\n",
      "  4.81228164e-01 5.08629580e-01 1.50941263e-01 3.67509343e-01\n",
      "  8.24284412e-01 9.09519118e-01 9.44494803e-02 8.25093665e-01\n",
      "  2.40382173e-01 9.05164251e-01 3.45889611e-01 2.32393353e-01\n",
      "  2.50702859e-01 4.80560931e-02 9.59462207e-01 1.43119642e-01\n",
      "  9.60556635e-01 6.71617423e-01 3.57128784e-01 9.39032673e-01\n",
      "  3.73577562e-01 2.37511831e-01 1.69431803e-01 7.89359644e-01\n",
      "  1.27669791e-01 6.36380013e-01 2.12382949e-01 7.46952553e-01\n",
      "  6.92339622e-01 9.55235248e-01 1.11644086e-01 5.90099037e-01\n",
      "  7.51772165e-02 6.89083308e-01 1.23316385e-01 8.00791668e-01\n",
      "  4.69692196e-01 3.68739007e-01 5.13012024e-01 9.94896566e-01\n",
      "  9.26154597e-01 5.79550189e-01 5.40842311e-01 9.24861957e-01\n",
      "  4.96920780e-01 3.09724906e-01 9.76910818e-01 8.82766628e-01\n",
      "  8.42197116e-01 5.27531776e-02 5.77227018e-01 9.42859930e-01\n",
      "  9.52948560e-01 8.25825341e-02 9.22193149e-01 9.69060658e-02\n",
      "  6.48941489e-01 9.51224782e-01 1.90818027e-01 5.94341358e-01\n",
      "  8.19252116e-01 8.25573115e-01 9.12805896e-01 7.80922826e-01\n",
      "  1.40309369e-01 3.04252139e-01 1.01053843e-01 2.08801241e-01\n",
      "  8.76146311e-01 5.92030121e-01 6.43663818e-02 6.71947933e-02\n",
      "  8.62971412e-02 2.24990196e-01 6.52594238e-01 6.14419563e-01\n",
      "  8.17653250e-01 4.13458802e-01 5.53448041e-01 9.86145207e-01\n",
      "  9.98191120e-01 7.21720950e-01 6.89866891e-01 1.79573476e-01\n",
      "  3.67400558e-01 5.29387916e-01 9.16688575e-01 2.23344672e-01\n",
      "  1.03063362e-01 7.59471045e-01 5.64283078e-01 4.23044516e-01\n",
      "  6.15077095e-01 2.35899439e-01 8.00321257e-01 9.25091893e-01\n",
      "  8.50850138e-01 5.93755262e-01 1.46316566e-02 9.47500615e-01\n",
      "  1.23527263e-01 6.62992122e-01 5.01377560e-01 1.08908441e-01\n",
      "  1.02226900e-01 7.68037398e-01 4.78170926e-01 7.49383543e-02\n",
      "  6.96274773e-01 8.01908364e-02 4.29904184e-01 5.64892188e-01\n",
      "  3.91926038e-01 5.87246409e-01 4.26950649e-01 4.31595740e-01\n",
      "  6.36688732e-02 3.29822239e-01 1.59509101e-01 7.22765879e-01\n",
      "  1.45305165e-01 3.38375528e-01 8.64578862e-01 7.93487417e-02\n",
      "  8.82633661e-01 1.26546152e-01 4.54606974e-01 7.47059441e-01\n",
      "  8.38634419e-01 7.88226499e-01 4.87967205e-01 1.02019747e-01\n",
      "  7.53810401e-01 8.80489280e-01 5.26653591e-01 1.70817847e-01\n",
      "  4.59564824e-01 9.68476896e-01 3.58467177e-01 5.99280666e-01\n",
      "  9.17966636e-01 7.24702862e-01 1.48526008e-01 5.61813875e-01\n",
      "  8.79329082e-01 3.57885153e-01 9.23127827e-01 3.06262254e-01\n",
      "  1.99881151e-01 5.77187578e-01 5.64162525e-01 6.50661917e-01\n",
      "  8.99009625e-01 9.87202372e-01 1.40548651e-01 4.20654875e-01\n",
      "  8.65556276e-01 4.93313264e-01 8.35903142e-02 7.62220448e-01\n",
      "  8.09680690e-01 7.33137300e-01 8.53763099e-01 7.27776246e-01\n",
      "  7.21582039e-01 5.75461383e-01 1.85620316e-01 7.60926846e-01\n",
      "  8.68974103e-01 9.52512314e-01 1.98945709e-01 7.96124152e-01\n",
      "  7.70699298e-02 9.90867764e-01 1.97821445e-01 4.74306819e-02\n",
      "  4.47456047e-01 7.70371644e-01 3.76046047e-01 8.45117896e-01\n",
      "  3.03885480e-01 2.27203918e-01 7.00252433e-02 3.80341037e-01\n",
      "  7.33006974e-02 7.45885258e-01 9.68562310e-01 1.31807357e-01\n",
      "  1.19654485e-03 3.14189631e-01 4.05160923e-01 8.80285048e-01\n",
      "  6.47525173e-01 1.54611795e-01 1.79603231e-01 4.07184683e-01\n",
      "  6.54965305e-01 5.78998977e-01 2.89462347e-01 9.09063334e-01\n",
      "  9.27781213e-01 9.15100132e-01 1.25429960e-01 1.68074972e-01\n",
      "  5.24865986e-01 6.45063292e-01 5.74094271e-02 8.80089283e-01\n",
      "  4.66096194e-01 4.75780539e-01 9.24933335e-01 5.30037457e-01\n",
      "  3.95256707e-01 7.50703979e-01 2.51698593e-01 9.60974120e-01\n",
      "  6.68684755e-01 8.39507283e-01 9.94627575e-01 7.65494918e-01\n",
      "  5.10774956e-01 3.75950139e-01 4.98985193e-01 5.77137583e-02\n",
      "  7.28718511e-01 7.74313777e-01 7.94838028e-01 4.15563283e-01\n",
      "  4.42371539e-01 2.55198468e-01 6.40348423e-01 3.30215265e-01\n",
      "  1.65389669e-01 3.51072458e-01 6.43719907e-01 4.72076964e-01\n",
      "  2.43997026e-01 4.43186664e-01 5.10130572e-01 4.09448573e-01\n",
      "  2.07544262e-01 8.90952399e-01 9.23890791e-01 6.80901338e-01\n",
      "  1.56206559e-01 3.89107129e-01 7.27507129e-01 6.75583562e-01\n",
      "  8.96685811e-01 2.88796266e-01 5.00034943e-01 7.30104917e-01\n",
      "  1.90240991e-01 5.63493707e-01 2.45765037e-01 3.42275201e-01\n",
      "  4.26161724e-01 8.07064742e-01 4.93442500e-01 8.56493311e-02\n",
      "  7.99056150e-01 3.11792717e-01 7.64798505e-01 7.40960715e-01\n",
      "  4.86176119e-01 1.69087803e-01 4.74831430e-01 3.51389153e-01\n",
      "  9.34606312e-01 9.79402423e-01 3.23952975e-01 4.71702039e-01\n",
      "  6.93216151e-02 9.35378657e-01 5.14892468e-01 1.31686993e-01\n",
      "  2.70925865e-01 1.35214464e-01 9.95822585e-01 1.72690268e-01\n",
      "  4.46666840e-01 4.99654106e-01 8.38698201e-01 2.13982003e-01\n",
      "  9.03463600e-01 9.88371352e-01 8.00224126e-01 5.48045715e-01\n",
      "  5.24009938e-01 5.01579469e-01 1.10466916e-01 8.55208538e-01\n",
      "  8.68560863e-01 9.84718598e-01 7.90500807e-01 5.71454998e-01\n",
      "  2.88322090e-01 9.33086314e-01 8.21220606e-01 6.92273922e-01\n",
      "  2.08958790e-01 5.16257262e-01 7.86104279e-02 2.31568707e-01\n",
      "  9.92588901e-01 6.36633213e-01 2.47814042e-01 8.49973346e-01\n",
      "  3.60297891e-01 1.71872255e-01 1.25626699e-01 4.33507669e-01\n",
      "  3.68688198e-01 9.46471137e-01 7.36775814e-01 3.60588591e-01\n",
      "  8.07343818e-01 3.95633127e-01 4.06488278e-01 7.38374830e-01\n",
      "  8.54079125e-01 6.67331132e-01 7.23884303e-01 1.74516289e-01\n",
      "  9.23318504e-02 8.46739438e-01 3.91471445e-01 8.22147702e-01\n",
      "  2.63924886e-01 4.67585160e-01 2.50121216e-01 5.23052258e-01\n",
      "  2.45261913e-01 5.58054364e-01 1.28876429e-01 2.39204697e-02\n",
      "  3.29882332e-01 8.42635726e-01 4.12447152e-01 7.62066576e-01\n",
      "  1.83379653e-01 3.04314381e-01 5.46342680e-01 7.63142678e-01\n",
      "  8.09617908e-01 2.56444065e-01 9.98509233e-01 7.98166577e-02\n",
      "  7.79694679e-01 8.78991270e-01 6.58039600e-01 9.70725286e-01\n",
      "  1.45846297e-01 9.45139994e-01 5.79382750e-02 4.42617129e-01\n",
      "  7.66993138e-01 7.57243338e-02 1.64846238e-01 6.43921284e-01\n",
      "  5.57340716e-01 8.70972923e-01 2.54168439e-01 3.17429014e-02\n",
      "  3.30283340e-01 4.70610522e-02 8.03718718e-01 2.99264728e-01\n",
      "  7.07815909e-01 3.52622519e-01 3.39146185e-01 2.17982808e-01\n",
      "  3.35983008e-01 1.38971629e-02 7.24102615e-01 8.14940691e-01\n",
      "  2.41671187e-01 1.75020558e-01 2.95912344e-02 9.32510007e-01\n",
      "  4.65659927e-01 9.79156979e-01 8.04188833e-01 1.61055277e-01\n",
      "  3.98134122e-01 1.59920458e-01 4.77515844e-01 2.23942520e-02\n",
      "  7.50459885e-01 8.18717623e-01 8.06204090e-01 5.60801895e-01\n",
      "  9.13307674e-01 8.93197496e-01 2.04373069e-01 3.99961224e-01\n",
      "  6.82199113e-01 4.78408794e-01 2.04209731e-01 8.36641909e-01\n",
      "  7.14359429e-01 2.42735974e-01 6.69383279e-01 4.88088706e-01\n",
      "  9.52259151e-01 2.82846793e-01 9.38234917e-01 7.98670454e-01\n",
      "  8.46960255e-01 8.50858890e-01 2.85465514e-03 2.91992980e-01\n",
      "  7.72435756e-01 7.34947090e-01 4.99514328e-01 9.60712556e-01\n",
      "  2.69284783e-01 7.67296662e-02 8.13081165e-01 9.14838023e-01\n",
      "  8.52968212e-01 1.87449950e-01 9.12945883e-01 8.93457736e-01\n",
      "  1.53591950e-01 8.52525345e-01 4.12778780e-01 6.47749136e-01\n",
      "  4.37656932e-01 6.53453777e-01 8.43419696e-01 2.65395473e-01\n",
      "  2.97421470e-01 7.01264059e-01 6.26206767e-01 1.01215045e-01\n",
      "  2.28350619e-01 5.24519910e-01 6.52694898e-01 7.56415402e-01\n",
      "  6.24618641e-01 4.88470103e-01 7.96416717e-01 9.54908852e-01\n",
      "  3.26709526e-01 2.17745072e-01 8.58649514e-01 2.18613204e-01\n",
      "  9.66918213e-01 8.53105177e-01 3.38957981e-01 7.39853333e-01\n",
      "  9.03348781e-01 7.61602071e-02 9.34538100e-01 5.81185255e-02\n",
      "  5.65192079e-01 5.48708677e-01 2.58700013e-01 4.96641141e-01\n",
      "  9.55182342e-01 6.37763701e-01 4.51715339e-01 8.25709910e-01\n",
      "  6.62379443e-01 7.54700545e-01 8.38817892e-01 4.09201434e-01\n",
      "  6.80218175e-01 2.08234013e-01 7.07779532e-02 2.82826546e-01\n",
      "  3.36600932e-01 5.06691541e-01 2.40678285e-01 9.35607639e-01\n",
      "  3.39754635e-02 4.14584626e-01 6.14983856e-01 5.42605902e-01\n",
      "  7.55451504e-01 2.23749815e-01 4.93268551e-01 9.54488433e-01\n",
      "  8.64360440e-02 6.86285973e-01 7.55464557e-01 1.94827235e-01\n",
      "  2.51685623e-01 2.73426790e-01 4.60019370e-01 3.83417904e-01\n",
      "  5.09557110e-01 6.18533124e-01 8.51088855e-01 2.35299308e-01\n",
      "  4.01863472e-01 6.53858933e-01 6.34518095e-01]]\n",
      "Change in theta: 8.761531523329247e-07\n",
      "---------------------------------------------\n",
      "Iteration number 7 finished, Running time 222.24440598487854, Roo2 iteration 7 = -16894883.00363069\n",
      "Iteration 1:\n",
      "Theta: [[6.82561661e-01 2.04509899e-01 3.44615482e-01 5.75105346e-01\n",
      "  1.67787077e-01 3.75636643e-01 1.16344025e-01 8.83576064e-01\n",
      "  5.22030624e-01 3.47329424e-01 8.63874299e-01 1.62091088e-01\n",
      "  1.26210574e-01 5.95394485e-01 9.41471263e-01 7.64153562e-01\n",
      "  5.77483598e-01 7.73443166e-01 1.63600912e-01 7.90831186e-01\n",
      "  9.35559641e-02 2.68785313e-01 3.84666106e-01 4.97075362e-01\n",
      "  4.94978330e-01 6.36563592e-01 3.65346448e-01 2.17171801e-01\n",
      "  4.22678020e-01 8.48209212e-01 8.36623591e-01 9.86244595e-01\n",
      "  4.69577063e-01 5.17756324e-01 1.62473854e-02 8.68926406e-01\n",
      "  6.89855995e-01 6.26062749e-01 4.87917499e-01 5.22111684e-01\n",
      "  4.13592839e-01 4.41743971e-01 7.98425023e-01 4.11078095e-01\n",
      "  7.66473318e-01 2.83438222e-01 6.01811403e-01 7.80319719e-02\n",
      "  9.99841344e-01 2.20889085e-01 7.66507494e-01 6.18836110e-01\n",
      "  6.52108964e-01 9.73267474e-01 3.28893626e-01 2.16665681e-02\n",
      "  3.20333971e-01 1.12509824e-01 5.44370694e-02 9.88043128e-01\n",
      "  6.05685159e-02 8.64978982e-01 4.53201520e-02 4.81465142e-01\n",
      "  7.91197115e-01 3.88671996e-01 4.32167880e-01 7.85593860e-02\n",
      "  1.92380329e-01 9.14153724e-01 7.07714219e-01 5.38966830e-02\n",
      "  8.76081908e-01 5.85038487e-01 5.29687031e-01 9.12774107e-01\n",
      "  7.89511452e-01 6.30177064e-01 9.55825111e-01 4.74077486e-01\n",
      "  4.29739268e-01 9.38216122e-01 6.08027549e-01 7.48810348e-01\n",
      "  5.09951446e-01 8.82816913e-01 9.07723383e-01 5.37797712e-01\n",
      "  5.87235821e-01 1.60260496e-01 8.63039483e-01 9.98974341e-02\n",
      "  9.15912438e-01 1.18092155e-01 5.79005403e-01 3.95705746e-01\n",
      "  2.48608324e-01 9.28299383e-01 2.19431887e-01 6.00716459e-02\n",
      "  9.26510426e-01 7.02458980e-01 4.80483907e-01 4.96197387e-01\n",
      "  6.63121666e-03 1.45505289e-01 4.15106259e-01 3.65032626e-02\n",
      "  7.29428206e-01 5.36869923e-01 3.39490888e-01 6.57787788e-01\n",
      "  1.25016831e-01 4.34478352e-01 8.53795688e-02 9.71353050e-01\n",
      "  2.27614107e-01 7.88499997e-01 8.18919055e-01 6.22095461e-01\n",
      "  9.20599681e-01 3.11776462e-01 1.55999177e-01 9.24300690e-01\n",
      "  5.82775209e-01 9.49785246e-01 3.92867375e-02 7.11908408e-01\n",
      "  5.12336922e-01 7.59396898e-01 8.43586849e-01 1.22661828e-01\n",
      "  4.76985996e-01 4.99914759e-01 3.46108487e-01 6.76117259e-01\n",
      "  1.99214130e-01 3.46413204e-01 7.67277453e-01 6.21556388e-02\n",
      "  4.65655114e-01 1.35877293e-01 2.75001365e-02 8.14435390e-01\n",
      "  5.12275062e-01 2.89712610e-01 2.26153593e-01 8.59420661e-01\n",
      "  8.31760758e-01 7.21020334e-01 2.36525899e-01 3.34401723e-01\n",
      "  5.17861557e-03 2.74514442e-01 9.88256967e-02 8.88433273e-01\n",
      "  6.15296564e-01 3.40344833e-01 8.85020315e-01 4.82973315e-01\n",
      "  6.52129620e-01 5.35459871e-01 8.15423419e-01 4.69721170e-01\n",
      "  9.84661708e-01 4.41009332e-02 8.85714914e-01 6.94558597e-01\n",
      "  8.33638454e-01 4.62193750e-01 6.11414991e-01 2.26850572e-01\n",
      "  7.93872064e-01 2.29904575e-01 4.08249619e-01 4.87997124e-01\n",
      "  3.36771028e-01 3.71361801e-01 6.58098537e-03 3.56340950e-01\n",
      "  7.18383524e-01 1.38372952e-01 9.48255885e-01 1.24267129e-01\n",
      "  8.18086350e-01 7.58720018e-01 5.16478275e-01 3.77482399e-01\n",
      "  1.32103997e-01 8.55630345e-01 9.48049141e-01 2.25128028e-01\n",
      "  5.16187875e-01 7.49098125e-01 6.40098153e-01 8.66485235e-01\n",
      "  1.47625643e-01 7.42595789e-01 6.45730619e-01 7.58012226e-01\n",
      "  8.00785558e-01 2.21029281e-01 6.03582965e-03 6.27638856e-01\n",
      "  2.51505725e-01 7.70829642e-01 1.97884698e-01 9.04961262e-01\n",
      "  8.38348994e-01 4.17768506e-01 7.56705315e-01 5.77370115e-01\n",
      "  9.48709409e-01 6.54347220e-01 5.91196049e-01 9.88752343e-01\n",
      "  5.36349779e-01 9.00702642e-01 1.28355879e-01 3.92129424e-01\n",
      "  9.80289263e-01 1.78493664e-01 5.28759849e-01 2.79112999e-01\n",
      "  8.04528337e-04 4.24798526e-01 3.87210488e-01 8.86476039e-01\n",
      "  3.16537836e-01 2.40250838e-01 1.78140236e-01 5.45038741e-01\n",
      "  9.51948023e-02 1.16790190e-01 4.73088950e-01 2.60799840e-02\n",
      "  9.36581763e-01 4.60447042e-01 9.32271776e-01 8.83178482e-01\n",
      "  6.57825921e-02 3.07753091e-01 1.89235629e-01 8.87236465e-01\n",
      "  6.49852232e-01 2.15830622e-01 1.20421678e-01 8.95807098e-01\n",
      "  9.55013860e-01 5.13458253e-01 2.84482040e-01 2.51484368e-01\n",
      "  6.40154413e-01 7.51358923e-01 3.00902316e-01 5.88137547e-01\n",
      "  4.87409503e-01 8.58451647e-01 1.42272715e-01 7.98684929e-01\n",
      "  2.73840583e-01 3.83828682e-01 8.75985772e-01 5.68019075e-01\n",
      "  9.93533112e-01 2.36364327e-01 1.41576505e-01 1.24062671e-01\n",
      "  5.27901158e-01 3.77258272e-01 1.40452333e-02 5.92216715e-01\n",
      "  2.26221161e-01 2.20725527e-01 9.22151776e-01 3.00996302e-01\n",
      "  8.47027671e-01 4.54753056e-01 7.00663667e-01 4.32766326e-01\n",
      "  2.82772152e-01 1.21173106e-02 9.12957405e-01 9.16771109e-02\n",
      "  4.80175664e-01 7.40656093e-01 1.07376004e-01 5.77271732e-01\n",
      "  2.73540339e-01 8.93712304e-01 8.18041575e-01 1.64493857e-01\n",
      "  3.99027229e-01 2.98309422e-01 1.90755988e-02 3.37445039e-01\n",
      "  8.73752271e-01 5.34784659e-01 1.41222700e-02 1.63425519e-01\n",
      "  7.28860246e-01 6.17547016e-01 5.02936171e-01 4.73819284e-01\n",
      "  9.99523317e-01 4.57396215e-02 1.03987628e-03 8.25570476e-01\n",
      "  3.13211277e-01 2.34164656e-02 5.74187513e-01 3.39230355e-01\n",
      "  2.72257982e-01 6.21517327e-01 7.02690664e-01 1.13274819e-01\n",
      "  4.60238219e-01 1.75216549e-01 8.24612061e-01 7.05468259e-02\n",
      "  6.04561179e-01 8.05300364e-01 9.78882890e-01 5.73995216e-01\n",
      "  5.04719122e-01 2.05043749e-01 2.04854266e-01 9.94119386e-01\n",
      "  4.66930017e-02 9.70106933e-01 9.72991836e-01 1.93447627e-01\n",
      "  9.06018784e-01 1.03405062e-01 2.73907417e-01 1.36019389e-01\n",
      "  2.00711646e-01 5.62364277e-01 6.69022259e-01 8.94437011e-01\n",
      "  1.83278068e-01 6.23307148e-01 3.17584195e-01 6.02581077e-01\n",
      "  1.42328066e-01 4.40892799e-01 1.35375517e-01 4.46407955e-01\n",
      "  8.82981764e-01 7.66566665e-01 8.33544322e-01 6.51907360e-01\n",
      "  4.96783417e-01 1.96701934e-01 4.71307287e-01 8.09266127e-01\n",
      "  9.68944053e-01 1.04456500e-01 9.82636424e-02 5.16435000e-02\n",
      "  5.56985652e-01 3.29720521e-01 4.34721451e-01 7.55819927e-01\n",
      "  8.99326308e-01 7.09514700e-01 2.51626897e-01 7.89573228e-01\n",
      "  1.51661943e-01 2.22337063e-01 2.34419183e-01 5.26790467e-01\n",
      "  7.36706984e-01 4.32942330e-01 2.72477698e-02 7.41947007e-01\n",
      "  5.81478596e-01 6.78588354e-01 3.52215171e-01 3.95187688e-01\n",
      "  2.58010394e-01 9.22841746e-01 5.25832092e-01 8.64674939e-01\n",
      "  8.70169402e-02 6.75840101e-01 1.13506546e-01 8.76529918e-01\n",
      "  8.34078760e-01 2.01383364e-01 1.08213216e-01 4.38347688e-01\n",
      "  2.80586011e-01 4.26066632e-01 7.02962427e-01 6.32350399e-01\n",
      "  3.66928832e-01 3.11654401e-01 3.82063826e-01 7.29465500e-01\n",
      "  9.68570101e-03 6.19748235e-02 3.53977758e-01 2.69952616e-01\n",
      "  9.66137218e-01 7.30316121e-01 9.43734815e-01 7.51385451e-01\n",
      "  5.49218957e-01 5.10240421e-01 6.98464330e-01 5.46666646e-01\n",
      "  4.81229943e-01 5.08631359e-01 1.50943041e-01 3.67511121e-01\n",
      "  8.24286190e-01 9.09520896e-01 9.44512586e-02 8.25095444e-01\n",
      "  2.40383952e-01 9.05166029e-01 3.45891390e-01 2.32395132e-01\n",
      "  2.50704637e-01 4.80578715e-02 9.59463985e-01 1.43121421e-01\n",
      "  9.60558414e-01 6.71619201e-01 3.57130563e-01 9.39034452e-01\n",
      "  3.73579340e-01 2.37513609e-01 1.69433581e-01 7.89361422e-01\n",
      "  1.27671569e-01 6.36381791e-01 2.12384728e-01 7.46954332e-01\n",
      "  6.92341400e-01 9.55237026e-01 1.11645864e-01 5.90100815e-01\n",
      "  7.51789949e-02 6.89085086e-01 1.23318163e-01 8.00793446e-01\n",
      "  4.69693974e-01 3.68740785e-01 5.13013803e-01 9.94898344e-01\n",
      "  9.26156375e-01 5.79551968e-01 5.40844089e-01 9.24863735e-01\n",
      "  4.96922558e-01 3.09726684e-01 9.76912596e-01 8.82768406e-01\n",
      "  8.42198895e-01 5.27549559e-02 5.77228796e-01 9.42861709e-01\n",
      "  9.52950338e-01 8.25843125e-02 9.22194927e-01 9.69078441e-02\n",
      "  6.48943268e-01 9.51226560e-01 1.90819805e-01 5.94343136e-01\n",
      "  8.19253894e-01 8.25574893e-01 9.12807675e-01 7.80924604e-01\n",
      "  1.40311147e-01 3.04253917e-01 1.01055621e-01 2.08803020e-01\n",
      "  8.76148089e-01 5.92031899e-01 6.43681602e-02 6.71965717e-02\n",
      "  8.62989196e-02 2.24991975e-01 6.52596017e-01 6.14421342e-01\n",
      "  8.17655028e-01 4.13460580e-01 5.53449819e-01 9.86146986e-01\n",
      "  9.98192899e-01 7.21722728e-01 6.89868669e-01 1.79575254e-01\n",
      "  3.67402337e-01 5.29389695e-01 9.16690353e-01 2.23346450e-01\n",
      "  1.03065140e-01 7.59472823e-01 5.64284856e-01 4.23046295e-01\n",
      "  6.15078873e-01 2.35901217e-01 8.00323036e-01 9.25093671e-01\n",
      "  8.50851916e-01 5.93757040e-01 1.46334350e-02 9.47502394e-01\n",
      "  1.23529041e-01 6.62993901e-01 5.01379338e-01 1.08910219e-01\n",
      "  1.02228679e-01 7.68039176e-01 4.78172705e-01 7.49401326e-02\n",
      "  6.96276551e-01 8.01926148e-02 4.29905963e-01 5.64893966e-01\n",
      "  3.91927817e-01 5.87248188e-01 4.26952427e-01 4.31597518e-01\n",
      "  6.36706515e-02 3.29824017e-01 1.59510879e-01 7.22767658e-01\n",
      "  1.45306944e-01 3.38377306e-01 8.64580640e-01 7.93505200e-02\n",
      "  8.82635439e-01 1.26547931e-01 4.54608753e-01 7.47061219e-01\n",
      "  8.38636197e-01 7.88228278e-01 4.87968983e-01 1.02021526e-01\n",
      "  7.53812179e-01 8.80491058e-01 5.26655370e-01 1.70819625e-01\n",
      "  4.59566602e-01 9.68478674e-01 3.58468956e-01 5.99282444e-01\n",
      "  9.17968414e-01 7.24704640e-01 1.48527786e-01 5.61815653e-01\n",
      "  8.79330860e-01 3.57886931e-01 9.23129606e-01 3.06264032e-01\n",
      "  1.99882929e-01 5.77189356e-01 5.64164303e-01 6.50663696e-01\n",
      "  8.99011404e-01 9.87204150e-01 1.40550430e-01 4.20656654e-01\n",
      "  8.65558055e-01 4.93315042e-01 8.35920925e-02 7.62222226e-01\n",
      "  8.09682468e-01 7.33139078e-01 8.53764877e-01 7.27778024e-01\n",
      "  7.21583817e-01 5.75463162e-01 1.85622094e-01 7.60928624e-01\n",
      "  8.68975881e-01 9.52514093e-01 1.98947487e-01 7.96125930e-01\n",
      "  7.70717082e-02 9.90869542e-01 1.97823223e-01 4.74324603e-02\n",
      "  4.47457825e-01 7.70373422e-01 3.76047826e-01 8.45119675e-01\n",
      "  3.03887258e-01 2.27205696e-01 7.00270216e-02 3.80342816e-01\n",
      "  7.33024758e-02 7.45887037e-01 9.68564089e-01 1.31809135e-01\n",
      "  1.19832320e-03 3.14191409e-01 4.05162701e-01 8.80286827e-01\n",
      "  6.47526952e-01 1.54613573e-01 1.79605009e-01 4.07186462e-01\n",
      "  6.54967083e-01 5.79000756e-01 2.89464126e-01 9.09065113e-01\n",
      "  9.27782991e-01 9.15101911e-01 1.25431739e-01 1.68076750e-01\n",
      "  5.24867764e-01 6.45065070e-01 5.74112055e-02 8.80091062e-01\n",
      "  4.66097973e-01 4.75782317e-01 9.24935114e-01 5.30039235e-01\n",
      "  3.95258485e-01 7.50705758e-01 2.51700372e-01 9.60975899e-01\n",
      "  6.68686534e-01 8.39509061e-01 9.94629354e-01 7.65496696e-01\n",
      "  5.10776735e-01 3.75951917e-01 4.98986972e-01 5.77155367e-02\n",
      "  7.28720289e-01 7.74315556e-01 7.94839807e-01 4.15565062e-01\n",
      "  4.42373317e-01 2.55200246e-01 6.40350202e-01 3.30217043e-01\n",
      "  1.65391448e-01 3.51074236e-01 6.43721685e-01 4.72078743e-01\n",
      "  2.43998805e-01 4.43188442e-01 5.10132350e-01 4.09450351e-01\n",
      "  2.07546040e-01 8.90954178e-01 9.23892570e-01 6.80903116e-01\n",
      "  1.56208337e-01 3.89108908e-01 7.27508907e-01 6.75585340e-01\n",
      "  8.96687589e-01 2.88798045e-01 5.00036721e-01 7.30106695e-01\n",
      "  1.90242770e-01 5.63495485e-01 2.45766816e-01 3.42276980e-01\n",
      "  4.26163502e-01 8.07066520e-01 4.93444279e-01 8.56511095e-02\n",
      "  7.99057928e-01 3.11794495e-01 7.64800284e-01 7.40962493e-01\n",
      "  4.86177897e-01 1.69089581e-01 4.74833209e-01 3.51390931e-01\n",
      "  9.34608090e-01 9.79404201e-01 3.23954753e-01 4.71703817e-01\n",
      "  6.93233935e-02 9.35380436e-01 5.14894246e-01 1.31688771e-01\n",
      "  2.70927643e-01 1.35216242e-01 9.95824363e-01 1.72692046e-01\n",
      "  4.46668618e-01 4.99655884e-01 8.38699979e-01 2.13983781e-01\n",
      "  9.03465379e-01 9.88373130e-01 8.00225904e-01 5.48047494e-01\n",
      "  5.24011717e-01 5.01581247e-01 1.10468694e-01 8.55210316e-01\n",
      "  8.68562641e-01 9.84720376e-01 7.90502585e-01 5.71456776e-01\n",
      "  2.88323868e-01 9.33088092e-01 8.21222384e-01 6.92275700e-01\n",
      "  2.08960568e-01 5.16259041e-01 7.86122063e-02 2.31570485e-01\n",
      "  9.92590679e-01 6.36634992e-01 2.47815820e-01 8.49975125e-01\n",
      "  3.60299669e-01 1.71874033e-01 1.25628477e-01 4.33509447e-01\n",
      "  3.68689976e-01 9.46472915e-01 7.36777593e-01 3.60590369e-01\n",
      "  8.07345596e-01 3.95634905e-01 4.06490057e-01 7.38376608e-01\n",
      "  8.54080903e-01 6.67332911e-01 7.23886082e-01 1.74518068e-01\n",
      "  9.23336287e-02 8.46741216e-01 3.91473224e-01 8.22149481e-01\n",
      "  2.63926665e-01 4.67586938e-01 2.50122995e-01 5.23054036e-01\n",
      "  2.45263691e-01 5.58056142e-01 1.28878207e-01 2.39222481e-02\n",
      "  3.29884111e-01 8.42637504e-01 4.12448930e-01 7.62068355e-01\n",
      "  1.83381432e-01 3.04316159e-01 5.46344459e-01 7.63144456e-01\n",
      "  8.09619686e-01 2.56445844e-01 9.98511011e-01 7.98184361e-02\n",
      "  7.79696458e-01 8.78993048e-01 6.58041378e-01 9.70727065e-01\n",
      "  1.45848075e-01 9.45141772e-01 5.79400533e-02 4.42618907e-01\n",
      "  7.66994917e-01 7.57261121e-02 1.64848016e-01 6.43923063e-01\n",
      "  5.57342494e-01 8.70974701e-01 2.54170217e-01 3.17446798e-02\n",
      "  3.30285118e-01 4.70628305e-02 8.03720496e-01 2.99266506e-01\n",
      "  7.07817688e-01 3.52624297e-01 3.39147963e-01 2.17984586e-01\n",
      "  3.35984787e-01 1.38989413e-02 7.24104394e-01 8.14942469e-01\n",
      "  2.41672965e-01 1.75022337e-01 2.95930128e-02 9.32511785e-01\n",
      "  4.65661706e-01 9.79158758e-01 8.04190612e-01 1.61057055e-01\n",
      "  3.98135900e-01 1.59922236e-01 4.77517622e-01 2.23960304e-02\n",
      "  7.50461663e-01 8.18719401e-01 8.06205869e-01 5.60803673e-01\n",
      "  9.13309452e-01 8.93199274e-01 2.04374847e-01 3.99963002e-01\n",
      "  6.82200892e-01 4.78410573e-01 2.04211509e-01 8.36643688e-01\n",
      "  7.14361207e-01 2.42737752e-01 6.69385057e-01 4.88090484e-01\n",
      "  9.52260930e-01 2.82848571e-01 9.38236696e-01 7.98672232e-01\n",
      "  8.46962033e-01 8.50860669e-01 2.85643350e-03 2.91994759e-01\n",
      "  7.72437535e-01 7.34948869e-01 4.99516107e-01 9.60714335e-01\n",
      "  2.69286561e-01 7.67314445e-02 8.13082943e-01 9.14839801e-01\n",
      "  8.52969990e-01 1.87451728e-01 9.12947662e-01 8.93459514e-01\n",
      "  1.53593728e-01 8.52527123e-01 4.12780558e-01 6.47750915e-01\n",
      "  4.37658710e-01 6.53455555e-01 8.43421474e-01 2.65397251e-01\n",
      "  2.97423248e-01 7.01265837e-01 6.26208545e-01 1.01216823e-01\n",
      "  2.28352398e-01 5.24521688e-01 6.52696676e-01 7.56417180e-01\n",
      "  6.24620419e-01 4.88471882e-01 7.96418496e-01 9.54910631e-01\n",
      "  3.26711304e-01 2.17746850e-01 8.58651292e-01 2.18614982e-01\n",
      "  9.66919991e-01 8.53106955e-01 3.38959759e-01 7.39855111e-01\n",
      "  9.03350559e-01 7.61619854e-02 9.34539878e-01 5.81203038e-02\n",
      "  5.65193857e-01 5.48710456e-01 2.58701792e-01 4.96642920e-01\n",
      "  9.55184120e-01 6.37765480e-01 4.51717117e-01 8.25711688e-01\n",
      "  6.62381221e-01 7.54702323e-01 8.38819670e-01 4.09203212e-01\n",
      "  6.80219954e-01 2.08235791e-01 7.07797316e-02 2.82828325e-01\n",
      "  3.36602710e-01 5.06693319e-01 2.40680064e-01 9.35609417e-01\n",
      "  3.39772419e-02 4.14586404e-01 6.14985635e-01 5.42607681e-01\n",
      "  7.55453282e-01 2.23751594e-01 4.93270329e-01 9.54490211e-01\n",
      "  8.64378224e-02 6.86287752e-01 7.55466335e-01 1.94829013e-01\n",
      "  2.51687401e-01 2.73428569e-01 4.60021148e-01 3.83419682e-01\n",
      "  5.09558888e-01 6.18534903e-01 8.51090634e-01 2.35301087e-01\n",
      "  4.01865251e-01 6.53860711e-01 6.34519873e-01]]\n",
      "Change in theta: 9.79518380068407e-07\n",
      "---------------------------------------------\n",
      "Iteration number 8 finished, Running time 253.1675000190735, Roo2 iteration 8 = -5660024.732786086\n",
      "Iteration 1:\n",
      "Theta: [[6.82563241e-01 2.04511478e-01 3.44617061e-01 5.75106926e-01\n",
      "  1.67788657e-01 3.75638223e-01 1.16345605e-01 8.83577643e-01\n",
      "  5.22032203e-01 3.47331004e-01 8.63875878e-01 1.62092668e-01\n",
      "  1.26212154e-01 5.95396065e-01 9.41472843e-01 7.64155141e-01\n",
      "  5.77485178e-01 7.73444745e-01 1.63602491e-01 7.90832766e-01\n",
      "  9.35575436e-02 2.68786893e-01 3.84667685e-01 4.97076942e-01\n",
      "  4.94979910e-01 6.36565172e-01 3.65348027e-01 2.17173381e-01\n",
      "  4.22679600e-01 8.48210791e-01 8.36625170e-01 9.86246175e-01\n",
      "  4.69578642e-01 5.17757904e-01 1.62489649e-02 8.68927985e-01\n",
      "  6.89857574e-01 6.26064328e-01 4.87919078e-01 5.22113263e-01\n",
      "  4.13594419e-01 4.41745551e-01 7.98426602e-01 4.11079674e-01\n",
      "  7.66474897e-01 2.83439801e-01 6.01812983e-01 7.80335514e-02\n",
      "  9.99842924e-01 2.20890665e-01 7.66509074e-01 6.18837690e-01\n",
      "  6.52110544e-01 9.73269054e-01 3.28895206e-01 2.16681476e-02\n",
      "  3.20335550e-01 1.12511404e-01 5.44386489e-02 9.88044707e-01\n",
      "  6.05700954e-02 8.64980562e-01 4.53217315e-02 4.81466721e-01\n",
      "  7.91198694e-01 3.88673576e-01 4.32169460e-01 7.85609655e-02\n",
      "  1.92381908e-01 9.14155303e-01 7.07715799e-01 5.38982625e-02\n",
      "  8.76083487e-01 5.85040067e-01 5.29688611e-01 9.12775687e-01\n",
      "  7.89513032e-01 6.30178644e-01 9.55826691e-01 4.74079065e-01\n",
      "  4.29740847e-01 9.38217701e-01 6.08029128e-01 7.48811928e-01\n",
      "  5.09953026e-01 8.82818493e-01 9.07724963e-01 5.37799292e-01\n",
      "  5.87237401e-01 1.60262075e-01 8.63041062e-01 9.98990136e-02\n",
      "  9.15914017e-01 1.18093735e-01 5.79006983e-01 3.95707326e-01\n",
      "  2.48609904e-01 9.28300962e-01 2.19433466e-01 6.00732254e-02\n",
      "  9.26512005e-01 7.02460559e-01 4.80485487e-01 4.96198966e-01\n",
      "  6.63279616e-03 1.45506869e-01 4.15107838e-01 3.65048421e-02\n",
      "  7.29429785e-01 5.36871502e-01 3.39492467e-01 6.57789368e-01\n",
      "  1.25018411e-01 4.34479931e-01 8.53811483e-02 9.71354629e-01\n",
      "  2.27615686e-01 7.88501577e-01 8.18920635e-01 6.22097040e-01\n",
      "  9.20601261e-01 3.11778041e-01 1.56000756e-01 9.24302270e-01\n",
      "  5.82776788e-01 9.49786825e-01 3.92883170e-02 7.11909988e-01\n",
      "  5.12338502e-01 7.59398478e-01 8.43588428e-01 1.22663408e-01\n",
      "  4.76987575e-01 4.99916339e-01 3.46110066e-01 6.76118838e-01\n",
      "  1.99215710e-01 3.46414783e-01 7.67279032e-01 6.21572183e-02\n",
      "  4.65656694e-01 1.35878872e-01 2.75017160e-02 8.14436970e-01\n",
      "  5.12276642e-01 2.89714189e-01 2.26155172e-01 8.59422241e-01\n",
      "  8.31762338e-01 7.21021913e-01 2.36527478e-01 3.34403303e-01\n",
      "  5.18019507e-03 2.74516022e-01 9.88272762e-02 8.88434852e-01\n",
      "  6.15298143e-01 3.40346412e-01 8.85021895e-01 4.82974894e-01\n",
      "  6.52131199e-01 5.35461451e-01 8.15424998e-01 4.69722750e-01\n",
      "  9.84663287e-01 4.41025127e-02 8.85716493e-01 6.94560177e-01\n",
      "  8.33640034e-01 4.62195330e-01 6.11416571e-01 2.26852151e-01\n",
      "  7.93873644e-01 2.29906155e-01 4.08251198e-01 4.87998703e-01\n",
      "  3.36772608e-01 3.71363380e-01 6.58256487e-03 3.56342529e-01\n",
      "  7.18385103e-01 1.38374532e-01 9.48257464e-01 1.24268708e-01\n",
      "  8.18087930e-01 7.58721597e-01 5.16479855e-01 3.77483978e-01\n",
      "  1.32105577e-01 8.55631924e-01 9.48050721e-01 2.25129608e-01\n",
      "  5.16189454e-01 7.49099705e-01 6.40099733e-01 8.66486815e-01\n",
      "  1.47627223e-01 7.42597369e-01 6.45732198e-01 7.58013806e-01\n",
      "  8.00787138e-01 2.21030860e-01 6.03740915e-03 6.27640436e-01\n",
      "  2.51507304e-01 7.70831221e-01 1.97886278e-01 9.04962842e-01\n",
      "  8.38350573e-01 4.17770086e-01 7.56706894e-01 5.77371695e-01\n",
      "  9.48710989e-01 6.54348800e-01 5.91197629e-01 9.88753922e-01\n",
      "  5.36351359e-01 9.00704221e-01 1.28357459e-01 3.92131003e-01\n",
      "  9.80290843e-01 1.78495243e-01 5.28761428e-01 2.79114578e-01\n",
      "  8.06107840e-04 4.24800105e-01 3.87212068e-01 8.86477619e-01\n",
      "  3.16539415e-01 2.40252417e-01 1.78141816e-01 5.45040321e-01\n",
      "  9.51963818e-02 1.16791770e-01 4.73090530e-01 2.60815635e-02\n",
      "  9.36583342e-01 4.60448621e-01 9.32273355e-01 8.83180061e-01\n",
      "  6.57841716e-02 3.07754671e-01 1.89237208e-01 8.87238044e-01\n",
      "  6.49853811e-01 2.15832201e-01 1.20423257e-01 8.95808677e-01\n",
      "  9.55015440e-01 5.13459832e-01 2.84483619e-01 2.51485948e-01\n",
      "  6.40155993e-01 7.51360502e-01 3.00903896e-01 5.88139126e-01\n",
      "  4.87411082e-01 8.58453226e-01 1.42274295e-01 7.98686508e-01\n",
      "  2.73842162e-01 3.83830262e-01 8.75987351e-01 5.68020654e-01\n",
      "  9.93534691e-01 2.36365906e-01 1.41578085e-01 1.24064251e-01\n",
      "  5.27902738e-01 3.77259852e-01 1.40468128e-02 5.92218294e-01\n",
      "  2.26222740e-01 2.20727107e-01 9.22153356e-01 3.00997882e-01\n",
      "  8.47029250e-01 4.54754636e-01 7.00665247e-01 4.32767905e-01\n",
      "  2.82773731e-01 1.21188901e-02 9.12958985e-01 9.16786904e-02\n",
      "  4.80177243e-01 7.40657672e-01 1.07377584e-01 5.77273311e-01\n",
      "  2.73541918e-01 8.93713883e-01 8.18043154e-01 1.64495437e-01\n",
      "  3.99028808e-01 2.98311001e-01 1.90771783e-02 3.37446618e-01\n",
      "  8.73753850e-01 5.34786239e-01 1.41238495e-02 1.63427098e-01\n",
      "  7.28861825e-01 6.17548596e-01 5.02937750e-01 4.73820864e-01\n",
      "  9.99524897e-01 4.57412010e-02 1.04145578e-03 8.25572056e-01\n",
      "  3.13212856e-01 2.34180451e-02 5.74189092e-01 3.39231934e-01\n",
      "  2.72259561e-01 6.21518907e-01 7.02692244e-01 1.13276398e-01\n",
      "  4.60239799e-01 1.75218129e-01 8.24613641e-01 7.05484054e-02\n",
      "  6.04562758e-01 8.05301943e-01 9.78884470e-01 5.73996795e-01\n",
      "  5.04720701e-01 2.05045329e-01 2.04855845e-01 9.94120965e-01\n",
      "  4.66945813e-02 9.70108512e-01 9.72993415e-01 1.93449207e-01\n",
      "  9.06020364e-01 1.03406641e-01 2.73908996e-01 1.36020969e-01\n",
      "  2.00713225e-01 5.62365857e-01 6.69023839e-01 8.94438590e-01\n",
      "  1.83279647e-01 6.23308727e-01 3.17585775e-01 6.02582657e-01\n",
      "  1.42329645e-01 4.40894379e-01 1.35377097e-01 4.46409534e-01\n",
      "  8.82983344e-01 7.66568244e-01 8.33545902e-01 6.51908940e-01\n",
      "  4.96784996e-01 1.96703514e-01 4.71308866e-01 8.09267707e-01\n",
      "  9.68945632e-01 1.04458079e-01 9.82652219e-02 5.16450795e-02\n",
      "  5.56987231e-01 3.29722100e-01 4.34723030e-01 7.55821506e-01\n",
      "  8.99327888e-01 7.09516279e-01 2.51628476e-01 7.89574807e-01\n",
      "  1.51663522e-01 2.22338642e-01 2.34420763e-01 5.26792046e-01\n",
      "  7.36708564e-01 4.32943909e-01 2.72493493e-02 7.41948587e-01\n",
      "  5.81480175e-01 6.78589934e-01 3.52216750e-01 3.95189268e-01\n",
      "  2.58011974e-01 9.22843326e-01 5.25833671e-01 8.64676518e-01\n",
      "  8.70185197e-02 6.75841680e-01 1.13508125e-01 8.76531497e-01\n",
      "  8.34080340e-01 2.01384944e-01 1.08214796e-01 4.38349268e-01\n",
      "  2.80587591e-01 4.26068212e-01 7.02964007e-01 6.32351978e-01\n",
      "  3.66930411e-01 3.11655980e-01 3.82065406e-01 7.29467080e-01\n",
      "  9.68728051e-03 6.19764030e-02 3.53979337e-01 2.69954195e-01\n",
      "  9.66138797e-01 7.30317700e-01 9.43736395e-01 7.51387031e-01\n",
      "  5.49220536e-01 5.10242000e-01 6.98465910e-01 5.46668226e-01\n",
      "  4.81231522e-01 5.08632938e-01 1.50944621e-01 3.67512701e-01\n",
      "  8.24287769e-01 9.09522476e-01 9.44528381e-02 8.25097023e-01\n",
      "  2.40385531e-01 9.05167609e-01 3.45892969e-01 2.32396711e-01\n",
      "  2.50706217e-01 4.80594510e-02 9.59465565e-01 1.43123000e-01\n",
      "  9.60559993e-01 6.71620781e-01 3.57132142e-01 9.39036031e-01\n",
      "  3.73580920e-01 2.37515188e-01 1.69435161e-01 7.89363001e-01\n",
      "  1.27673148e-01 6.36383371e-01 2.12386307e-01 7.46955911e-01\n",
      "  6.92342980e-01 9.55238606e-01 1.11647443e-01 5.90102395e-01\n",
      "  7.51805744e-02 6.89086666e-01 1.23319743e-01 8.00795026e-01\n",
      "  4.69695553e-01 3.68742365e-01 5.13015382e-01 9.94899924e-01\n",
      "  9.26157955e-01 5.79553547e-01 5.40845669e-01 9.24865314e-01\n",
      "  4.96924138e-01 3.09728264e-01 9.76914176e-01 8.82769986e-01\n",
      "  8.42200474e-01 5.27565354e-02 5.77230376e-01 9.42863288e-01\n",
      "  9.52951917e-01 8.25858920e-02 9.22196506e-01 9.69094236e-02\n",
      "  6.48944847e-01 9.51228140e-01 1.90821385e-01 5.94344715e-01\n",
      "  8.19255474e-01 8.25576473e-01 9.12809254e-01 7.80926183e-01\n",
      "  1.40312727e-01 3.04255496e-01 1.01057201e-01 2.08804599e-01\n",
      "  8.76149669e-01 5.92033479e-01 6.43697397e-02 6.71981512e-02\n",
      "  8.63004991e-02 2.24993554e-01 6.52597596e-01 6.14422921e-01\n",
      "  8.17656608e-01 4.13462160e-01 5.53451399e-01 9.86148565e-01\n",
      "  9.98194478e-01 7.21724308e-01 6.89870248e-01 1.79576833e-01\n",
      "  3.67403916e-01 5.29391274e-01 9.16691932e-01 2.23348029e-01\n",
      "  1.03066720e-01 7.59474402e-01 5.64286436e-01 4.23047874e-01\n",
      "  6.15080452e-01 2.35902796e-01 8.00324615e-01 9.25095251e-01\n",
      "  8.50853496e-01 5.93758620e-01 1.46350145e-02 9.47503973e-01\n",
      "  1.23530621e-01 6.62995480e-01 5.01380918e-01 1.08911799e-01\n",
      "  1.02230258e-01 7.68040756e-01 4.78174284e-01 7.49417121e-02\n",
      "  6.96278130e-01 8.01941943e-02 4.29907542e-01 5.64895546e-01\n",
      "  3.91929396e-01 5.87249767e-01 4.26954007e-01 4.31599098e-01\n",
      "  6.36722310e-02 3.29825596e-01 1.59512458e-01 7.22769237e-01\n",
      "  1.45308523e-01 3.38378886e-01 8.64582220e-01 7.93520995e-02\n",
      "  8.82637019e-01 1.26549510e-01 4.54610332e-01 7.47062799e-01\n",
      "  8.38637776e-01 7.88229857e-01 4.87970563e-01 1.02023105e-01\n",
      "  7.53813759e-01 8.80492638e-01 5.26656949e-01 1.70821205e-01\n",
      "  4.59568181e-01 9.68480254e-01 3.58470535e-01 5.99284024e-01\n",
      "  9.17969994e-01 7.24706220e-01 1.48529366e-01 5.61817232e-01\n",
      "  8.79332440e-01 3.57888511e-01 9.23131185e-01 3.06265612e-01\n",
      "  1.99884509e-01 5.77190936e-01 5.64165883e-01 6.50665275e-01\n",
      "  8.99012983e-01 9.87205729e-01 1.40552009e-01 4.20658233e-01\n",
      "  8.65559634e-01 4.93316622e-01 8.35936720e-02 7.62223806e-01\n",
      "  8.09684047e-01 7.33140658e-01 8.53766457e-01 7.27779603e-01\n",
      "  7.21585397e-01 5.75464741e-01 1.85623673e-01 7.60930204e-01\n",
      "  8.68977461e-01 9.52515672e-01 1.98949067e-01 7.96127510e-01\n",
      "  7.70732877e-02 9.90871122e-01 1.97824802e-01 4.74340398e-02\n",
      "  4.47459404e-01 7.70375002e-01 3.76049405e-01 8.45121254e-01\n",
      "  3.03888838e-01 2.27207276e-01 7.00286011e-02 3.80344395e-01\n",
      "  7.33040553e-02 7.45888616e-01 9.68565668e-01 1.31810715e-01\n",
      "  1.19990270e-03 3.14192989e-01 4.05164280e-01 8.80288406e-01\n",
      "  6.47528531e-01 1.54615153e-01 1.79606589e-01 4.07188041e-01\n",
      "  6.54968663e-01 5.79002335e-01 2.89465705e-01 9.09066692e-01\n",
      "  9.27784571e-01 9.15103490e-01 1.25433318e-01 1.68078330e-01\n",
      "  5.24869344e-01 6.45066650e-01 5.74127850e-02 8.80092641e-01\n",
      "  4.66099552e-01 4.75783897e-01 9.24936693e-01 5.30040815e-01\n",
      "  3.95260065e-01 7.50707337e-01 2.51701951e-01 9.60977478e-01\n",
      "  6.68688113e-01 8.39510641e-01 9.94630933e-01 7.65498276e-01\n",
      "  5.10778314e-01 3.75953496e-01 4.98988551e-01 5.77171162e-02\n",
      "  7.28721868e-01 7.74317135e-01 7.94841386e-01 4.15566641e-01\n",
      "  4.42374897e-01 2.55201826e-01 6.40351781e-01 3.30218623e-01\n",
      "  1.65393027e-01 3.51075816e-01 6.43723265e-01 4.72080322e-01\n",
      "  2.44000384e-01 4.43190022e-01 5.10133930e-01 4.09451931e-01\n",
      "  2.07547620e-01 8.90955757e-01 9.23894149e-01 6.80904696e-01\n",
      "  1.56209917e-01 3.89110487e-01 7.27510487e-01 6.75586920e-01\n",
      "  8.96689169e-01 2.88799624e-01 5.00038301e-01 7.30108275e-01\n",
      "  1.90244349e-01 5.63497064e-01 2.45768395e-01 3.42278559e-01\n",
      "  4.26165082e-01 8.07068100e-01 4.93445858e-01 8.56526890e-02\n",
      "  7.99059508e-01 3.11796075e-01 7.64801863e-01 7.40964072e-01\n",
      "  4.86179477e-01 1.69091161e-01 4.74834788e-01 3.51392511e-01\n",
      "  9.34609669e-01 9.79405781e-01 3.23956333e-01 4.71705397e-01\n",
      "  6.93249730e-02 9.35382015e-01 5.14895825e-01 1.31690351e-01\n",
      "  2.70929223e-01 1.35217821e-01 9.95825942e-01 1.72693626e-01\n",
      "  4.46670198e-01 4.99657464e-01 8.38701559e-01 2.13985360e-01\n",
      "  9.03466958e-01 9.88374710e-01 8.00227484e-01 5.48049073e-01\n",
      "  5.24013296e-01 5.01582827e-01 1.10470274e-01 8.55211896e-01\n",
      "  8.68564221e-01 9.84721955e-01 7.90504165e-01 5.71458356e-01\n",
      "  2.88325448e-01 9.33089671e-01 8.21223964e-01 6.92277280e-01\n",
      "  2.08962148e-01 5.16260620e-01 7.86137858e-02 2.31572065e-01\n",
      "  9.92592258e-01 6.36636571e-01 2.47817400e-01 8.49976704e-01\n",
      "  3.60301249e-01 1.71875613e-01 1.25630056e-01 4.33511027e-01\n",
      "  3.68691556e-01 9.46474495e-01 7.36779172e-01 3.60591948e-01\n",
      "  8.07347176e-01 3.95636485e-01 4.06491636e-01 7.38378188e-01\n",
      "  8.54082483e-01 6.67334490e-01 7.23887661e-01 1.74519647e-01\n",
      "  9.23352082e-02 8.46742795e-01 3.91474803e-01 8.22151060e-01\n",
      "  2.63928244e-01 4.67588518e-01 2.50124574e-01 5.23055615e-01\n",
      "  2.45265271e-01 5.58057722e-01 1.28879787e-01 2.39238276e-02\n",
      "  3.29885690e-01 8.42639083e-01 4.12450510e-01 7.62069934e-01\n",
      "  1.83383011e-01 3.04317739e-01 5.46346038e-01 7.63146035e-01\n",
      "  8.09621266e-01 2.56447423e-01 9.98512591e-01 7.98200156e-02\n",
      "  7.79698037e-01 8.78994628e-01 6.58042958e-01 9.70728644e-01\n",
      "  1.45849655e-01 9.45143351e-01 5.79416328e-02 4.42620487e-01\n",
      "  7.66996496e-01 7.57276916e-02 1.64849595e-01 6.43924642e-01\n",
      "  5.57344074e-01 8.70976280e-01 2.54171797e-01 3.17462593e-02\n",
      "  3.30286698e-01 4.70644100e-02 8.03722076e-01 2.99268086e-01\n",
      "  7.07819267e-01 3.52625877e-01 3.39149543e-01 2.17986166e-01\n",
      "  3.35986366e-01 1.39005208e-02 7.24105973e-01 8.14944048e-01\n",
      "  2.41674544e-01 1.75023916e-01 2.95945923e-02 9.32513365e-01\n",
      "  4.65663285e-01 9.79160337e-01 8.04192191e-01 1.61058635e-01\n",
      "  3.98137480e-01 1.59923816e-01 4.77519202e-01 2.23976099e-02\n",
      "  7.50463243e-01 8.18720981e-01 8.06207448e-01 5.60805253e-01\n",
      "  9.13311032e-01 8.93200854e-01 2.04376427e-01 3.99964582e-01\n",
      "  6.82202471e-01 4.78412152e-01 2.04213088e-01 8.36645267e-01\n",
      "  7.14362786e-01 2.42739332e-01 6.69386637e-01 4.88092064e-01\n",
      "  9.52262509e-01 2.82850151e-01 9.38238275e-01 7.98673812e-01\n",
      "  8.46963612e-01 8.50862248e-01 2.85801300e-03 2.91996338e-01\n",
      "  7.72439114e-01 7.34950448e-01 4.99517686e-01 9.60715914e-01\n",
      "  2.69288141e-01 7.67330240e-02 8.13084522e-01 9.14841380e-01\n",
      "  8.52971570e-01 1.87453308e-01 9.12949241e-01 8.93461094e-01\n",
      "  1.53595308e-01 8.52528703e-01 4.12782138e-01 6.47752494e-01\n",
      "  4.37660290e-01 6.53457135e-01 8.43423054e-01 2.65398831e-01\n",
      "  2.97424828e-01 7.01267417e-01 6.26210125e-01 1.01218403e-01\n",
      "  2.28353977e-01 5.24523268e-01 6.52698256e-01 7.56418760e-01\n",
      "  6.24621999e-01 4.88473461e-01 7.96420075e-01 9.54912210e-01\n",
      "  3.26712884e-01 2.17748429e-01 8.58652872e-01 2.18616562e-01\n",
      "  9.66921571e-01 8.53108535e-01 3.38961338e-01 7.39856691e-01\n",
      "  9.03352139e-01 7.61635649e-02 9.34541457e-01 5.81218833e-02\n",
      "  5.65195437e-01 5.48712035e-01 2.58703371e-01 4.96644499e-01\n",
      "  9.55185700e-01 6.37767059e-01 4.51718697e-01 8.25713268e-01\n",
      "  6.62382801e-01 7.54703903e-01 8.38821250e-01 4.09204792e-01\n",
      "  6.80221533e-01 2.08237371e-01 7.07813111e-02 2.82829904e-01\n",
      "  3.36604290e-01 5.06694899e-01 2.40681643e-01 9.35610997e-01\n",
      "  3.39788214e-02 4.14587984e-01 6.14987214e-01 5.42609260e-01\n",
      "  7.55454861e-01 2.23753173e-01 4.93271908e-01 9.54491791e-01\n",
      "  8.64394019e-02 6.86289331e-01 7.55467915e-01 1.94830593e-01\n",
      "  2.51688981e-01 2.73430148e-01 4.60022728e-01 3.83421262e-01\n",
      "  5.09560468e-01 6.18536482e-01 8.51092213e-01 2.35302666e-01\n",
      "  4.01866830e-01 6.53862290e-01 6.34521453e-01]]\n",
      "Change in theta: 1.0761572469246158e-06\n",
      "---------------------------------------------\n",
      "Iteration number 9 finished, Running time 328.97541189193726, Roo2 iteration 9 = -14729590.578192798\n",
      "Iteration 1:\n",
      "Theta: [[6.82564678e-01 2.04512915e-01 3.44618499e-01 5.75108363e-01\n",
      "  1.67790094e-01 3.75639660e-01 1.16347042e-01 8.83579081e-01\n",
      "  5.22033640e-01 3.47332441e-01 8.63877316e-01 1.62094105e-01\n",
      "  1.26213591e-01 5.95397502e-01 9.41474280e-01 7.64156579e-01\n",
      "  5.77486615e-01 7.73446183e-01 1.63603929e-01 7.90834203e-01\n",
      "  9.35589809e-02 2.68788330e-01 3.84669123e-01 4.97078379e-01\n",
      "  4.94981347e-01 6.36566609e-01 3.65349465e-01 2.17174818e-01\n",
      "  4.22681037e-01 8.48212229e-01 8.36626608e-01 9.86247612e-01\n",
      "  4.69580079e-01 5.17759341e-01 1.62504022e-02 8.68929422e-01\n",
      "  6.89859011e-01 6.26065766e-01 4.87920516e-01 5.22114700e-01\n",
      "  4.13595856e-01 4.41746988e-01 7.98428039e-01 4.11081112e-01\n",
      "  7.66476334e-01 2.83441239e-01 6.01814420e-01 7.80349887e-02\n",
      "  9.99844361e-01 2.20892102e-01 7.66510511e-01 6.18839127e-01\n",
      "  6.52111981e-01 9.73270491e-01 3.28896643e-01 2.16695850e-02\n",
      "  3.20336988e-01 1.12512841e-01 5.44400862e-02 9.88046145e-01\n",
      "  6.05715327e-02 8.64981999e-01 4.53231688e-02 4.81468158e-01\n",
      "  7.91200131e-01 3.88675013e-01 4.32170897e-01 7.85624028e-02\n",
      "  1.92383345e-01 9.14156740e-01 7.07717236e-01 5.38996998e-02\n",
      "  8.76084925e-01 5.85041504e-01 5.29690048e-01 9.12777124e-01\n",
      "  7.89514469e-01 6.30180081e-01 9.55828128e-01 4.74080503e-01\n",
      "  4.29742285e-01 9.38219139e-01 6.08030566e-01 7.48813365e-01\n",
      "  5.09954463e-01 8.82819930e-01 9.07726400e-01 5.37800729e-01\n",
      "  5.87238838e-01 1.60263513e-01 8.63042500e-01 9.99004509e-02\n",
      "  9.15915455e-01 1.18095172e-01 5.79008420e-01 3.95708763e-01\n",
      "  2.48611341e-01 9.28302400e-01 2.19434903e-01 6.00746627e-02\n",
      "  9.26513442e-01 7.02461997e-01 4.80486924e-01 4.96200404e-01\n",
      "  6.63423348e-03 1.45508306e-01 4.15109276e-01 3.65062794e-02\n",
      "  7.29431222e-01 5.36872939e-01 3.39493905e-01 6.57790805e-01\n",
      "  1.25019848e-01 4.34481368e-01 8.53825856e-02 9.71356066e-01\n",
      "  2.27617123e-01 7.88503014e-01 8.18922072e-01 6.22098477e-01\n",
      "  9.20602698e-01 3.11779478e-01 1.56002193e-01 9.24303707e-01\n",
      "  5.82778226e-01 9.49788262e-01 3.92897543e-02 7.11911425e-01\n",
      "  5.12339939e-01 7.59399915e-01 8.43589866e-01 1.22664845e-01\n",
      "  4.76989013e-01 4.99917776e-01 3.46111504e-01 6.76120275e-01\n",
      "  1.99217147e-01 3.46416220e-01 7.67280469e-01 6.21586556e-02\n",
      "  4.65658131e-01 1.35880310e-01 2.75031533e-02 8.14438407e-01\n",
      "  5.12278079e-01 2.89715626e-01 2.26156609e-01 8.59423678e-01\n",
      "  8.31763775e-01 7.21023351e-01 2.36528916e-01 3.34404740e-01\n",
      "  5.18163239e-03 2.74517459e-01 9.88287135e-02 8.88436290e-01\n",
      "  6.15299580e-01 3.40347850e-01 8.85023332e-01 4.82976332e-01\n",
      "  6.52132637e-01 5.35462888e-01 8.15426436e-01 4.69724187e-01\n",
      "  9.84664725e-01 4.41039500e-02 8.85717931e-01 6.94561614e-01\n",
      "  8.33641471e-01 4.62196767e-01 6.11418008e-01 2.26853588e-01\n",
      "  7.93875081e-01 2.29907592e-01 4.08252636e-01 4.88000140e-01\n",
      "  3.36774045e-01 3.71364818e-01 6.58400219e-03 3.56343966e-01\n",
      "  7.18386541e-01 1.38375969e-01 9.48258902e-01 1.24270146e-01\n",
      "  8.18089367e-01 7.58723035e-01 5.16481292e-01 3.77485416e-01\n",
      "  1.32107014e-01 8.55633362e-01 9.48052158e-01 2.25131045e-01\n",
      "  5.16190892e-01 7.49101142e-01 6.40101170e-01 8.66488252e-01\n",
      "  1.47628660e-01 7.42598806e-01 6.45733636e-01 7.58015243e-01\n",
      "  8.00788575e-01 2.21032297e-01 6.03884647e-03 6.27641873e-01\n",
      "  2.51508741e-01 7.70832659e-01 1.97887715e-01 9.04964279e-01\n",
      "  8.38352010e-01 4.17771523e-01 7.56708332e-01 5.77373132e-01\n",
      "  9.48712426e-01 6.54350237e-01 5.91199066e-01 9.88755360e-01\n",
      "  5.36352796e-01 9.00705659e-01 1.28358896e-01 3.92132440e-01\n",
      "  9.80292280e-01 1.78496680e-01 5.28762866e-01 2.79116015e-01\n",
      "  8.07545155e-04 4.24801542e-01 3.87213505e-01 8.86479056e-01\n",
      "  3.16540853e-01 2.40253855e-01 1.78143253e-01 5.45041758e-01\n",
      "  9.51978192e-02 1.16793207e-01 4.73091967e-01 2.60830008e-02\n",
      "  9.36584779e-01 4.60450059e-01 9.32274793e-01 8.83181498e-01\n",
      "  6.57856089e-02 3.07756108e-01 1.89238645e-01 8.87239482e-01\n",
      "  6.49855249e-01 2.15833639e-01 1.20424694e-01 8.95810115e-01\n",
      "  9.55016877e-01 5.13461269e-01 2.84485057e-01 2.51487385e-01\n",
      "  6.40157430e-01 7.51361940e-01 3.00905333e-01 5.88140563e-01\n",
      "  4.87412519e-01 8.58454663e-01 1.42275732e-01 7.98687946e-01\n",
      "  2.73843600e-01 3.83831699e-01 8.75988789e-01 5.68022092e-01\n",
      "  9.93536129e-01 2.36367344e-01 1.41579522e-01 1.24065688e-01\n",
      "  5.27904175e-01 3.77261289e-01 1.40482501e-02 5.92219732e-01\n",
      "  2.26224177e-01 2.20728544e-01 9.22154793e-01 3.00999319e-01\n",
      "  8.47030687e-01 4.54756073e-01 7.00666684e-01 4.32769342e-01\n",
      "  2.82775169e-01 1.21203274e-02 9.12960422e-01 9.16801277e-02\n",
      "  4.80178680e-01 7.40659110e-01 1.07379021e-01 5.77274749e-01\n",
      "  2.73543356e-01 8.93715321e-01 8.18044591e-01 1.64496874e-01\n",
      "  3.99030246e-01 2.98312438e-01 1.90786157e-02 3.37448056e-01\n",
      "  8.73755287e-01 5.34787676e-01 1.41252868e-02 1.63428536e-01\n",
      "  7.28863262e-01 6.17550033e-01 5.02939187e-01 4.73822301e-01\n",
      "  9.99526334e-01 4.57426383e-02 1.04289310e-03 8.25573493e-01\n",
      "  3.13214294e-01 2.34194824e-02 5.74190530e-01 3.39233371e-01\n",
      "  2.72260998e-01 6.21520344e-01 7.02693681e-01 1.13277835e-01\n",
      "  4.60241236e-01 1.75219566e-01 8.24615078e-01 7.05498427e-02\n",
      "  6.04564195e-01 8.05303381e-01 9.78885907e-01 5.73998232e-01\n",
      "  5.04722139e-01 2.05046766e-01 2.04857282e-01 9.94122403e-01\n",
      "  4.66960186e-02 9.70109950e-01 9.72994853e-01 1.93450644e-01\n",
      "  9.06021801e-01 1.03408079e-01 2.73910434e-01 1.36022406e-01\n",
      "  2.00714663e-01 5.62367294e-01 6.69025276e-01 8.94440028e-01\n",
      "  1.83281085e-01 6.23310165e-01 3.17587212e-01 6.02584094e-01\n",
      "  1.42331083e-01 4.40895816e-01 1.35378534e-01 4.46410972e-01\n",
      "  8.82984781e-01 7.66569682e-01 8.33547339e-01 6.51910377e-01\n",
      "  4.96786434e-01 1.96704951e-01 4.71310304e-01 8.09269144e-01\n",
      "  9.68947070e-01 1.04459517e-01 9.82666592e-02 5.16465169e-02\n",
      "  5.56988669e-01 3.29723537e-01 4.34724468e-01 7.55822944e-01\n",
      "  8.99329325e-01 7.09517716e-01 2.51629914e-01 7.89576245e-01\n",
      "  1.51664960e-01 2.22340080e-01 2.34422200e-01 5.26793483e-01\n",
      "  7.36710001e-01 4.32945347e-01 2.72507866e-02 7.41950024e-01\n",
      "  5.81481612e-01 6.78591371e-01 3.52218187e-01 3.95190705e-01\n",
      "  2.58013411e-01 9.22844763e-01 5.25835109e-01 8.64677956e-01\n",
      "  8.70199570e-02 6.75843118e-01 1.13509563e-01 8.76532935e-01\n",
      "  8.34081777e-01 2.01386381e-01 1.08216233e-01 4.38350705e-01\n",
      "  2.80589028e-01 4.26069649e-01 7.02965444e-01 6.32353416e-01\n",
      "  3.66931849e-01 3.11657418e-01 3.82066843e-01 7.29468517e-01\n",
      "  9.68871783e-03 6.19778403e-02 3.53980775e-01 2.69955633e-01\n",
      "  9.66140234e-01 7.30319138e-01 9.43737832e-01 7.51388468e-01\n",
      "  5.49221973e-01 5.10243438e-01 6.98467347e-01 5.46669663e-01\n",
      "  4.81232960e-01 5.08634376e-01 1.50946058e-01 3.67514138e-01\n",
      "  8.24289207e-01 9.09523913e-01 9.44542754e-02 8.25098460e-01\n",
      "  2.40386969e-01 9.05169046e-01 3.45894407e-01 2.32398148e-01\n",
      "  2.50707654e-01 4.80608883e-02 9.59467002e-01 1.43124438e-01\n",
      "  9.60561430e-01 6.71622218e-01 3.57133579e-01 9.39037468e-01\n",
      "  3.73582357e-01 2.37516626e-01 1.69436598e-01 7.89364439e-01\n",
      "  1.27674586e-01 6.36384808e-01 2.12387744e-01 7.46957349e-01\n",
      "  6.92344417e-01 9.55240043e-01 1.11648881e-01 5.90103832e-01\n",
      "  7.51820117e-02 6.89088103e-01 1.23321180e-01 8.00796463e-01\n",
      "  4.69696991e-01 3.68743802e-01 5.13016820e-01 9.94901361e-01\n",
      "  9.26159392e-01 5.79554985e-01 5.40847106e-01 9.24866752e-01\n",
      "  4.96925575e-01 3.09729701e-01 9.76915613e-01 8.82771423e-01\n",
      "  8.42201911e-01 5.27579727e-02 5.77231813e-01 9.42864725e-01\n",
      "  9.52953355e-01 8.25873293e-02 9.22197944e-01 9.69108610e-02\n",
      "  6.48946284e-01 9.51229577e-01 1.90822822e-01 5.94346153e-01\n",
      "  8.19256911e-01 8.25577910e-01 9.12810691e-01 7.80927621e-01\n",
      "  1.40314164e-01 3.04256934e-01 1.01058638e-01 2.08806036e-01\n",
      "  8.76151106e-01 5.92034916e-01 6.43711770e-02 6.71995885e-02\n",
      "  8.63019364e-02 2.24994992e-01 6.52599034e-01 6.14424358e-01\n",
      "  8.17658045e-01 4.13463597e-01 5.53452836e-01 9.86150002e-01\n",
      "  9.98195915e-01 7.21725745e-01 6.89871686e-01 1.79578271e-01\n",
      "  3.67405353e-01 5.29392711e-01 9.16693370e-01 2.23349467e-01\n",
      "  1.03068157e-01 7.59475840e-01 5.64287873e-01 4.23049312e-01\n",
      "  6.15081890e-01 2.35904234e-01 8.00326052e-01 9.25096688e-01\n",
      "  8.50854933e-01 5.93760057e-01 1.46364518e-02 9.47505410e-01\n",
      "  1.23532058e-01 6.62996918e-01 5.01382355e-01 1.08913236e-01\n",
      "  1.02231695e-01 7.68042193e-01 4.78175721e-01 7.49431494e-02\n",
      "  6.96279568e-01 8.01956316e-02 4.29908980e-01 5.64896983e-01\n",
      "  3.91930834e-01 5.87251204e-01 4.26955444e-01 4.31600535e-01\n",
      "  6.36736683e-02 3.29827034e-01 1.59513896e-01 7.22770675e-01\n",
      "  1.45309960e-01 3.38380323e-01 8.64583657e-01 7.93535369e-02\n",
      "  8.82638456e-01 1.26550947e-01 4.54611770e-01 7.47064236e-01\n",
      "  8.38639214e-01 7.88231294e-01 4.87972000e-01 1.02024543e-01\n",
      "  7.53815196e-01 8.80494075e-01 5.26658386e-01 1.70822642e-01\n",
      "  4.59569619e-01 9.68481691e-01 3.58471973e-01 5.99285461e-01\n",
      "  9.17971431e-01 7.24707657e-01 1.48530803e-01 5.61818670e-01\n",
      "  8.79333877e-01 3.57889948e-01 9.23132622e-01 3.06267049e-01\n",
      "  1.99885946e-01 5.77192373e-01 5.64167320e-01 6.50666713e-01\n",
      "  8.99014420e-01 9.87207167e-01 1.40553447e-01 4.20659671e-01\n",
      "  8.65561072e-01 4.93318059e-01 8.35951093e-02 7.62225243e-01\n",
      "  8.09685485e-01 7.33142095e-01 8.53767894e-01 7.27781041e-01\n",
      "  7.21586834e-01 5.75466179e-01 1.85625111e-01 7.60931641e-01\n",
      "  8.68978898e-01 9.52517109e-01 1.98950504e-01 7.96128947e-01\n",
      "  7.70747250e-02 9.90872559e-01 1.97826240e-01 4.74354771e-02\n",
      "  4.47460842e-01 7.70376439e-01 3.76050842e-01 8.45122691e-01\n",
      "  3.03890275e-01 2.27208713e-01 7.00300385e-02 3.80345833e-01\n",
      "  7.33054926e-02 7.45890054e-01 9.68567105e-01 1.31812152e-01\n",
      "  1.20134002e-03 3.14194426e-01 4.05165718e-01 8.80289844e-01\n",
      "  6.47529969e-01 1.54616590e-01 1.79608026e-01 4.07189479e-01\n",
      "  6.54970100e-01 5.79003773e-01 2.89467142e-01 9.09068129e-01\n",
      "  9.27786008e-01 9.15104928e-01 1.25434755e-01 1.68079767e-01\n",
      "  5.24870781e-01 6.45068087e-01 5.74142223e-02 8.80094078e-01\n",
      "  4.66100989e-01 4.75785334e-01 9.24938130e-01 5.30042252e-01\n",
      "  3.95261502e-01 7.50708774e-01 2.51703389e-01 9.60978915e-01\n",
      "  6.68689551e-01 8.39512078e-01 9.94632370e-01 7.65499713e-01\n",
      "  5.10779752e-01 3.75954934e-01 4.98989988e-01 5.77185535e-02\n",
      "  7.28723306e-01 7.74318573e-01 7.94842823e-01 4.15568079e-01\n",
      "  4.42376334e-01 2.55203263e-01 6.40353219e-01 3.30220060e-01\n",
      "  1.65394465e-01 3.51077253e-01 6.43724702e-01 4.72081759e-01\n",
      "  2.44001821e-01 4.43191459e-01 5.10135367e-01 4.09453368e-01\n",
      "  2.07549057e-01 8.90957194e-01 9.23895586e-01 6.80906133e-01\n",
      "  1.56211354e-01 3.89111925e-01 7.27511924e-01 6.75588357e-01\n",
      "  8.96690606e-01 2.88801062e-01 5.00039738e-01 7.30109712e-01\n",
      "  1.90245786e-01 5.63498502e-01 2.45769832e-01 3.42279996e-01\n",
      "  4.26166519e-01 8.07069537e-01 4.93447295e-01 8.56541263e-02\n",
      "  7.99060945e-01 3.11797512e-01 7.64803300e-01 7.40965510e-01\n",
      "  4.86180914e-01 1.69092598e-01 4.74836225e-01 3.51393948e-01\n",
      "  9.34611107e-01 9.79407218e-01 3.23957770e-01 4.71706834e-01\n",
      "  6.93264103e-02 9.35383453e-01 5.14897263e-01 1.31691788e-01\n",
      "  2.70930660e-01 1.35219259e-01 9.95827380e-01 1.72695063e-01\n",
      "  4.46671635e-01 4.99658901e-01 8.38702996e-01 2.13986798e-01\n",
      "  9.03468396e-01 9.88376147e-01 8.00228921e-01 5.48050511e-01\n",
      "  5.24014734e-01 5.01584264e-01 1.10471711e-01 8.55213333e-01\n",
      "  8.68565658e-01 9.84723393e-01 7.90505602e-01 5.71459793e-01\n",
      "  2.88326885e-01 9.33091109e-01 8.21225401e-01 6.92278717e-01\n",
      "  2.08963585e-01 5.16262057e-01 7.86152231e-02 2.31573502e-01\n",
      "  9.92593696e-01 6.36638008e-01 2.47818837e-01 8.49978142e-01\n",
      "  3.60302686e-01 1.71877050e-01 1.25631494e-01 4.33512464e-01\n",
      "  3.68692993e-01 9.46475932e-01 7.36780609e-01 3.60593386e-01\n",
      "  8.07348613e-01 3.95637922e-01 4.06493073e-01 7.38379625e-01\n",
      "  8.54083920e-01 6.67335927e-01 7.23889098e-01 1.74521085e-01\n",
      "  9.23366455e-02 8.46744233e-01 3.91476241e-01 8.22152498e-01\n",
      "  2.63929681e-01 4.67589955e-01 2.50126012e-01 5.23057053e-01\n",
      "  2.45266708e-01 5.58059159e-01 1.28881224e-01 2.39252649e-02\n",
      "  3.29887128e-01 8.42640521e-01 4.12451947e-01 7.62071372e-01\n",
      "  1.83384448e-01 3.04319176e-01 5.46347475e-01 7.63147473e-01\n",
      "  8.09622703e-01 2.56448860e-01 9.98514028e-01 7.98214529e-02\n",
      "  7.79699474e-01 8.78996065e-01 6.58044395e-01 9.70730082e-01\n",
      "  1.45851092e-01 9.45144789e-01 5.79430702e-02 4.42621924e-01\n",
      "  7.66997933e-01 7.57291289e-02 1.64851033e-01 6.43926079e-01\n",
      "  5.57345511e-01 8.70977718e-01 2.54173234e-01 3.17476966e-02\n",
      "  3.30288135e-01 4.70658473e-02 8.03723513e-01 2.99269523e-01\n",
      "  7.07820704e-01 3.52627314e-01 3.39150980e-01 2.17987603e-01\n",
      "  3.35987804e-01 1.39019581e-02 7.24107411e-01 8.14945486e-01\n",
      "  2.41675982e-01 1.75025354e-01 2.95960296e-02 9.32514802e-01\n",
      "  4.65664722e-01 9.79161775e-01 8.04193629e-01 1.61060072e-01\n",
      "  3.98138917e-01 1.59925253e-01 4.77520639e-01 2.23990472e-02\n",
      "  7.50464680e-01 8.18722418e-01 8.06208886e-01 5.60806690e-01\n",
      "  9.13312469e-01 8.93202291e-01 2.04377864e-01 3.99966019e-01\n",
      "  6.82203908e-01 4.78413590e-01 2.04214526e-01 8.36646704e-01\n",
      "  7.14364224e-01 2.42740769e-01 6.69388074e-01 4.88093501e-01\n",
      "  9.52263946e-01 2.82851588e-01 9.38239713e-01 7.98675249e-01\n",
      "  8.46965050e-01 8.50863686e-01 2.85945031e-03 2.91997776e-01\n",
      "  7.72440551e-01 7.34951886e-01 4.99519124e-01 9.60717351e-01\n",
      "  2.69289578e-01 7.67344613e-02 8.13085960e-01 9.14842818e-01\n",
      "  8.52973007e-01 1.87454745e-01 9.12950678e-01 8.93462531e-01\n",
      "  1.53596745e-01 8.52530140e-01 4.12783575e-01 6.47753932e-01\n",
      "  4.37661727e-01 6.53458572e-01 8.43424491e-01 2.65400268e-01\n",
      "  2.97426265e-01 7.01268854e-01 6.26211562e-01 1.01219840e-01\n",
      "  2.28355415e-01 5.24524705e-01 6.52699693e-01 7.56420197e-01\n",
      "  6.24623436e-01 4.88474898e-01 7.96421513e-01 9.54913647e-01\n",
      "  3.26714321e-01 2.17749867e-01 8.58654309e-01 2.18617999e-01\n",
      "  9.66923008e-01 8.53109972e-01 3.38962776e-01 7.39858128e-01\n",
      "  9.03353576e-01 7.61650022e-02 9.34542895e-01 5.81233206e-02\n",
      "  5.65196874e-01 5.48713472e-01 2.58704809e-01 4.96645937e-01\n",
      "  9.55187137e-01 6.37768496e-01 4.51720134e-01 8.25714705e-01\n",
      "  6.62384238e-01 7.54705340e-01 8.38822687e-01 4.09206229e-01\n",
      "  6.80222970e-01 2.08238808e-01 7.07827484e-02 2.82831341e-01\n",
      "  3.36605727e-01 5.06696336e-01 2.40683080e-01 9.35612434e-01\n",
      "  3.39802587e-02 4.14589421e-01 6.14988651e-01 5.42610697e-01\n",
      "  7.55456299e-01 2.23754610e-01 4.93273346e-01 9.54493228e-01\n",
      "  8.64408392e-02 6.86290769e-01 7.55469352e-01 1.94832030e-01\n",
      "  2.51690418e-01 2.73431585e-01 4.60024165e-01 3.83422699e-01\n",
      "  5.09561905e-01 6.18537920e-01 8.51093651e-01 2.35304103e-01\n",
      "  4.01868268e-01 6.53863728e-01 6.34522890e-01]]\n",
      "Change in theta: 1.168046826305615e-06\n",
      "---------------------------------------------\n",
      "Iteration number 10 finished, Running time 273.9844229221344, Roo2 iteration 10 = -7225030.539656546\n",
      "Iteration 1:\n",
      "Theta: [[6.82565958e-01 2.04514195e-01 3.44619778e-01 5.75109643e-01\n",
      "  1.67791374e-01 3.75640940e-01 1.16348322e-01 8.83580361e-01\n",
      "  5.22034920e-01 3.47333721e-01 8.63878596e-01 1.62095385e-01\n",
      "  1.26214871e-01 5.95398782e-01 9.41475560e-01 7.64157858e-01\n",
      "  5.77487895e-01 7.73447463e-01 1.63605209e-01 7.90835483e-01\n",
      "  9.35602608e-02 2.68789610e-01 3.84670403e-01 4.97079659e-01\n",
      "  4.94982627e-01 6.36567889e-01 3.65350744e-01 2.17176098e-01\n",
      "  4.22682317e-01 8.48213508e-01 8.36627888e-01 9.86248892e-01\n",
      "  4.69581359e-01 5.17760621e-01 1.62516821e-02 8.68930702e-01\n",
      "  6.89860291e-01 6.26067046e-01 4.87921796e-01 5.22115980e-01\n",
      "  4.13597136e-01 4.41748268e-01 7.98429319e-01 4.11082391e-01\n",
      "  7.66477614e-01 2.83442519e-01 6.01815700e-01 7.80362686e-02\n",
      "  9.99845641e-01 2.20893382e-01 7.66511791e-01 6.18840407e-01\n",
      "  6.52113261e-01 9.73271771e-01 3.28897923e-01 2.16708649e-02\n",
      "  3.20338268e-01 1.12514121e-01 5.44413661e-02 9.88047424e-01\n",
      "  6.05728126e-02 8.64983279e-01 4.53244487e-02 4.81469438e-01\n",
      "  7.91201411e-01 3.88676293e-01 4.32172177e-01 7.85636827e-02\n",
      "  1.92384625e-01 9.14158020e-01 7.07718516e-01 5.39009797e-02\n",
      "  8.76086204e-01 5.85042784e-01 5.29691328e-01 9.12778404e-01\n",
      "  7.89515749e-01 6.30181361e-01 9.55829408e-01 4.74081783e-01\n",
      "  4.29743564e-01 9.38220419e-01 6.08031846e-01 7.48814645e-01\n",
      "  5.09955743e-01 8.82821210e-01 9.07727680e-01 5.37802009e-01\n",
      "  5.87240118e-01 1.60264792e-01 8.63043780e-01 9.99017308e-02\n",
      "  9.15916734e-01 1.18096452e-01 5.79009700e-01 3.95710043e-01\n",
      "  2.48612621e-01 9.28303679e-01 2.19436183e-01 6.00759426e-02\n",
      "  9.26514722e-01 7.02463276e-01 4.80488204e-01 4.96201683e-01\n",
      "  6.63551337e-03 1.45509586e-01 4.15110556e-01 3.65075593e-02\n",
      "  7.29432502e-01 5.36874219e-01 3.39495185e-01 6.57792085e-01\n",
      "  1.25021128e-01 4.34482648e-01 8.53838655e-02 9.71357346e-01\n",
      "  2.27618403e-01 7.88504294e-01 8.18923352e-01 6.22099757e-01\n",
      "  9.20603978e-01 3.11780758e-01 1.56003473e-01 9.24304987e-01\n",
      "  5.82779505e-01 9.49789542e-01 3.92910342e-02 7.11912705e-01\n",
      "  5.12341219e-01 7.59401195e-01 8.43591146e-01 1.22666125e-01\n",
      "  4.76990293e-01 4.99919056e-01 3.46112783e-01 6.76121555e-01\n",
      "  1.99218427e-01 3.46417500e-01 7.67281749e-01 6.21599355e-02\n",
      "  4.65659411e-01 1.35881589e-01 2.75044332e-02 8.14439687e-01\n",
      "  5.12279359e-01 2.89716906e-01 2.26157889e-01 8.59424958e-01\n",
      "  8.31765055e-01 7.21024631e-01 2.36530195e-01 3.34406020e-01\n",
      "  5.18291228e-03 2.74518739e-01 9.88299934e-02 8.88437570e-01\n",
      "  6.15300860e-01 3.40349129e-01 8.85024612e-01 4.82977611e-01\n",
      "  6.52133917e-01 5.35464168e-01 8.15427716e-01 4.69725467e-01\n",
      "  9.84666004e-01 4.41052299e-02 8.85719211e-01 6.94562894e-01\n",
      "  8.33642751e-01 4.62198047e-01 6.11419288e-01 2.26854868e-01\n",
      "  7.93876361e-01 2.29908872e-01 4.08253916e-01 4.88001420e-01\n",
      "  3.36775325e-01 3.71366098e-01 6.58528208e-03 3.56345246e-01\n",
      "  7.18387820e-01 1.38377249e-01 9.48260182e-01 1.24271425e-01\n",
      "  8.18090647e-01 7.58724314e-01 5.16482572e-01 3.77486695e-01\n",
      "  1.32108294e-01 8.55634641e-01 9.48053438e-01 2.25132325e-01\n",
      "  5.16192171e-01 7.49102422e-01 6.40102450e-01 8.66489532e-01\n",
      "  1.47629940e-01 7.42600086e-01 6.45734915e-01 7.58016523e-01\n",
      "  8.00789855e-01 2.21033577e-01 6.04012636e-03 6.27643153e-01\n",
      "  2.51510021e-01 7.70833939e-01 1.97888995e-01 9.04965559e-01\n",
      "  8.38353290e-01 4.17772803e-01 7.56709612e-01 5.77374412e-01\n",
      "  9.48713706e-01 6.54351517e-01 5.91200346e-01 9.88756640e-01\n",
      "  5.36354076e-01 9.00706938e-01 1.28360176e-01 3.92133720e-01\n",
      "  9.80293560e-01 1.78497960e-01 5.28764146e-01 2.79117295e-01\n",
      "  8.08825049e-04 4.24802822e-01 3.87214785e-01 8.86480336e-01\n",
      "  3.16542132e-01 2.40255134e-01 1.78144533e-01 5.45043038e-01\n",
      "  9.51990990e-02 1.16794487e-01 4.73093247e-01 2.60842807e-02\n",
      "  9.36586059e-01 4.60451339e-01 9.32276073e-01 8.83182778e-01\n",
      "  6.57868888e-02 3.07757388e-01 1.89239925e-01 8.87240762e-01\n",
      "  6.49856529e-01 2.15834919e-01 1.20425974e-01 8.95811395e-01\n",
      "  9.55018157e-01 5.13462549e-01 2.84486337e-01 2.51488665e-01\n",
      "  6.40158710e-01 7.51363220e-01 3.00906613e-01 5.88141843e-01\n",
      "  4.87413799e-01 8.58455943e-01 1.42277012e-01 7.98689225e-01\n",
      "  2.73844880e-01 3.83832979e-01 8.75990069e-01 5.68023372e-01\n",
      "  9.93537408e-01 2.36368624e-01 1.41580802e-01 1.24066968e-01\n",
      "  5.27905455e-01 3.77262569e-01 1.40495300e-02 5.92221012e-01\n",
      "  2.26225457e-01 2.20729824e-01 9.22156073e-01 3.01000599e-01\n",
      "  8.47031967e-01 4.54757353e-01 7.00667964e-01 4.32770622e-01\n",
      "  2.82776449e-01 1.21216073e-02 9.12961702e-01 9.16814076e-02\n",
      "  4.80179960e-01 7.40660390e-01 1.07380301e-01 5.77276028e-01\n",
      "  2.73544636e-01 8.93716601e-01 8.18045871e-01 1.64498154e-01\n",
      "  3.99031526e-01 2.98313718e-01 1.90798956e-02 3.37449336e-01\n",
      "  8.73756567e-01 5.34788956e-01 1.41265667e-02 1.63429816e-01\n",
      "  7.28864542e-01 6.17551313e-01 5.02940467e-01 4.73823581e-01\n",
      "  9.99527614e-01 4.57439182e-02 1.04417299e-03 8.25574773e-01\n",
      "  3.13215573e-01 2.34207623e-02 5.74191809e-01 3.39234651e-01\n",
      "  2.72262278e-01 6.21521624e-01 7.02694961e-01 1.13279115e-01\n",
      "  4.60242516e-01 1.75220846e-01 8.24616358e-01 7.05511226e-02\n",
      "  6.04565475e-01 8.05304661e-01 9.78887187e-01 5.73999512e-01\n",
      "  5.04723418e-01 2.05048046e-01 2.04858562e-01 9.94123682e-01\n",
      "  4.66972985e-02 9.70111230e-01 9.72996133e-01 1.93451924e-01\n",
      "  9.06023081e-01 1.03409358e-01 2.73911714e-01 1.36023686e-01\n",
      "  2.00715942e-01 5.62368574e-01 6.69026556e-01 8.94441308e-01\n",
      "  1.83282365e-01 6.23311444e-01 3.17588492e-01 6.02585374e-01\n",
      "  1.42332363e-01 4.40897096e-01 1.35379814e-01 4.46412252e-01\n",
      "  8.82986061e-01 7.66570962e-01 8.33548619e-01 6.51911657e-01\n",
      "  4.96787714e-01 1.96706231e-01 4.71311584e-01 8.09270424e-01\n",
      "  9.68948350e-01 1.04460796e-01 9.82679391e-02 5.16477968e-02\n",
      "  5.56989949e-01 3.29724817e-01 4.34725748e-01 7.55824224e-01\n",
      "  8.99330605e-01 7.09518996e-01 2.51631194e-01 7.89577524e-01\n",
      "  1.51666239e-01 2.22341360e-01 2.34423480e-01 5.26794763e-01\n",
      "  7.36711281e-01 4.32946626e-01 2.72520665e-02 7.41951304e-01\n",
      "  5.81482892e-01 6.78592651e-01 3.52219467e-01 3.95191985e-01\n",
      "  2.58014691e-01 9.22846043e-01 5.25836389e-01 8.64679235e-01\n",
      "  8.70212369e-02 6.75844398e-01 1.13510843e-01 8.76534214e-01\n",
      "  8.34083057e-01 2.01387661e-01 1.08217513e-01 4.38351985e-01\n",
      "  2.80590308e-01 4.26070929e-01 7.02966724e-01 6.32354696e-01\n",
      "  3.66933129e-01 3.11658698e-01 3.82068123e-01 7.29469797e-01\n",
      "  9.68999772e-03 6.19791202e-02 3.53982055e-01 2.69956912e-01\n",
      "  9.66141514e-01 7.30320418e-01 9.43739112e-01 7.51389748e-01\n",
      "  5.49223253e-01 5.10244718e-01 6.98468627e-01 5.46670943e-01\n",
      "  4.81234240e-01 5.08635656e-01 1.50947338e-01 3.67515418e-01\n",
      "  8.24290487e-01 9.09525193e-01 9.44555553e-02 8.25099740e-01\n",
      "  2.40388248e-01 9.05170326e-01 3.45895686e-01 2.32399428e-01\n",
      "  2.50708934e-01 4.80621682e-02 9.59468282e-01 1.43125718e-01\n",
      "  9.60562710e-01 6.71623498e-01 3.57134859e-01 9.39038748e-01\n",
      "  3.73583637e-01 2.37517906e-01 1.69437878e-01 7.89365719e-01\n",
      "  1.27675866e-01 6.36386088e-01 2.12389024e-01 7.46958628e-01\n",
      "  6.92345697e-01 9.55241323e-01 1.11650161e-01 5.90105112e-01\n",
      "  7.51832916e-02 6.89089383e-01 1.23322460e-01 8.00797743e-01\n",
      "  4.69698271e-01 3.68745082e-01 5.13018099e-01 9.94902641e-01\n",
      "  9.26160672e-01 5.79556265e-01 5.40848386e-01 9.24868032e-01\n",
      "  4.96926855e-01 3.09730981e-01 9.76916893e-01 8.82772703e-01\n",
      "  8.42203191e-01 5.27592526e-02 5.77233093e-01 9.42866005e-01\n",
      "  9.52954635e-01 8.25886092e-02 9.22199224e-01 9.69121409e-02\n",
      "  6.48947564e-01 9.51230857e-01 1.90824102e-01 5.94347433e-01\n",
      "  8.19258191e-01 8.25579190e-01 9.12811971e-01 7.80928901e-01\n",
      "  1.40315444e-01 3.04258214e-01 1.01059918e-01 2.08807316e-01\n",
      "  8.76152386e-01 5.92036196e-01 6.43724569e-02 6.72008684e-02\n",
      "  8.63032163e-02 2.24996271e-01 6.52600313e-01 6.14425638e-01\n",
      "  8.17659325e-01 4.13464877e-01 5.53454116e-01 9.86151282e-01\n",
      "  9.98197195e-01 7.21727025e-01 6.89872966e-01 1.79579551e-01\n",
      "  3.67406633e-01 5.29393991e-01 9.16694650e-01 2.23350747e-01\n",
      "  1.03069437e-01 7.59477120e-01 5.64289153e-01 4.23050592e-01\n",
      "  6.15083170e-01 2.35905514e-01 8.00327332e-01 9.25097968e-01\n",
      "  8.50856213e-01 5.93761337e-01 1.46377317e-02 9.47506690e-01\n",
      "  1.23533338e-01 6.62998197e-01 5.01383635e-01 1.08914516e-01\n",
      "  1.02232975e-01 7.68043473e-01 4.78177001e-01 7.49444293e-02\n",
      "  6.96280848e-01 8.01969115e-02 4.29910259e-01 5.64898263e-01\n",
      "  3.91932113e-01 5.87252484e-01 4.26956724e-01 4.31601815e-01\n",
      "  6.36749482e-02 3.29828314e-01 1.59515176e-01 7.22771954e-01\n",
      "  1.45311240e-01 3.38381603e-01 8.64584937e-01 7.93548168e-02\n",
      "  8.82639736e-01 1.26552227e-01 4.54613050e-01 7.47065516e-01\n",
      "  8.38640494e-01 7.88232574e-01 4.87973280e-01 1.02025822e-01\n",
      "  7.53816476e-01 8.80495355e-01 5.26659666e-01 1.70823922e-01\n",
      "  4.59570899e-01 9.68482971e-01 3.58473253e-01 5.99286741e-01\n",
      "  9.17972711e-01 7.24708937e-01 1.48532083e-01 5.61819950e-01\n",
      "  8.79335157e-01 3.57891228e-01 9.23133902e-01 3.06268329e-01\n",
      "  1.99887226e-01 5.77193653e-01 5.64168600e-01 6.50667993e-01\n",
      "  8.99015700e-01 9.87208447e-01 1.40554726e-01 4.20660951e-01\n",
      "  8.65562351e-01 4.93319339e-01 8.35963892e-02 7.62226523e-01\n",
      "  8.09686765e-01 7.33143375e-01 8.53769174e-01 7.27782321e-01\n",
      "  7.21588114e-01 5.75467459e-01 1.85626391e-01 7.60932921e-01\n",
      "  8.68980178e-01 9.52518389e-01 1.98951784e-01 7.96130227e-01\n",
      "  7.70760049e-02 9.90873839e-01 1.97827520e-01 4.74367570e-02\n",
      "  4.47462122e-01 7.70377719e-01 3.76052122e-01 8.45123971e-01\n",
      "  3.03891555e-01 2.27209993e-01 7.00313183e-02 3.80347113e-01\n",
      "  7.33067725e-02 7.45891333e-01 9.68568385e-01 1.31813432e-01\n",
      "  1.20261991e-03 3.14195706e-01 4.05166998e-01 8.80291124e-01\n",
      "  6.47531249e-01 1.54617870e-01 1.79609306e-01 4.07190758e-01\n",
      "  6.54971380e-01 5.79005052e-01 2.89468422e-01 9.09069409e-01\n",
      "  9.27787288e-01 9.15106208e-01 1.25436035e-01 1.68081047e-01\n",
      "  5.24872061e-01 6.45069367e-01 5.74155022e-02 8.80095358e-01\n",
      "  4.66102269e-01 4.75786614e-01 9.24939410e-01 5.30043532e-01\n",
      "  3.95262782e-01 7.50710054e-01 2.51704668e-01 9.60980195e-01\n",
      "  6.68690831e-01 8.39513358e-01 9.94633650e-01 7.65500993e-01\n",
      "  5.10781032e-01 3.75956214e-01 4.98991268e-01 5.77198334e-02\n",
      "  7.28724586e-01 7.74319852e-01 7.94844103e-01 4.15569359e-01\n",
      "  4.42377614e-01 2.55204543e-01 6.40354498e-01 3.30221340e-01\n",
      "  1.65395745e-01 3.51078533e-01 6.43725982e-01 4.72083039e-01\n",
      "  2.44003101e-01 4.43192739e-01 5.10136647e-01 4.09454648e-01\n",
      "  2.07550337e-01 8.90958474e-01 9.23896866e-01 6.80907413e-01\n",
      "  1.56212634e-01 3.89113204e-01 7.27513204e-01 6.75589637e-01\n",
      "  8.96691886e-01 2.88802341e-01 5.00041018e-01 7.30110992e-01\n",
      "  1.90247066e-01 5.63499782e-01 2.45771112e-01 3.42281276e-01\n",
      "  4.26167799e-01 8.07070817e-01 4.93448575e-01 8.56554062e-02\n",
      "  7.99062225e-01 3.11798792e-01 7.64804580e-01 7.40966790e-01\n",
      "  4.86182194e-01 1.69093878e-01 4.74837505e-01 3.51395228e-01\n",
      "  9.34612387e-01 9.79408498e-01 3.23959050e-01 4.71708114e-01\n",
      "  6.93276902e-02 9.35384732e-01 5.14898543e-01 1.31693068e-01\n",
      "  2.70931940e-01 1.35220539e-01 9.95828660e-01 1.72696343e-01\n",
      "  4.46672915e-01 4.99660181e-01 8.38704276e-01 2.13988078e-01\n",
      "  9.03469675e-01 9.88377427e-01 8.00230201e-01 5.48051791e-01\n",
      "  5.24016014e-01 5.01585544e-01 1.10472991e-01 8.55214613e-01\n",
      "  8.68566938e-01 9.84724673e-01 7.90506882e-01 5.71461073e-01\n",
      "  2.88328165e-01 9.33092389e-01 8.21226681e-01 6.92279997e-01\n",
      "  2.08964865e-01 5.16263337e-01 7.86165030e-02 2.31574782e-01\n",
      "  9.92594976e-01 6.36639288e-01 2.47820117e-01 8.49979421e-01\n",
      "  3.60303966e-01 1.71878330e-01 1.25632774e-01 4.33513744e-01\n",
      "  3.68694273e-01 9.46477212e-01 7.36781889e-01 3.60594666e-01\n",
      "  8.07349893e-01 3.95639202e-01 4.06494353e-01 7.38380905e-01\n",
      "  8.54085200e-01 6.67337207e-01 7.23890378e-01 1.74522364e-01\n",
      "  9.23379254e-02 8.46745513e-01 3.91477521e-01 8.22153778e-01\n",
      "  2.63930961e-01 4.67591235e-01 2.50127292e-01 5.23058333e-01\n",
      "  2.45267988e-01 5.58060439e-01 1.28882504e-01 2.39265448e-02\n",
      "  3.29888407e-01 8.42641801e-01 4.12453227e-01 7.62072651e-01\n",
      "  1.83385728e-01 3.04320456e-01 5.46348755e-01 7.63148753e-01\n",
      "  8.09623983e-01 2.56450140e-01 9.98515308e-01 7.98227328e-02\n",
      "  7.79700754e-01 8.78997345e-01 6.58045675e-01 9.70731361e-01\n",
      "  1.45852372e-01 9.45146069e-01 5.79443500e-02 4.42623204e-01\n",
      "  7.66999213e-01 7.57304088e-02 1.64852313e-01 6.43927359e-01\n",
      "  5.57346791e-01 8.70978998e-01 2.54174514e-01 3.17489765e-02\n",
      "  3.30289415e-01 4.70671272e-02 8.03724793e-01 2.99270803e-01\n",
      "  7.07821984e-01 3.52628594e-01 3.39152260e-01 2.17988883e-01\n",
      "  3.35989084e-01 1.39032380e-02 7.24108690e-01 8.14946766e-01\n",
      "  2.41677262e-01 1.75026634e-01 2.95973095e-02 9.32516082e-01\n",
      "  4.65666002e-01 9.79163055e-01 8.04194908e-01 1.61061352e-01\n",
      "  3.98140197e-01 1.59926533e-01 4.77521919e-01 2.24003271e-02\n",
      "  7.50465960e-01 8.18723698e-01 8.06210165e-01 5.60807970e-01\n",
      "  9.13313749e-01 8.93203571e-01 2.04379144e-01 3.99967299e-01\n",
      "  6.82205188e-01 4.78414869e-01 2.04215806e-01 8.36647984e-01\n",
      "  7.14365504e-01 2.42742049e-01 6.69389354e-01 4.88094781e-01\n",
      "  9.52265226e-01 2.82852868e-01 9.38240992e-01 7.98676529e-01\n",
      "  8.46966330e-01 8.50864966e-01 2.86073021e-03 2.91999055e-01\n",
      "  7.72441831e-01 7.34953165e-01 4.99520404e-01 9.60718631e-01\n",
      "  2.69290858e-01 7.67357412e-02 8.13087240e-01 9.14844098e-01\n",
      "  8.52974287e-01 1.87456025e-01 9.12951958e-01 8.93463811e-01\n",
      "  1.53598025e-01 8.52531420e-01 4.12784855e-01 6.47755212e-01\n",
      "  4.37663007e-01 6.53459852e-01 8.43425771e-01 2.65401548e-01\n",
      "  2.97427545e-01 7.01270134e-01 6.26212842e-01 1.01221120e-01\n",
      "  2.28356694e-01 5.24525985e-01 6.52700973e-01 7.56421477e-01\n",
      "  6.24624716e-01 4.88476178e-01 7.96422793e-01 9.54914927e-01\n",
      "  3.26715601e-01 2.17751147e-01 8.58655589e-01 2.18619279e-01\n",
      "  9.66924288e-01 8.53111252e-01 3.38964056e-01 7.39859408e-01\n",
      "  9.03354856e-01 7.61662821e-02 9.34544175e-01 5.81246005e-02\n",
      "  5.65198154e-01 5.48714752e-01 2.58706089e-01 4.96647216e-01\n",
      "  9.55188417e-01 6.37769776e-01 4.51721414e-01 8.25715985e-01\n",
      "  6.62385518e-01 7.54706620e-01 8.38823967e-01 4.09207509e-01\n",
      "  6.80224250e-01 2.08240088e-01 7.07840283e-02 2.82832621e-01\n",
      "  3.36607007e-01 5.06697616e-01 2.40684360e-01 9.35613714e-01\n",
      "  3.39815386e-02 4.14590701e-01 6.14989931e-01 5.42611977e-01\n",
      "  7.55457579e-01 2.23755890e-01 4.93274626e-01 9.54494508e-01\n",
      "  8.64421191e-02 6.86292048e-01 7.55470632e-01 1.94833310e-01\n",
      "  2.51691698e-01 2.73432865e-01 4.60025445e-01 3.83423979e-01\n",
      "  5.09563185e-01 6.18539199e-01 8.51094930e-01 2.35305383e-01\n",
      "  4.01869547e-01 6.53865008e-01 6.34524170e-01]]\n",
      "Change in theta: 1.2530405222672494e-06\n",
      "---------------------------------------------\n",
      "Iteration number 11 finished, Running time 250.5505120754242, Roo2 iteration 11 = -5658012.878251934\n",
      "Iteration 1:\n",
      "Theta: [[6.82567220e-01 2.04515458e-01 3.44621041e-01 5.75110906e-01\n",
      "  1.67792636e-01 3.75642203e-01 1.16349585e-01 8.83581623e-01\n",
      "  5.22036183e-01 3.47334984e-01 8.63879858e-01 1.62096647e-01\n",
      "  1.26216133e-01 5.95400045e-01 9.41476822e-01 7.64159121e-01\n",
      "  5.77489157e-01 7.73448725e-01 1.63606471e-01 7.90836746e-01\n",
      "  9.35615234e-02 2.68790873e-01 3.84671665e-01 4.97080922e-01\n",
      "  4.94983889e-01 6.36569152e-01 3.65352007e-01 2.17177361e-01\n",
      "  4.22683579e-01 8.48214771e-01 8.36629150e-01 9.86250154e-01\n",
      "  4.69582622e-01 5.17761883e-01 1.62529447e-02 8.68931965e-01\n",
      "  6.89861554e-01 6.26068308e-01 4.87923058e-01 5.22117243e-01\n",
      "  4.13598399e-01 4.41749530e-01 7.98430582e-01 4.11083654e-01\n",
      "  7.66478877e-01 2.83443781e-01 6.01816963e-01 7.80375312e-02\n",
      "  9.99846903e-01 2.20894644e-01 7.66513054e-01 6.18841669e-01\n",
      "  6.52114524e-01 9.73273034e-01 3.28899186e-01 2.16721274e-02\n",
      "  3.20339530e-01 1.12515384e-01 5.44426287e-02 9.88048687e-01\n",
      "  6.05740752e-02 8.64984542e-01 4.53257113e-02 4.81470701e-01\n",
      "  7.91202674e-01 3.88677556e-01 4.32173439e-01 7.85649453e-02\n",
      "  1.92385888e-01 9.14159283e-01 7.07719779e-01 5.39022423e-02\n",
      "  8.76087467e-01 5.85044047e-01 5.29692590e-01 9.12779666e-01\n",
      "  7.89517011e-01 6.30182623e-01 9.55830671e-01 4.74083045e-01\n",
      "  4.29744827e-01 9.38221681e-01 6.08033108e-01 7.48815907e-01\n",
      "  5.09957005e-01 8.82822473e-01 9.07728943e-01 5.37803271e-01\n",
      "  5.87241380e-01 1.60266055e-01 8.63045042e-01 9.99029934e-02\n",
      "  9.15917997e-01 1.18097715e-01 5.79010962e-01 3.95711306e-01\n",
      "  2.48613884e-01 9.28304942e-01 2.19437446e-01 6.00772052e-02\n",
      "  9.26515985e-01 7.02464539e-01 4.80489467e-01 4.96202946e-01\n",
      "  6.63677596e-03 1.45510849e-01 4.15111818e-01 3.65088219e-02\n",
      "  7.29433765e-01 5.36875482e-01 3.39496447e-01 6.57793348e-01\n",
      "  1.25022391e-01 4.34483911e-01 8.53851281e-02 9.71358609e-01\n",
      "  2.27619666e-01 7.88505557e-01 8.18924614e-01 6.22101020e-01\n",
      "  9.20605240e-01 3.11782021e-01 1.56004736e-01 9.24306250e-01\n",
      "  5.82780768e-01 9.49790805e-01 3.92922968e-02 7.11913967e-01\n",
      "  5.12342482e-01 7.59402458e-01 8.43592408e-01 1.22667388e-01\n",
      "  4.76991555e-01 4.99920319e-01 3.46114046e-01 6.76122818e-01\n",
      "  1.99219690e-01 3.46418763e-01 7.67283012e-01 6.21611981e-02\n",
      "  4.65660674e-01 1.35882852e-01 2.75056958e-02 8.14440949e-01\n",
      "  5.12280621e-01 2.89718169e-01 2.26159152e-01 8.59426221e-01\n",
      "  8.31766317e-01 7.21025893e-01 2.36531458e-01 3.34407282e-01\n",
      "  5.18417486e-03 2.74520002e-01 9.88312560e-02 8.88438832e-01\n",
      "  6.15302123e-01 3.40350392e-01 8.85025874e-01 4.82978874e-01\n",
      "  6.52135179e-01 5.35465430e-01 8.15428978e-01 4.69726730e-01\n",
      "  9.84667267e-01 4.41064925e-02 8.85720473e-01 6.94564156e-01\n",
      "  8.33644014e-01 4.62199310e-01 6.11420550e-01 2.26856131e-01\n",
      "  7.93877623e-01 2.29910135e-01 4.08255178e-01 4.88002683e-01\n",
      "  3.36776587e-01 3.71367360e-01 6.58654466e-03 3.56346509e-01\n",
      "  7.18389083e-01 1.38378511e-01 9.48261444e-01 1.24272688e-01\n",
      "  8.18091910e-01 7.58725577e-01 5.16483835e-01 3.77487958e-01\n",
      "  1.32109556e-01 8.55635904e-01 9.48054701e-01 2.25133587e-01\n",
      "  5.16193434e-01 7.49103684e-01 6.40103712e-01 8.66490795e-01\n",
      "  1.47631203e-01 7.42601349e-01 6.45736178e-01 7.58017785e-01\n",
      "  8.00791118e-01 2.21034840e-01 6.04138894e-03 6.27644416e-01\n",
      "  2.51511284e-01 7.70835201e-01 1.97890258e-01 9.04966821e-01\n",
      "  8.38354553e-01 4.17774065e-01 7.56710874e-01 5.77375675e-01\n",
      "  9.48714969e-01 6.54352780e-01 5.91201609e-01 9.88757902e-01\n",
      "  5.36355338e-01 9.00708201e-01 1.28361438e-01 3.92134983e-01\n",
      "  9.80294822e-01 1.78499223e-01 5.28765408e-01 2.79118558e-01\n",
      "  8.10087631e-04 4.24804085e-01 3.87216047e-01 8.86481598e-01\n",
      "  3.16543395e-01 2.40256397e-01 1.78145796e-01 5.45044300e-01\n",
      "  9.52003616e-02 1.16795750e-01 4.73094509e-01 2.60855433e-02\n",
      "  9.36587322e-01 4.60452601e-01 9.32277335e-01 8.83184041e-01\n",
      "  6.57881514e-02 3.07758651e-01 1.89241188e-01 8.87242024e-01\n",
      "  6.49857791e-01 2.15836181e-01 1.20427237e-01 8.95812657e-01\n",
      "  9.55019420e-01 5.13463812e-01 2.84487599e-01 2.51489927e-01\n",
      "  6.40159972e-01 7.51364482e-01 3.00907876e-01 5.88143106e-01\n",
      "  4.87415062e-01 8.58457206e-01 1.42278274e-01 7.98690488e-01\n",
      "  2.73846142e-01 3.83834241e-01 8.75991331e-01 5.68024634e-01\n",
      "  9.93538671e-01 2.36369886e-01 1.41582065e-01 1.24068231e-01\n",
      "  5.27906718e-01 3.77263831e-01 1.40507926e-02 5.92222274e-01\n",
      "  2.26226720e-01 2.20731087e-01 9.22157336e-01 3.01001862e-01\n",
      "  8.47033230e-01 4.54758615e-01 7.00669226e-01 4.32771885e-01\n",
      "  2.82777711e-01 1.21228699e-02 9.12962965e-01 9.16826702e-02\n",
      "  4.80181223e-01 7.40661652e-01 1.07381563e-01 5.77277291e-01\n",
      "  2.73545898e-01 8.93717863e-01 8.18047134e-01 1.64499417e-01\n",
      "  3.99032788e-01 2.98314981e-01 1.90811581e-02 3.37450598e-01\n",
      "  8.73757830e-01 5.34790218e-01 1.41278293e-02 1.63431078e-01\n",
      "  7.28865805e-01 6.17552576e-01 5.02941730e-01 4.73824843e-01\n",
      "  9.99528877e-01 4.57451808e-02 1.04543558e-03 8.25576035e-01\n",
      "  3.13216836e-01 2.34220249e-02 5.74193072e-01 3.39235914e-01\n",
      "  2.72263541e-01 6.21522887e-01 7.02696224e-01 1.13280378e-01\n",
      "  4.60243779e-01 1.75222109e-01 8.24617621e-01 7.05523852e-02\n",
      "  6.04566738e-01 8.05305923e-01 9.78888450e-01 5.74000775e-01\n",
      "  5.04724681e-01 2.05049308e-01 2.04859825e-01 9.94124945e-01\n",
      "  4.66985610e-02 9.70112492e-01 9.72997395e-01 1.93453186e-01\n",
      "  9.06024343e-01 1.03410621e-01 2.73912976e-01 1.36024949e-01\n",
      "  2.00717205e-01 5.62369836e-01 6.69027819e-01 8.94442570e-01\n",
      "  1.83283627e-01 6.23312707e-01 3.17589755e-01 6.02586637e-01\n",
      "  1.42333625e-01 4.40898359e-01 1.35381077e-01 4.46413514e-01\n",
      "  8.82987323e-01 7.66572224e-01 8.33549881e-01 6.51912920e-01\n",
      "  4.96788976e-01 1.96707494e-01 4.71312846e-01 8.09271687e-01\n",
      "  9.68949612e-01 1.04462059e-01 9.82692017e-02 5.16490593e-02\n",
      "  5.56991211e-01 3.29726080e-01 4.34727010e-01 7.55825486e-01\n",
      "  8.99331868e-01 7.09520259e-01 2.51632456e-01 7.89578787e-01\n",
      "  1.51667502e-01 2.22342622e-01 2.34424743e-01 5.26796026e-01\n",
      "  7.36712543e-01 4.32947889e-01 2.72533291e-02 7.41952567e-01\n",
      "  5.81484155e-01 6.78593913e-01 3.52220730e-01 3.95193247e-01\n",
      "  2.58015953e-01 9.22847306e-01 5.25837651e-01 8.64680498e-01\n",
      "  8.70224995e-02 6.75845660e-01 1.13512105e-01 8.76535477e-01\n",
      "  8.34084319e-01 2.01388923e-01 1.08218776e-01 4.38353248e-01\n",
      "  2.80591571e-01 4.26072192e-01 7.02967986e-01 6.32355958e-01\n",
      "  3.66934391e-01 3.11659960e-01 3.82069386e-01 7.29471059e-01\n",
      "  9.69126030e-03 6.19803828e-02 3.53983317e-01 2.69958175e-01\n",
      "  9.66142777e-01 7.30321680e-01 9.43740375e-01 7.51391011e-01\n",
      "  5.49224516e-01 5.10245980e-01 6.98469889e-01 5.46672205e-01\n",
      "  4.81235502e-01 5.08636918e-01 1.50948601e-01 3.67516680e-01\n",
      "  8.24291749e-01 9.09526456e-01 9.44568179e-02 8.25101003e-01\n",
      "  2.40389511e-01 9.05171588e-01 3.45896949e-01 2.32400691e-01\n",
      "  2.50710197e-01 4.80634307e-02 9.59469545e-01 1.43126980e-01\n",
      "  9.60563973e-01 6.71624761e-01 3.57136122e-01 9.39040011e-01\n",
      "  3.73584899e-01 2.37519168e-01 1.69439140e-01 7.89366981e-01\n",
      "  1.27677128e-01 6.36387350e-01 2.12390287e-01 7.46959891e-01\n",
      "  6.92346960e-01 9.55242586e-01 1.11651423e-01 5.90106374e-01\n",
      "  7.51845542e-02 6.89090646e-01 1.23323723e-01 8.00799005e-01\n",
      "  4.69699533e-01 3.68746345e-01 5.13019362e-01 9.94903904e-01\n",
      "  9.26161935e-01 5.79557527e-01 5.40849649e-01 9.24869294e-01\n",
      "  4.96928117e-01 3.09732244e-01 9.76918156e-01 8.82773966e-01\n",
      "  8.42204454e-01 5.27605152e-02 5.77234355e-01 9.42867268e-01\n",
      "  9.52955897e-01 8.25898718e-02 9.22200486e-01 9.69134034e-02\n",
      "  6.48948827e-01 9.51232119e-01 1.90825365e-01 5.94348695e-01\n",
      "  8.19259453e-01 8.25580453e-01 9.12813234e-01 7.80930163e-01\n",
      "  1.40316707e-01 3.04259476e-01 1.01061180e-01 2.08808579e-01\n",
      "  8.76153649e-01 5.92037458e-01 6.43737195e-02 6.72021310e-02\n",
      "  8.63044788e-02 2.24997534e-01 6.52601576e-01 6.14426901e-01\n",
      "  8.17660588e-01 4.13466140e-01 5.53455378e-01 9.86152545e-01\n",
      "  9.98198458e-01 7.21728287e-01 6.89874228e-01 1.79580813e-01\n",
      "  3.67407896e-01 5.29395254e-01 9.16695912e-01 2.23352009e-01\n",
      "  1.03070700e-01 7.59478382e-01 5.64290416e-01 4.23051854e-01\n",
      "  6.15084432e-01 2.35906776e-01 8.00328595e-01 9.25099231e-01\n",
      "  8.50857476e-01 5.93762599e-01 1.46389943e-02 9.47507953e-01\n",
      "  1.23534600e-01 6.62999460e-01 5.01384898e-01 1.08915779e-01\n",
      "  1.02234238e-01 7.68044735e-01 4.78178264e-01 7.49456919e-02\n",
      "  6.96282110e-01 8.01981741e-02 4.29911522e-01 5.64899526e-01\n",
      "  3.91933376e-01 5.87253747e-01 4.26957987e-01 4.31603078e-01\n",
      "  6.36762108e-02 3.29829576e-01 1.59516438e-01 7.22773217e-01\n",
      "  1.45312503e-01 3.38382865e-01 8.64586199e-01 7.93560793e-02\n",
      "  8.82640998e-01 1.26553490e-01 4.54614312e-01 7.47066778e-01\n",
      "  8.38641756e-01 7.88233837e-01 4.87974542e-01 1.02027085e-01\n",
      "  7.53817739e-01 8.80496617e-01 5.26660929e-01 1.70825185e-01\n",
      "  4.59572161e-01 9.68484234e-01 3.58474515e-01 5.99288004e-01\n",
      "  9.17973974e-01 7.24710199e-01 1.48533345e-01 5.61821212e-01\n",
      "  8.79336420e-01 3.57892490e-01 9.23135165e-01 3.06269592e-01\n",
      "  1.99888489e-01 5.77194916e-01 5.64169862e-01 6.50669255e-01\n",
      "  8.99016963e-01 9.87209709e-01 1.40555989e-01 4.20662213e-01\n",
      "  8.65563614e-01 4.93320601e-01 8.35976518e-02 7.62227786e-01\n",
      "  8.09688027e-01 7.33144638e-01 8.53770437e-01 7.27783583e-01\n",
      "  7.21589377e-01 5.75468721e-01 1.85627653e-01 7.60934183e-01\n",
      "  8.68981441e-01 9.52519652e-01 1.98953046e-01 7.96131490e-01\n",
      "  7.70772675e-02 9.90875102e-01 1.97828782e-01 4.74380196e-02\n",
      "  4.47463384e-01 7.70378981e-01 3.76053385e-01 8.45125234e-01\n",
      "  3.03892817e-01 2.27211256e-01 7.00325809e-02 3.80348375e-01\n",
      "  7.33080351e-02 7.45892596e-01 9.68569648e-01 1.31814694e-01\n",
      "  1.20388250e-03 3.14196969e-01 4.05168260e-01 8.80292386e-01\n",
      "  6.47532511e-01 1.54619133e-01 1.79610568e-01 4.07192021e-01\n",
      "  6.54972643e-01 5.79006315e-01 2.89469685e-01 9.09070672e-01\n",
      "  9.27788550e-01 9.15107470e-01 1.25437298e-01 1.68082309e-01\n",
      "  5.24873323e-01 6.45070630e-01 5.74167648e-02 8.80096621e-01\n",
      "  4.66103532e-01 4.75787877e-01 9.24940673e-01 5.30044794e-01\n",
      "  3.95264045e-01 7.50711317e-01 2.51705931e-01 9.60981458e-01\n",
      "  6.68692093e-01 8.39514621e-01 9.94634913e-01 7.65502256e-01\n",
      "  5.10782294e-01 3.75957476e-01 4.98992531e-01 5.77210960e-02\n",
      "  7.28725848e-01 7.74321115e-01 7.94845366e-01 4.15570621e-01\n",
      "  4.42378876e-01 2.55205806e-01 6.40355761e-01 3.30222603e-01\n",
      "  1.65397007e-01 3.51079796e-01 6.43727245e-01 4.72084302e-01\n",
      "  2.44004364e-01 4.43194002e-01 5.10137910e-01 4.09455911e-01\n",
      "  2.07551599e-01 8.90959737e-01 9.23898129e-01 6.80908676e-01\n",
      "  1.56213896e-01 3.89114467e-01 7.27514467e-01 6.75590900e-01\n",
      "  8.96693149e-01 2.88803604e-01 5.00042281e-01 7.30112255e-01\n",
      "  1.90248329e-01 5.63501044e-01 2.45772375e-01 3.42282539e-01\n",
      "  4.26169062e-01 8.07072079e-01 4.93449838e-01 8.56566688e-02\n",
      "  7.99063488e-01 3.11800054e-01 7.64805843e-01 7.40968052e-01\n",
      "  4.86183457e-01 1.69095141e-01 4.74838768e-01 3.51396491e-01\n",
      "  9.34613649e-01 9.79409761e-01 3.23960312e-01 4.71709377e-01\n",
      "  6.93289528e-02 9.35385995e-01 5.14899805e-01 1.31694331e-01\n",
      "  2.70933203e-01 1.35221801e-01 9.95829922e-01 1.72697605e-01\n",
      "  4.46674178e-01 4.99661444e-01 8.38705539e-01 2.13989340e-01\n",
      "  9.03470938e-01 9.88378690e-01 8.00231463e-01 5.48053053e-01\n",
      "  5.24017276e-01 5.01586807e-01 1.10474254e-01 8.55215875e-01\n",
      "  8.68568200e-01 9.84725935e-01 7.90508145e-01 5.71462336e-01\n",
      "  2.88329428e-01 9.33093651e-01 8.21227944e-01 6.92281259e-01\n",
      "  2.08966128e-01 5.16264600e-01 7.86177656e-02 2.31576045e-01\n",
      "  9.92596238e-01 6.36640551e-01 2.47821379e-01 8.49980684e-01\n",
      "  3.60305229e-01 1.71879592e-01 1.25634036e-01 4.33515007e-01\n",
      "  3.68695535e-01 9.46478475e-01 7.36783152e-01 3.60595928e-01\n",
      "  8.07351155e-01 3.95640465e-01 4.06495616e-01 7.38382168e-01\n",
      "  8.54086463e-01 6.67338470e-01 7.23891641e-01 1.74523627e-01\n",
      "  9.23391880e-02 8.46746775e-01 3.91478783e-01 8.22155040e-01\n",
      "  2.63932224e-01 4.67592498e-01 2.50128554e-01 5.23059595e-01\n",
      "  2.45269251e-01 5.58061702e-01 1.28883766e-01 2.39278074e-02\n",
      "  3.29889670e-01 8.42643063e-01 4.12454490e-01 7.62073914e-01\n",
      "  1.83386991e-01 3.04321718e-01 5.46350018e-01 7.63150015e-01\n",
      "  8.09625246e-01 2.56451403e-01 9.98516571e-01 7.98239954e-02\n",
      "  7.79702017e-01 8.78998608e-01 6.58046937e-01 9.70732624e-01\n",
      "  1.45853635e-01 9.45147331e-01 5.79456126e-02 4.42624466e-01\n",
      "  7.67000476e-01 7.57316714e-02 1.64853575e-01 6.43928622e-01\n",
      "  5.57348053e-01 8.70980260e-01 2.54175777e-01 3.17502391e-02\n",
      "  3.30290677e-01 4.70683898e-02 8.03726055e-01 2.99272066e-01\n",
      "  7.07823247e-01 3.52629857e-01 3.39153522e-01 2.17990145e-01\n",
      "  3.35990346e-01 1.39045006e-02 7.24109953e-01 8.14948028e-01\n",
      "  2.41678524e-01 1.75027896e-01 2.95985721e-02 9.32517344e-01\n",
      "  4.65667265e-01 9.79164317e-01 8.04196171e-01 1.61062614e-01\n",
      "  3.98141459e-01 1.59927796e-01 4.77523181e-01 2.24015897e-02\n",
      "  7.50467223e-01 8.18724960e-01 8.06211428e-01 5.60809232e-01\n",
      "  9.13315011e-01 8.93204833e-01 2.04380407e-01 3.99968562e-01\n",
      "  6.82206451e-01 4.78416132e-01 2.04217068e-01 8.36649247e-01\n",
      "  7.14366766e-01 2.42743311e-01 6.69390617e-01 4.88096044e-01\n",
      "  9.52266489e-01 2.82854130e-01 9.38242255e-01 7.98677792e-01\n",
      "  8.46967592e-01 8.50866228e-01 2.86199279e-03 2.92000318e-01\n",
      "  7.72443094e-01 7.34954428e-01 4.99521666e-01 9.60719894e-01\n",
      "  2.69292120e-01 7.67370038e-02 8.13088502e-01 9.14845360e-01\n",
      "  8.52975549e-01 1.87457288e-01 9.12953221e-01 8.93465073e-01\n",
      "  1.53599288e-01 8.52532683e-01 4.12786117e-01 6.47756474e-01\n",
      "  4.37664269e-01 6.53461114e-01 8.43427033e-01 2.65402811e-01\n",
      "  2.97428808e-01 7.01271397e-01 6.26214104e-01 1.01222383e-01\n",
      "  2.28357957e-01 5.24527248e-01 6.52702235e-01 7.56422739e-01\n",
      "  6.24625979e-01 4.88477441e-01 7.96424055e-01 9.54916190e-01\n",
      "  3.26716863e-01 2.17752409e-01 8.58656851e-01 2.18620541e-01\n",
      "  9.66925551e-01 8.53112515e-01 3.38965318e-01 7.39860671e-01\n",
      "  9.03356119e-01 7.61675447e-02 9.34545437e-01 5.81258631e-02\n",
      "  5.65199417e-01 5.48716015e-01 2.58707351e-01 4.96648479e-01\n",
      "  9.55189679e-01 6.37771039e-01 4.51722677e-01 8.25717248e-01\n",
      "  6.62386780e-01 7.54707882e-01 8.38825230e-01 4.09208771e-01\n",
      "  6.80225513e-01 2.08241351e-01 7.07852909e-02 2.82833884e-01\n",
      "  3.36608270e-01 5.06698878e-01 2.40685623e-01 9.35614977e-01\n",
      "  3.39828011e-02 4.14591963e-01 6.14991194e-01 5.42613240e-01\n",
      "  7.55458841e-01 2.23757153e-01 4.93275888e-01 9.54495770e-01\n",
      "  8.64433817e-02 6.86293311e-01 7.55471894e-01 1.94834573e-01\n",
      "  2.51692961e-01 2.73434128e-01 4.60026708e-01 3.83425242e-01\n",
      "  5.09564448e-01 6.18540462e-01 8.51096193e-01 2.35306646e-01\n",
      "  4.01870810e-01 6.53866270e-01 6.34525433e-01]]\n",
      "Change in theta: 1.3398088882476107e-06\n",
      "---------------------------------------------\n",
      "Iteration number 12 finished, Running time 257.0776619911194, Roo2 iteration 12 = -3307675.975928636\n",
      "Iteration 1:\n",
      "Theta: [[6.82568394e-01 2.04516631e-01 3.44622214e-01 5.75112079e-01\n",
      "  1.67793810e-01 3.75643376e-01 1.16350758e-01 8.83582796e-01\n",
      "  5.22037356e-01 3.47336157e-01 8.63881032e-01 1.62097821e-01\n",
      "  1.26217307e-01 5.95401218e-01 9.41477996e-01 7.64160294e-01\n",
      "  5.77490331e-01 7.73449899e-01 1.63607645e-01 7.90837919e-01\n",
      "  9.35626968e-02 2.68792046e-01 3.84672839e-01 4.97082095e-01\n",
      "  4.94985063e-01 6.36570325e-01 3.65353180e-01 2.17178534e-01\n",
      "  4.22684753e-01 8.48215944e-01 8.36630323e-01 9.86251328e-01\n",
      "  4.69583795e-01 5.17763057e-01 1.62541181e-02 8.68933138e-01\n",
      "  6.89862727e-01 6.26069482e-01 4.87924232e-01 5.22118416e-01\n",
      "  4.13599572e-01 4.41750704e-01 7.98431755e-01 4.11084827e-01\n",
      "  7.66480050e-01 2.83444955e-01 6.01818136e-01 7.80387046e-02\n",
      "  9.99848077e-01 2.20895818e-01 7.66514227e-01 6.18842843e-01\n",
      "  6.52115697e-01 9.73274207e-01 3.28900359e-01 2.16733008e-02\n",
      "  3.20340704e-01 1.12516557e-01 5.44438021e-02 9.88049860e-01\n",
      "  6.05752486e-02 8.64985715e-01 4.53268847e-02 4.81471874e-01\n",
      "  7.91203847e-01 3.88678729e-01 4.32174613e-01 7.85661187e-02\n",
      "  1.92387061e-01 9.14160456e-01 7.07720952e-01 5.39034157e-02\n",
      "  8.76088640e-01 5.85045220e-01 5.29693764e-01 9.12780840e-01\n",
      "  7.89518185e-01 6.30183797e-01 9.55831844e-01 4.74084219e-01\n",
      "  4.29746000e-01 9.38222855e-01 6.08034282e-01 7.48817081e-01\n",
      "  5.09958179e-01 8.82823646e-01 9.07730116e-01 5.37804445e-01\n",
      "  5.87242554e-01 1.60267228e-01 8.63046216e-01 9.99041668e-02\n",
      "  9.15919170e-01 1.18098888e-01 5.79012136e-01 3.95712479e-01\n",
      "  2.48615057e-01 9.28306115e-01 2.19438619e-01 6.00783786e-02\n",
      "  9.26517158e-01 7.02465712e-01 4.80490640e-01 4.96204119e-01\n",
      "  6.63794935e-03 1.45512022e-01 4.15112991e-01 3.65099953e-02\n",
      "  7.29434938e-01 5.36876655e-01 3.39497621e-01 6.57794521e-01\n",
      "  1.25023564e-01 4.34485084e-01 8.53863015e-02 9.71359782e-01\n",
      "  2.27620839e-01 7.88506730e-01 8.18925788e-01 6.22102193e-01\n",
      "  9.20606414e-01 3.11783194e-01 1.56005909e-01 9.24307423e-01\n",
      "  5.82781941e-01 9.49791978e-01 3.92934702e-02 7.11915141e-01\n",
      "  5.12343655e-01 7.59403631e-01 8.43593582e-01 1.22668561e-01\n",
      "  4.76992728e-01 4.99921492e-01 3.46115219e-01 6.76123991e-01\n",
      "  1.99220863e-01 3.46419936e-01 7.67284185e-01 6.21623715e-02\n",
      "  4.65661847e-01 1.35884025e-01 2.75068692e-02 8.14442123e-01\n",
      "  5.12281795e-01 2.89719342e-01 2.26160325e-01 8.59427394e-01\n",
      "  8.31767491e-01 7.21027067e-01 2.36532631e-01 3.34408456e-01\n",
      "  5.18534826e-03 2.74521175e-01 9.88324294e-02 8.88440005e-01\n",
      "  6.15303296e-01 3.40351565e-01 8.85027048e-01 4.82980047e-01\n",
      "  6.52136353e-01 5.35466604e-01 8.15430152e-01 4.69727903e-01\n",
      "  9.84668440e-01 4.41076659e-02 8.85721647e-01 6.94565330e-01\n",
      "  8.33645187e-01 4.62200483e-01 6.11421724e-01 2.26857304e-01\n",
      "  7.93878797e-01 2.29911308e-01 4.08256352e-01 4.88003856e-01\n",
      "  3.36777761e-01 3.71368534e-01 6.58771806e-03 3.56347682e-01\n",
      "  7.18390256e-01 1.38379685e-01 9.48262618e-01 1.24273861e-01\n",
      "  8.18093083e-01 7.58726750e-01 5.16485008e-01 3.77489131e-01\n",
      "  1.32110730e-01 8.55637077e-01 9.48055874e-01 2.25134761e-01\n",
      "  5.16194607e-01 7.49104858e-01 6.40104886e-01 8.66491968e-01\n",
      "  1.47632376e-01 7.42602522e-01 6.45737351e-01 7.58018959e-01\n",
      "  8.00792291e-01 2.21036013e-01 6.04256234e-03 6.27645589e-01\n",
      "  2.51512457e-01 7.70836375e-01 1.97891431e-01 9.04967995e-01\n",
      "  8.38355726e-01 4.17775239e-01 7.56712048e-01 5.77376848e-01\n",
      "  9.48716142e-01 6.54353953e-01 5.91202782e-01 9.88759076e-01\n",
      "  5.36356512e-01 9.00709374e-01 1.28362612e-01 3.92136156e-01\n",
      "  9.80295996e-01 1.78500396e-01 5.28766582e-01 2.79119731e-01\n",
      "  8.11261030e-04 4.24805258e-01 3.87217221e-01 8.86482772e-01\n",
      "  3.16544568e-01 2.40257570e-01 1.78146969e-01 5.45045474e-01\n",
      "  9.52015350e-02 1.16796923e-01 4.73095683e-01 2.60867167e-02\n",
      "  9.36588495e-01 4.60453775e-01 9.32278509e-01 8.83185214e-01\n",
      "  6.57893248e-02 3.07759824e-01 1.89242361e-01 8.87243198e-01\n",
      "  6.49858965e-01 2.15837355e-01 1.20428410e-01 8.95813831e-01\n",
      "  9.55020593e-01 5.13464985e-01 2.84488773e-01 2.51491101e-01\n",
      "  6.40161146e-01 7.51365656e-01 3.00909049e-01 5.88144279e-01\n",
      "  4.87416235e-01 8.58458379e-01 1.42279448e-01 7.98691661e-01\n",
      "  2.73847316e-01 3.83835415e-01 8.75992505e-01 5.68025808e-01\n",
      "  9.93539844e-01 2.36371060e-01 1.41583238e-01 1.24069404e-01\n",
      "  5.27907891e-01 3.77265005e-01 1.40519660e-02 5.92223448e-01\n",
      "  2.26227893e-01 2.20732260e-01 9.22158509e-01 3.01003035e-01\n",
      "  8.47034403e-01 4.54759789e-01 7.00670400e-01 4.32773058e-01\n",
      "  2.82778885e-01 1.21240433e-02 9.12964138e-01 9.16838436e-02\n",
      "  4.80182396e-01 7.40662826e-01 1.07382737e-01 5.77278464e-01\n",
      "  2.73547072e-01 8.93719037e-01 8.18048307e-01 1.64500590e-01\n",
      "  3.99033962e-01 2.98316154e-01 1.90823315e-02 3.37451772e-01\n",
      "  8.73759003e-01 5.34791392e-01 1.41290027e-02 1.63432252e-01\n",
      "  7.28866978e-01 6.17553749e-01 5.02942903e-01 4.73826017e-01\n",
      "  9.99530050e-01 4.57463542e-02 1.04660897e-03 8.25577209e-01\n",
      "  3.13218009e-01 2.34231983e-02 5.74194245e-01 3.39237087e-01\n",
      "  2.72264714e-01 6.21524060e-01 7.02697397e-01 1.13281551e-01\n",
      "  4.60244952e-01 1.75223282e-01 8.24618794e-01 7.05535586e-02\n",
      "  6.04567911e-01 8.05307097e-01 9.78889623e-01 5.74001948e-01\n",
      "  5.04725854e-01 2.05050482e-01 2.04860998e-01 9.94126118e-01\n",
      "  4.66997344e-02 9.70113666e-01 9.72998569e-01 1.93454360e-01\n",
      "  9.06025517e-01 1.03411794e-01 2.73914149e-01 1.36026122e-01\n",
      "  2.00718378e-01 5.62371010e-01 6.69028992e-01 8.94443744e-01\n",
      "  1.83284801e-01 6.23313880e-01 3.17590928e-01 6.02587810e-01\n",
      "  1.42334799e-01 4.40899532e-01 1.35382250e-01 4.46414688e-01\n",
      "  8.82988497e-01 7.66573398e-01 8.33551055e-01 6.51914093e-01\n",
      "  4.96790150e-01 1.96708667e-01 4.71314020e-01 8.09272860e-01\n",
      "  9.68950786e-01 1.04463232e-01 9.82703751e-02 5.16502327e-02\n",
      "  5.56992385e-01 3.29727253e-01 4.34728184e-01 7.55826659e-01\n",
      "  8.99333041e-01 7.09521432e-01 2.51633630e-01 7.89579960e-01\n",
      "  1.51668675e-01 2.22343796e-01 2.34425916e-01 5.26797199e-01\n",
      "  7.36713717e-01 4.32949062e-01 2.72545025e-02 7.41953740e-01\n",
      "  5.81485328e-01 6.78595087e-01 3.52221903e-01 3.95194421e-01\n",
      "  2.58017127e-01 9.22848479e-01 5.25838825e-01 8.64681671e-01\n",
      "  8.70236729e-02 6.75846834e-01 1.13513279e-01 8.76536650e-01\n",
      "  8.34085493e-01 2.01390097e-01 1.08219949e-01 4.38354421e-01\n",
      "  2.80592744e-01 4.26073365e-01 7.02969160e-01 6.32357131e-01\n",
      "  3.66935565e-01 3.11661134e-01 3.82070559e-01 7.29472233e-01\n",
      "  9.69243370e-03 6.19815562e-02 3.53984491e-01 2.69959348e-01\n",
      "  9.66143950e-01 7.30322854e-01 9.43741548e-01 7.51392184e-01\n",
      "  5.49225689e-01 5.10247154e-01 6.98471063e-01 5.46673379e-01\n",
      "  4.81236675e-01 5.08638092e-01 1.50949774e-01 3.67517854e-01\n",
      "  8.24292923e-01 9.09527629e-01 9.44579913e-02 8.25102176e-01\n",
      "  2.40390684e-01 9.05172762e-01 3.45898122e-01 2.32401864e-01\n",
      "  2.50711370e-01 4.80646041e-02 9.59470718e-01 1.43128153e-01\n",
      "  9.60565146e-01 6.71625934e-01 3.57137295e-01 9.39041184e-01\n",
      "  3.73586073e-01 2.37520342e-01 1.69440314e-01 7.89368155e-01\n",
      "  1.27678302e-01 6.36388524e-01 2.12391460e-01 7.46961064e-01\n",
      "  6.92348133e-01 9.55243759e-01 1.11652597e-01 5.90107548e-01\n",
      "  7.51857276e-02 6.89091819e-01 1.23324896e-01 8.00800179e-01\n",
      "  4.69700707e-01 3.68747518e-01 5.13020535e-01 9.94905077e-01\n",
      "  9.26163108e-01 5.79558701e-01 5.40850822e-01 9.24870468e-01\n",
      "  4.96929291e-01 3.09733417e-01 9.76919329e-01 8.82775139e-01\n",
      "  8.42205627e-01 5.27616886e-02 5.77235529e-01 9.42868441e-01\n",
      "  9.52957071e-01 8.25910452e-02 9.22201660e-01 9.69145768e-02\n",
      "  6.48950000e-01 9.51233293e-01 1.90826538e-01 5.94349869e-01\n",
      "  8.19260627e-01 8.25581626e-01 9.12814407e-01 7.80931337e-01\n",
      "  1.40317880e-01 3.04260650e-01 1.01062354e-01 2.08809752e-01\n",
      "  8.76154822e-01 5.92038632e-01 6.43748929e-02 6.72033044e-02\n",
      "  8.63056522e-02 2.24998707e-01 6.52602749e-01 6.14428074e-01\n",
      "  8.17661761e-01 4.13467313e-01 5.53456552e-01 9.86153718e-01\n",
      "  9.98199631e-01 7.21729461e-01 6.89875402e-01 1.79581987e-01\n",
      "  3.67409069e-01 5.29396427e-01 9.16697086e-01 2.23353183e-01\n",
      "  1.03071873e-01 7.59479556e-01 5.64291589e-01 4.23053027e-01\n",
      "  6.15085606e-01 2.35907950e-01 8.00329768e-01 9.25100404e-01\n",
      "  8.50858649e-01 5.93763773e-01 1.46401677e-02 9.47509126e-01\n",
      "  1.23535774e-01 6.63000633e-01 5.01386071e-01 1.08916952e-01\n",
      "  1.02235411e-01 7.68045909e-01 4.78179437e-01 7.49468653e-02\n",
      "  6.96283284e-01 8.01993475e-02 4.29912695e-01 5.64900699e-01\n",
      "  3.91934549e-01 5.87254920e-01 4.26959160e-01 4.31604251e-01\n",
      "  6.36773842e-02 3.29830750e-01 1.59517612e-01 7.22774390e-01\n",
      "  1.45313676e-01 3.38384039e-01 8.64587373e-01 7.93572527e-02\n",
      "  8.82642172e-01 1.26554663e-01 4.54615486e-01 7.47067952e-01\n",
      "  8.38642930e-01 7.88235010e-01 4.87975716e-01 1.02028258e-01\n",
      "  7.53818912e-01 8.80497791e-01 5.26662102e-01 1.70826358e-01\n",
      "  4.59573335e-01 9.68485407e-01 3.58475689e-01 5.99289177e-01\n",
      "  9.17975147e-01 7.24711373e-01 1.48534519e-01 5.61822386e-01\n",
      "  8.79337593e-01 3.57893664e-01 9.23136338e-01 3.06270765e-01\n",
      "  1.99889662e-01 5.77196089e-01 5.64171036e-01 6.50670429e-01\n",
      "  8.99018136e-01 9.87210883e-01 1.40557162e-01 4.20663387e-01\n",
      "  8.65564787e-01 4.93321775e-01 8.35988252e-02 7.62228959e-01\n",
      "  8.09689201e-01 7.33145811e-01 8.53771610e-01 7.27784757e-01\n",
      "  7.21590550e-01 5.75469894e-01 1.85628827e-01 7.60935357e-01\n",
      "  8.68982614e-01 9.52520825e-01 1.98954220e-01 7.96132663e-01\n",
      "  7.70784409e-02 9.90876275e-01 1.97829956e-01 4.74391930e-02\n",
      "  4.47464558e-01 7.70380155e-01 3.76054558e-01 8.45126407e-01\n",
      "  3.03893991e-01 2.27212429e-01 7.00337543e-02 3.80349548e-01\n",
      "  7.33092085e-02 7.45893769e-01 9.68570821e-01 1.31815868e-01\n",
      "  1.20505589e-03 3.14198142e-01 4.05169434e-01 8.80293559e-01\n",
      "  6.47533685e-01 1.54620306e-01 1.79611742e-01 4.07193194e-01\n",
      "  6.54973816e-01 5.79007488e-01 2.89470858e-01 9.09071845e-01\n",
      "  9.27789724e-01 9.15108643e-01 1.25438471e-01 1.68083483e-01\n",
      "  5.24874497e-01 6.45071803e-01 5.74179382e-02 8.80097794e-01\n",
      "  4.66104705e-01 4.75789050e-01 9.24941846e-01 5.30045968e-01\n",
      "  3.95265218e-01 7.50712490e-01 2.51707104e-01 9.60982631e-01\n",
      "  6.68693267e-01 8.39515794e-01 9.94636086e-01 7.65503429e-01\n",
      "  5.10783468e-01 3.75958650e-01 4.98993704e-01 5.77222694e-02\n",
      "  7.28727022e-01 7.74322288e-01 7.94846539e-01 4.15571794e-01\n",
      "  4.42380050e-01 2.55206979e-01 6.40356934e-01 3.30223776e-01\n",
      "  1.65398181e-01 3.51080969e-01 6.43728418e-01 4.72085475e-01\n",
      "  2.44005537e-01 4.43195175e-01 5.10139083e-01 4.09457084e-01\n",
      "  2.07552773e-01 8.90960910e-01 9.23899302e-01 6.80909849e-01\n",
      "  1.56215070e-01 3.89115640e-01 7.27515640e-01 6.75592073e-01\n",
      "  8.96694322e-01 2.88804777e-01 5.00043454e-01 7.30113428e-01\n",
      "  1.90249502e-01 5.63502218e-01 2.45773548e-01 3.42283712e-01\n",
      "  4.26170235e-01 8.07073253e-01 4.93451011e-01 8.56578422e-02\n",
      "  7.99064661e-01 3.11801228e-01 7.64807016e-01 7.40969226e-01\n",
      "  4.86184630e-01 1.69096314e-01 4.74839941e-01 3.51397664e-01\n",
      "  9.34614823e-01 9.79410934e-01 3.23961486e-01 4.71710550e-01\n",
      "  6.93301262e-02 9.35387168e-01 5.14900979e-01 1.31695504e-01\n",
      "  2.70934376e-01 1.35222975e-01 9.95831096e-01 1.72698779e-01\n",
      "  4.46675351e-01 4.99662617e-01 8.38706712e-01 2.13990514e-01\n",
      "  9.03472111e-01 9.88379863e-01 8.00232637e-01 5.48054227e-01\n",
      "  5.24018449e-01 5.01587980e-01 1.10475427e-01 8.55217049e-01\n",
      "  8.68569374e-01 9.84727109e-01 7.90509318e-01 5.71463509e-01\n",
      "  2.88330601e-01 9.33094825e-01 8.21229117e-01 6.92282433e-01\n",
      "  2.08967301e-01 5.16265773e-01 7.86189390e-02 2.31577218e-01\n",
      "  9.92597412e-01 6.36641724e-01 2.47822553e-01 8.49981857e-01\n",
      "  3.60306402e-01 1.71880766e-01 1.25635210e-01 4.33516180e-01\n",
      "  3.68696709e-01 9.46479648e-01 7.36784325e-01 3.60597102e-01\n",
      "  8.07352329e-01 3.95641638e-01 4.06496789e-01 7.38383341e-01\n",
      "  8.54087636e-01 6.67339643e-01 7.23892814e-01 1.74524800e-01\n",
      "  9.23403614e-02 8.46747949e-01 3.91479956e-01 8.22156214e-01\n",
      "  2.63933397e-01 4.67593671e-01 2.50129728e-01 5.23060769e-01\n",
      "  2.45270424e-01 5.58062875e-01 1.28884940e-01 2.39289808e-02\n",
      "  3.29890843e-01 8.42644237e-01 4.12455663e-01 7.62075087e-01\n",
      "  1.83388164e-01 3.04322892e-01 5.46351191e-01 7.63151189e-01\n",
      "  8.09626419e-01 2.56452576e-01 9.98517744e-01 7.98251688e-02\n",
      "  7.79703190e-01 8.78999781e-01 6.58048111e-01 9.70733797e-01\n",
      "  1.45854808e-01 9.45148505e-01 5.79467860e-02 4.42625640e-01\n",
      "  7.67001649e-01 7.57328448e-02 1.64854749e-01 6.43929795e-01\n",
      "  5.57349227e-01 8.70981434e-01 2.54176950e-01 3.17514125e-02\n",
      "  3.30291851e-01 4.70695632e-02 8.03727229e-01 2.99273239e-01\n",
      "  7.07824420e-01 3.52631030e-01 3.39154696e-01 2.17991319e-01\n",
      "  3.35991519e-01 1.39056740e-02 7.24111126e-01 8.14949202e-01\n",
      "  2.41679698e-01 1.75029070e-01 2.95997455e-02 9.32518518e-01\n",
      "  4.65668438e-01 9.79165490e-01 8.04197344e-01 1.61063788e-01\n",
      "  3.98142633e-01 1.59928969e-01 4.77524355e-01 2.24027631e-02\n",
      "  7.50468396e-01 8.18726134e-01 8.06212601e-01 5.60810406e-01\n",
      "  9.13316185e-01 8.93206007e-01 2.04381580e-01 3.99969735e-01\n",
      "  6.82207624e-01 4.78417305e-01 2.04218242e-01 8.36650420e-01\n",
      "  7.14367940e-01 2.42744485e-01 6.69391790e-01 4.88097217e-01\n",
      "  9.52267662e-01 2.82855304e-01 9.38243428e-01 7.98678965e-01\n",
      "  8.46968766e-01 8.50867402e-01 2.86316619e-03 2.92001491e-01\n",
      "  7.72444267e-01 7.34955601e-01 4.99522839e-01 9.60721067e-01\n",
      "  2.69293294e-01 7.67381772e-02 8.13089676e-01 9.14846534e-01\n",
      "  8.52976723e-01 1.87458461e-01 9.12954394e-01 8.93466247e-01\n",
      "  1.53600461e-01 8.52533856e-01 4.12787291e-01 6.47757647e-01\n",
      "  4.37665443e-01 6.53462288e-01 8.43428207e-01 2.65403984e-01\n",
      "  2.97429981e-01 7.01272570e-01 6.26215278e-01 1.01223556e-01\n",
      "  2.28359130e-01 5.24528421e-01 6.52703409e-01 7.56423913e-01\n",
      "  6.24627152e-01 4.88478614e-01 7.96425229e-01 9.54917363e-01\n",
      "  3.26718037e-01 2.17753583e-01 8.58658025e-01 2.18621715e-01\n",
      "  9.66926724e-01 8.53113688e-01 3.38966492e-01 7.39861844e-01\n",
      "  9.03357292e-01 7.61687181e-02 9.34546611e-01 5.81270365e-02\n",
      "  5.65200590e-01 5.48717188e-01 2.58708525e-01 4.96649652e-01\n",
      "  9.55190853e-01 6.37772212e-01 4.51723850e-01 8.25718421e-01\n",
      "  6.62387954e-01 7.54709056e-01 8.38826403e-01 4.09209945e-01\n",
      "  6.80226686e-01 2.08242524e-01 7.07864643e-02 2.82835057e-01\n",
      "  3.36609443e-01 5.06700052e-01 2.40686796e-01 9.35616150e-01\n",
      "  3.39839745e-02 4.14593137e-01 6.14992367e-01 5.42614413e-01\n",
      "  7.55460015e-01 2.23758326e-01 4.93277062e-01 9.54496944e-01\n",
      "  8.64445551e-02 6.86294484e-01 7.55473068e-01 1.94835746e-01\n",
      "  2.51694134e-01 2.73435301e-01 4.60027881e-01 3.83426415e-01\n",
      "  5.09565621e-01 6.18541635e-01 8.51097366e-01 2.35307819e-01\n",
      "  4.01871983e-01 6.53867444e-01 6.34526606e-01]]\n",
      "Change in theta: 1.4230523424280167e-06\n",
      "---------------------------------------------\n",
      "Iteration number 13 finished, Running time 264.13520216941833, Roo2 iteration 13 = -6785810.98054181\n",
      "Iteration 1:\n",
      "Theta: [[6.82569448e-01 2.04517686e-01 3.44623269e-01 5.75113133e-01\n",
      "  1.67794864e-01 3.75644430e-01 1.16351812e-01 8.83583851e-01\n",
      "  5.22038411e-01 3.47337211e-01 8.63882086e-01 1.62098875e-01\n",
      "  1.26218361e-01 5.95402272e-01 9.41479050e-01 7.64161349e-01\n",
      "  5.77491385e-01 7.73450953e-01 1.63608699e-01 7.90838973e-01\n",
      "  9.35637511e-02 2.68793100e-01 3.84673893e-01 4.97083149e-01\n",
      "  4.94986117e-01 6.36571379e-01 3.65354235e-01 2.17179588e-01\n",
      "  4.22685807e-01 8.48216999e-01 8.36631378e-01 9.86252382e-01\n",
      "  4.69584850e-01 5.17764111e-01 1.62551724e-02 8.68934193e-01\n",
      "  6.89863782e-01 6.26070536e-01 4.87925286e-01 5.22119471e-01\n",
      "  4.13600626e-01 4.41751758e-01 7.98432810e-01 4.11085882e-01\n",
      "  7.66481105e-01 2.83446009e-01 6.01819190e-01 7.80397589e-02\n",
      "  9.99849131e-01 2.20896872e-01 7.66515281e-01 6.18843897e-01\n",
      "  6.52116752e-01 9.73275261e-01 3.28901413e-01 2.16743552e-02\n",
      "  3.20341758e-01 1.12517611e-01 5.44448564e-02 9.88050915e-01\n",
      "  6.05763029e-02 8.64986769e-01 4.53279390e-02 4.81472929e-01\n",
      "  7.91204902e-01 3.88679783e-01 4.32175667e-01 7.85671730e-02\n",
      "  1.92388116e-01 9.14161511e-01 7.07722006e-01 5.39044700e-02\n",
      "  8.76089695e-01 5.85046274e-01 5.29694818e-01 9.12781894e-01\n",
      "  7.89519239e-01 6.30184851e-01 9.55832898e-01 4.74085273e-01\n",
      "  4.29747055e-01 9.38223909e-01 6.08035336e-01 7.48818135e-01\n",
      "  5.09959233e-01 8.82824700e-01 9.07731170e-01 5.37805499e-01\n",
      "  5.87243608e-01 1.60268283e-01 8.63047270e-01 9.99052211e-02\n",
      "  9.15920225e-01 1.18099942e-01 5.79013190e-01 3.95713533e-01\n",
      "  2.48616111e-01 9.28307170e-01 2.19439674e-01 6.00794329e-02\n",
      "  9.26518213e-01 7.02466767e-01 4.80491694e-01 4.96205174e-01\n",
      "  6.63900367e-03 1.45513076e-01 4.15114046e-01 3.65110496e-02\n",
      "  7.29435993e-01 5.36877710e-01 3.39498675e-01 6.57795575e-01\n",
      "  1.25024618e-01 4.34486139e-01 8.53873558e-02 9.71360837e-01\n",
      "  2.27621894e-01 7.88507784e-01 8.18926842e-01 6.22103248e-01\n",
      "  9.20607468e-01 3.11784249e-01 1.56006964e-01 9.24308477e-01\n",
      "  5.82782996e-01 9.49793033e-01 3.92945245e-02 7.11916195e-01\n",
      "  5.12344709e-01 7.59404685e-01 8.43594636e-01 1.22669615e-01\n",
      "  4.76993783e-01 4.99922546e-01 3.46116274e-01 6.76125046e-01\n",
      "  1.99221917e-01 3.46420991e-01 7.67285240e-01 6.21634258e-02\n",
      "  4.65662901e-01 1.35885080e-01 2.75079235e-02 8.14443177e-01\n",
      "  5.12282849e-01 2.89720397e-01 2.26161380e-01 8.59428448e-01\n",
      "  8.31768545e-01 7.21028121e-01 2.36533686e-01 3.34409510e-01\n",
      "  5.18640258e-03 2.74522229e-01 9.88334837e-02 8.88441060e-01\n",
      "  6.15304351e-01 3.40352620e-01 8.85028102e-01 4.82981102e-01\n",
      "  6.52137407e-01 5.35467658e-01 8.15431206e-01 4.69728957e-01\n",
      "  9.84669495e-01 4.41087202e-02 8.85722701e-01 6.94566384e-01\n",
      "  8.33646241e-01 4.62201537e-01 6.11422778e-01 2.26858359e-01\n",
      "  7.93879851e-01 2.29912362e-01 4.08257406e-01 4.88004911e-01\n",
      "  3.36778815e-01 3.71369588e-01 6.58877238e-03 3.56348737e-01\n",
      "  7.18391311e-01 1.38380739e-01 9.48263672e-01 1.24274916e-01\n",
      "  8.18094137e-01 7.58727805e-01 5.16486062e-01 3.77490186e-01\n",
      "  1.32111784e-01 8.55638132e-01 9.48056928e-01 2.25135815e-01\n",
      "  5.16195662e-01 7.49105912e-01 6.40105940e-01 8.66493022e-01\n",
      "  1.47633430e-01 7.42603576e-01 6.45738406e-01 7.58020013e-01\n",
      "  8.00793345e-01 2.21037068e-01 6.04361666e-03 6.27646643e-01\n",
      "  2.51513512e-01 7.70837429e-01 1.97892485e-01 9.04969049e-01\n",
      "  8.38356781e-01 4.17776293e-01 7.56713102e-01 5.77377902e-01\n",
      "  9.48717196e-01 6.54355007e-01 5.91203836e-01 9.88760130e-01\n",
      "  5.36357566e-01 9.00710429e-01 1.28363666e-01 3.92137211e-01\n",
      "  9.80297050e-01 1.78501451e-01 5.28767636e-01 2.79120786e-01\n",
      "  8.12315344e-04 4.24806313e-01 3.87218275e-01 8.86483826e-01\n",
      "  3.16545623e-01 2.40258625e-01 1.78148023e-01 5.45046528e-01\n",
      "  9.52025893e-02 1.16797977e-01 4.73096737e-01 2.60877710e-02\n",
      "  9.36589550e-01 4.60454829e-01 9.32279563e-01 8.83186269e-01\n",
      "  6.57903791e-02 3.07760878e-01 1.89243416e-01 8.87244252e-01\n",
      "  6.49860019e-01 2.15838409e-01 1.20429465e-01 8.95814885e-01\n",
      "  9.55021647e-01 5.13466040e-01 2.84489827e-01 2.51492155e-01\n",
      "  6.40162200e-01 7.51366710e-01 3.00910103e-01 5.88145334e-01\n",
      "  4.87417290e-01 8.58459434e-01 1.42280502e-01 7.98692716e-01\n",
      "  2.73848370e-01 3.83836469e-01 8.75993559e-01 5.68026862e-01\n",
      "  9.93540899e-01 2.36372114e-01 1.41584292e-01 1.24070458e-01\n",
      "  5.27908945e-01 3.77266059e-01 1.40530203e-02 5.92224502e-01\n",
      "  2.26228948e-01 2.20733314e-01 9.22159563e-01 3.01004089e-01\n",
      "  8.47035458e-01 4.54760843e-01 7.00671454e-01 4.32774113e-01\n",
      "  2.82779939e-01 1.21250976e-02 9.12965192e-01 9.16848979e-02\n",
      "  4.80183451e-01 7.40663880e-01 1.07383791e-01 5.77279519e-01\n",
      "  2.73548126e-01 8.93720091e-01 8.18049362e-01 1.64501644e-01\n",
      "  3.99035016e-01 2.98317209e-01 1.90833858e-02 3.37452826e-01\n",
      "  8.73760058e-01 5.34792446e-01 1.41300570e-02 1.63433306e-01\n",
      "  7.28868033e-01 6.17554803e-01 5.02943958e-01 4.73827071e-01\n",
      "  9.99531104e-01 4.57474085e-02 1.04766329e-03 8.25578263e-01\n",
      "  3.13219064e-01 2.34242526e-02 5.74195300e-01 3.39238142e-01\n",
      "  2.72265769e-01 6.21525114e-01 7.02698451e-01 1.13282606e-01\n",
      "  4.60246006e-01 1.75224336e-01 8.24619848e-01 7.05546129e-02\n",
      "  6.04568966e-01 8.05308151e-01 9.78890677e-01 5.74003003e-01\n",
      "  5.04726909e-01 2.05051536e-01 2.04862053e-01 9.94127173e-01\n",
      "  4.67007888e-02 9.70114720e-01 9.72999623e-01 1.93455414e-01\n",
      "  9.06026571e-01 1.03412849e-01 2.73915204e-01 1.36027176e-01\n",
      "  2.00719433e-01 5.62372064e-01 6.69030046e-01 8.94444798e-01\n",
      "  1.83285855e-01 6.23314935e-01 3.17591982e-01 6.02588864e-01\n",
      "  1.42335853e-01 4.40900586e-01 1.35383304e-01 4.46415742e-01\n",
      "  8.82989551e-01 7.66574452e-01 8.33552109e-01 6.51915147e-01\n",
      "  4.96791204e-01 1.96709721e-01 4.71315074e-01 8.09273914e-01\n",
      "  9.68951840e-01 1.04464287e-01 9.82714294e-02 5.16512871e-02\n",
      "  5.56993439e-01 3.29728308e-01 4.34729238e-01 7.55827714e-01\n",
      "  8.99334095e-01 7.09522487e-01 2.51634684e-01 7.89581015e-01\n",
      "  1.51669730e-01 2.22344850e-01 2.34426970e-01 5.26798254e-01\n",
      "  7.36714771e-01 4.32950117e-01 2.72555568e-02 7.41954794e-01\n",
      "  5.81486383e-01 6.78596141e-01 3.52222958e-01 3.95195475e-01\n",
      "  2.58018181e-01 9.22849533e-01 5.25839879e-01 8.64682726e-01\n",
      "  8.70247272e-02 6.75847888e-01 1.13514333e-01 8.76537705e-01\n",
      "  8.34086547e-01 2.01391151e-01 1.08221003e-01 4.38355475e-01\n",
      "  2.80593798e-01 4.26074419e-01 7.02970214e-01 6.32358186e-01\n",
      "  3.66936619e-01 3.11662188e-01 3.82071613e-01 7.29473287e-01\n",
      "  9.69348802e-03 6.19826105e-02 3.53985545e-01 2.69960403e-01\n",
      "  9.66145005e-01 7.30323908e-01 9.43742602e-01 7.51393238e-01\n",
      "  5.49226744e-01 5.10248208e-01 6.98472117e-01 5.46674433e-01\n",
      "  4.81237730e-01 5.08639146e-01 1.50950828e-01 3.67518908e-01\n",
      "  8.24293977e-01 9.09528683e-01 9.44590456e-02 8.25103231e-01\n",
      "  2.40391739e-01 9.05173816e-01 3.45899177e-01 2.32402919e-01\n",
      "  2.50712424e-01 4.80656585e-02 9.59471772e-01 1.43129208e-01\n",
      "  9.60566201e-01 6.71626988e-01 3.57138350e-01 9.39042239e-01\n",
      "  3.73587127e-01 2.37521396e-01 1.69441368e-01 7.89369209e-01\n",
      "  1.27679356e-01 6.36389578e-01 2.12392515e-01 7.46962119e-01\n",
      "  6.92349188e-01 9.55244813e-01 1.11653651e-01 5.90108602e-01\n",
      "  7.51867819e-02 6.89092873e-01 1.23325950e-01 8.00801233e-01\n",
      "  4.69701761e-01 3.68748572e-01 5.13021590e-01 9.94906131e-01\n",
      "  9.26164162e-01 5.79559755e-01 5.40851876e-01 9.24871522e-01\n",
      "  4.96930345e-01 3.09734471e-01 9.76920383e-01 8.82776193e-01\n",
      "  8.42206682e-01 5.27627429e-02 5.77236583e-01 9.42869496e-01\n",
      "  9.52958125e-01 8.25920995e-02 9.22202714e-01 9.69156311e-02\n",
      "  6.48951055e-01 9.51234347e-01 1.90827592e-01 5.94350923e-01\n",
      "  8.19261681e-01 8.25582680e-01 9.12815462e-01 7.80932391e-01\n",
      "  1.40318934e-01 3.04261704e-01 1.01063408e-01 2.08810807e-01\n",
      "  8.76155876e-01 5.92039686e-01 6.43759472e-02 6.72043587e-02\n",
      "  8.63067066e-02 2.24999762e-01 6.52603804e-01 6.14429129e-01\n",
      "  8.17662815e-01 4.13468368e-01 5.53457606e-01 9.86154773e-01\n",
      "  9.98200686e-01 7.21730515e-01 6.89876456e-01 1.79583041e-01\n",
      "  3.67410124e-01 5.29397482e-01 9.16698140e-01 2.23354237e-01\n",
      "  1.03072927e-01 7.59480610e-01 5.64292644e-01 4.23054082e-01\n",
      "  6.15086660e-01 2.35909004e-01 8.00330823e-01 9.25101458e-01\n",
      "  8.50859703e-01 5.93764827e-01 1.46412220e-02 9.47510181e-01\n",
      "  1.23536828e-01 6.63001688e-01 5.01387125e-01 1.08918006e-01\n",
      "  1.02236466e-01 7.68046963e-01 4.78180492e-01 7.49479196e-02\n",
      "  6.96284338e-01 8.02004018e-02 4.29913750e-01 5.64901753e-01\n",
      "  3.91935604e-01 5.87255975e-01 4.26960214e-01 4.31605305e-01\n",
      "  6.36784385e-02 3.29831804e-01 1.59518666e-01 7.22775445e-01\n",
      "  1.45314731e-01 3.38385093e-01 8.64588427e-01 7.93583070e-02\n",
      "  8.82643226e-01 1.26555718e-01 4.54616540e-01 7.47069006e-01\n",
      "  8.38643984e-01 7.88236065e-01 4.87976770e-01 1.02029313e-01\n",
      "  7.53819966e-01 8.80498845e-01 5.26663157e-01 1.70827412e-01\n",
      "  4.59574389e-01 9.68486461e-01 3.58476743e-01 5.99290231e-01\n",
      "  9.17976201e-01 7.24712427e-01 1.48535573e-01 5.61823440e-01\n",
      "  8.79338647e-01 3.57894718e-01 9.23137393e-01 3.06271819e-01\n",
      "  1.99890716e-01 5.77197143e-01 5.64172090e-01 6.50671483e-01\n",
      "  8.99019191e-01 9.87211937e-01 1.40558217e-01 4.20664441e-01\n",
      "  8.65565842e-01 4.93322829e-01 8.35998795e-02 7.62230013e-01\n",
      "  8.09690255e-01 7.33146865e-01 8.53772664e-01 7.27785811e-01\n",
      "  7.21591604e-01 5.75470949e-01 1.85629881e-01 7.60936411e-01\n",
      "  8.68983668e-01 9.52521880e-01 1.98955274e-01 7.96133717e-01\n",
      "  7.70794952e-02 9.90877329e-01 1.97831010e-01 4.74402473e-02\n",
      "  4.47465612e-01 7.70381209e-01 3.76055613e-01 8.45127462e-01\n",
      "  3.03895045e-01 2.27213483e-01 7.00348086e-02 3.80350603e-01\n",
      "  7.33102628e-02 7.45894824e-01 9.68571876e-01 1.31816922e-01\n",
      "  1.20611021e-03 3.14199196e-01 4.05170488e-01 8.80294614e-01\n",
      "  6.47534739e-01 1.54621360e-01 1.79612796e-01 4.07194249e-01\n",
      "  6.54974870e-01 5.79008543e-01 2.89471913e-01 9.09072900e-01\n",
      "  9.27790778e-01 9.15109698e-01 1.25439526e-01 1.68084537e-01\n",
      "  5.24875551e-01 6.45072858e-01 5.74189925e-02 8.80098849e-01\n",
      "  4.66105760e-01 4.75790104e-01 9.24942901e-01 5.30047022e-01\n",
      "  3.95266272e-01 7.50713545e-01 2.51708159e-01 9.60983686e-01\n",
      "  6.68694321e-01 8.39516848e-01 9.94637141e-01 7.65504483e-01\n",
      "  5.10784522e-01 3.75959704e-01 4.98994759e-01 5.77233237e-02\n",
      "  7.28728076e-01 7.74323343e-01 7.94847594e-01 4.15572849e-01\n",
      "  4.42381104e-01 2.55208033e-01 6.40357989e-01 3.30224830e-01\n",
      "  1.65399235e-01 3.51082023e-01 6.43729472e-01 4.72086530e-01\n",
      "  2.44006592e-01 4.43196229e-01 5.10140137e-01 4.09458138e-01\n",
      "  2.07553827e-01 8.90961965e-01 9.23900357e-01 6.80910903e-01\n",
      "  1.56216124e-01 3.89116695e-01 7.27516694e-01 6.75593127e-01\n",
      "  8.96695376e-01 2.88805832e-01 5.00044508e-01 7.30114482e-01\n",
      "  1.90250557e-01 5.63503272e-01 2.45774603e-01 3.42284767e-01\n",
      "  4.26171289e-01 8.07074307e-01 4.93452066e-01 8.56588965e-02\n",
      "  7.99065715e-01 3.11802282e-01 7.64808071e-01 7.40970280e-01\n",
      "  4.86185684e-01 1.69097368e-01 4.74840996e-01 3.51398718e-01\n",
      "  9.34615877e-01 9.79411988e-01 3.23962540e-01 4.71711604e-01\n",
      "  6.93311805e-02 9.35388223e-01 5.14902033e-01 1.31696558e-01\n",
      "  2.70935430e-01 1.35224029e-01 9.95832150e-01 1.72699833e-01\n",
      "  4.46676406e-01 4.99663671e-01 8.38707766e-01 2.13991568e-01\n",
      "  9.03473166e-01 9.88380917e-01 8.00233691e-01 5.48055281e-01\n",
      "  5.24019504e-01 5.01589034e-01 1.10476481e-01 8.55218103e-01\n",
      "  8.68570428e-01 9.84728163e-01 7.90510372e-01 5.71464563e-01\n",
      "  2.88331655e-01 9.33095879e-01 8.21230171e-01 6.92283487e-01\n",
      "  2.08968355e-01 5.16266828e-01 7.86199933e-02 2.31578272e-01\n",
      "  9.92598466e-01 6.36642779e-01 2.47823607e-01 8.49982912e-01\n",
      "  3.60307456e-01 1.71881820e-01 1.25636264e-01 4.33517234e-01\n",
      "  3.68697763e-01 9.46480702e-01 7.36785380e-01 3.60598156e-01\n",
      "  8.07353383e-01 3.95642693e-01 4.06497844e-01 7.38384395e-01\n",
      "  8.54088690e-01 6.67340698e-01 7.23893869e-01 1.74525855e-01\n",
      "  9.23414157e-02 8.46749003e-01 3.91481011e-01 8.22157268e-01\n",
      "  2.63934452e-01 4.67594725e-01 2.50130782e-01 5.23061823e-01\n",
      "  2.45271478e-01 5.58063929e-01 1.28885994e-01 2.39300351e-02\n",
      "  3.29891898e-01 8.42645291e-01 4.12456717e-01 7.62076142e-01\n",
      "  1.83389219e-01 3.04323946e-01 5.46352246e-01 7.63152243e-01\n",
      "  8.09627473e-01 2.56453631e-01 9.98518798e-01 7.98262231e-02\n",
      "  7.79704245e-01 8.79000835e-01 6.58049165e-01 9.70734852e-01\n",
      "  1.45855862e-01 9.45149559e-01 5.79478403e-02 4.42626694e-01\n",
      "  7.67002704e-01 7.57338991e-02 1.64855803e-01 6.43930850e-01\n",
      "  5.57350281e-01 8.70982488e-01 2.54178004e-01 3.17524668e-02\n",
      "  3.30292905e-01 4.70706175e-02 8.03728283e-01 2.99274293e-01\n",
      "  7.07825475e-01 3.52632084e-01 3.39155750e-01 2.17992373e-01\n",
      "  3.35992574e-01 1.39067283e-02 7.24112181e-01 8.14950256e-01\n",
      "  2.41680752e-01 1.75030124e-01 2.96007998e-02 9.32519572e-01\n",
      "  4.65669493e-01 9.79166545e-01 8.04198399e-01 1.61064842e-01\n",
      "  3.98143687e-01 1.59930023e-01 4.77525409e-01 2.24038174e-02\n",
      "  7.50469450e-01 8.18727188e-01 8.06213656e-01 5.60811460e-01\n",
      "  9.13317239e-01 8.93207061e-01 2.04382634e-01 3.99970789e-01\n",
      "  6.82208679e-01 4.78418360e-01 2.04219296e-01 8.36651475e-01\n",
      "  7.14368994e-01 2.42745539e-01 6.69392844e-01 4.88098271e-01\n",
      "  9.52268717e-01 2.82856358e-01 9.38244483e-01 7.98680019e-01\n",
      "  8.46969820e-01 8.50868456e-01 2.86422050e-03 2.92002546e-01\n",
      "  7.72445322e-01 7.34956656e-01 4.99523894e-01 9.60722122e-01\n",
      "  2.69294348e-01 7.67392315e-02 8.13090730e-01 9.14847588e-01\n",
      "  8.52977777e-01 1.87459515e-01 9.12955449e-01 8.93467301e-01\n",
      "  1.53601515e-01 8.52534910e-01 4.12788345e-01 6.47758702e-01\n",
      "  4.37666497e-01 6.53463342e-01 8.43429261e-01 2.65405038e-01\n",
      "  2.97431036e-01 7.01273624e-01 6.26216332e-01 1.01224610e-01\n",
      "  2.28360185e-01 5.24529475e-01 6.52704463e-01 7.56424967e-01\n",
      "  6.24628206e-01 4.88479669e-01 7.96426283e-01 9.54918418e-01\n",
      "  3.26719091e-01 2.17754637e-01 8.58659079e-01 2.18622769e-01\n",
      "  9.66927778e-01 8.53114742e-01 3.38967546e-01 7.39862898e-01\n",
      "  9.03358346e-01 7.61697724e-02 9.34547665e-01 5.81280908e-02\n",
      "  5.65201644e-01 5.48718243e-01 2.58709579e-01 4.96650707e-01\n",
      "  9.55191907e-01 6.37773267e-01 4.51724904e-01 8.25719475e-01\n",
      "  6.62389008e-01 7.54710110e-01 8.38827457e-01 4.09210999e-01\n",
      "  6.80227741e-01 2.08243578e-01 7.07875186e-02 2.82836112e-01\n",
      "  3.36610497e-01 5.06701106e-01 2.40687851e-01 9.35617204e-01\n",
      "  3.39850289e-02 4.14594191e-01 6.14993422e-01 5.42615468e-01\n",
      "  7.55461069e-01 2.23759381e-01 4.93278116e-01 9.54497998e-01\n",
      "  8.64456094e-02 6.86295539e-01 7.55474122e-01 1.94836800e-01\n",
      "  2.51695188e-01 2.73436356e-01 4.60028935e-01 3.83427469e-01\n",
      "  5.09566675e-01 6.18542690e-01 8.51098421e-01 2.35308874e-01\n",
      "  4.01873038e-01 6.53868498e-01 6.34527660e-01]]\n",
      "Change in theta: 1.4999873397802719e-06\n",
      "---------------------------------------------\n",
      "Iteration number 14 finished, Running time 272.6415820121765, Roo2 iteration 14 = -2041608.8878878083\n",
      "Iteration 1:\n",
      "Theta: [[6.82570388e-01 2.04518625e-01 3.44624208e-01 5.75114073e-01\n",
      "  1.67795804e-01 3.75645370e-01 1.16352752e-01 8.83584790e-01\n",
      "  5.22039350e-01 3.47338151e-01 8.63883025e-01 1.62099815e-01\n",
      "  1.26219301e-01 5.95403212e-01 9.41479990e-01 7.64162288e-01\n",
      "  5.77492325e-01 7.73451892e-01 1.63609638e-01 7.90839913e-01\n",
      "  9.35646906e-02 2.68794040e-01 3.84674832e-01 4.97084089e-01\n",
      "  4.94987057e-01 6.36572319e-01 3.65355174e-01 2.17180528e-01\n",
      "  4.22686747e-01 8.48217938e-01 8.36632317e-01 9.86253322e-01\n",
      "  4.69585789e-01 5.17765051e-01 1.62561119e-02 8.68935132e-01\n",
      "  6.89864721e-01 6.26071475e-01 4.87926225e-01 5.22120410e-01\n",
      "  4.13601566e-01 4.41752698e-01 7.98433749e-01 4.11086821e-01\n",
      "  7.66482044e-01 2.83446948e-01 6.01820130e-01 7.80406984e-02\n",
      "  9.99850071e-01 2.20897812e-01 7.66516221e-01 6.18844837e-01\n",
      "  6.52117691e-01 9.73276201e-01 3.28902353e-01 2.16752947e-02\n",
      "  3.20342697e-01 1.12518551e-01 5.44457959e-02 9.88051854e-01\n",
      "  6.05772424e-02 8.64987709e-01 4.53288785e-02 4.81473868e-01\n",
      "  7.91205841e-01 3.88680723e-01 4.32176607e-01 7.85681125e-02\n",
      "  1.92389055e-01 9.14162450e-01 7.07722946e-01 5.39054095e-02\n",
      "  8.76090634e-01 5.85047214e-01 5.29695758e-01 9.12782834e-01\n",
      "  7.89520179e-01 6.30185791e-01 9.55833838e-01 4.74086212e-01\n",
      "  4.29747994e-01 9.38224848e-01 6.08036275e-01 7.48819075e-01\n",
      "  5.09960173e-01 8.82825640e-01 9.07732110e-01 5.37806439e-01\n",
      "  5.87244548e-01 1.60269222e-01 8.63048209e-01 9.99061606e-02\n",
      "  9.15921164e-01 1.18100882e-01 5.79014130e-01 3.95714473e-01\n",
      "  2.48617051e-01 9.28308109e-01 2.19440613e-01 6.00803724e-02\n",
      "  9.26519152e-01 7.02467706e-01 4.80492634e-01 4.96206113e-01\n",
      "  6.63994318e-03 1.45514016e-01 4.15114985e-01 3.65119891e-02\n",
      "  7.29436932e-01 5.36878649e-01 3.39499614e-01 6.57796515e-01\n",
      "  1.25025558e-01 4.34487078e-01 8.53882953e-02 9.71361776e-01\n",
      "  2.27622833e-01 7.88508724e-01 8.18927782e-01 6.22104187e-01\n",
      "  9.20608408e-01 3.11785188e-01 1.56007903e-01 9.24309417e-01\n",
      "  5.82783935e-01 9.49793972e-01 3.92954640e-02 7.11917135e-01\n",
      "  5.12345649e-01 7.59405625e-01 8.43595575e-01 1.22670555e-01\n",
      "  4.76994722e-01 4.99923486e-01 3.46117213e-01 6.76125985e-01\n",
      "  1.99222857e-01 3.46421930e-01 7.67286179e-01 6.21643653e-02\n",
      "  4.65663841e-01 1.35886019e-01 2.75088630e-02 8.14444117e-01\n",
      "  5.12283789e-01 2.89721336e-01 2.26162319e-01 8.59429388e-01\n",
      "  8.31769485e-01 7.21029060e-01 2.36534625e-01 3.34410450e-01\n",
      "  5.18734209e-03 2.74523169e-01 9.88344232e-02 8.88441999e-01\n",
      "  6.15305290e-01 3.40353559e-01 8.85029042e-01 4.82982041e-01\n",
      "  6.52138346e-01 5.35468598e-01 8.15432145e-01 4.69729897e-01\n",
      "  9.84670434e-01 4.41096597e-02 8.85723640e-01 6.94567324e-01\n",
      "  8.33647181e-01 4.62202477e-01 6.11423718e-01 2.26859298e-01\n",
      "  7.93880791e-01 2.29913302e-01 4.08258345e-01 4.88005850e-01\n",
      "  3.36779755e-01 3.71370527e-01 6.58971188e-03 3.56349676e-01\n",
      "  7.18392250e-01 1.38381679e-01 9.48264611e-01 1.24275855e-01\n",
      "  8.18095077e-01 7.58728744e-01 5.16487002e-01 3.77491125e-01\n",
      "  1.32112724e-01 8.55639071e-01 9.48057868e-01 2.25136755e-01\n",
      "  5.16196601e-01 7.49106852e-01 6.40106880e-01 8.66493962e-01\n",
      "  1.47634370e-01 7.42604516e-01 6.45739345e-01 7.58020953e-01\n",
      "  8.00794285e-01 2.21038007e-01 6.04455617e-03 6.27647583e-01\n",
      "  2.51514451e-01 7.70838368e-01 1.97893425e-01 9.04969989e-01\n",
      "  8.38357720e-01 4.17777233e-01 7.56714041e-01 5.77378842e-01\n",
      "  9.48718136e-01 6.54355947e-01 5.91204776e-01 9.88761069e-01\n",
      "  5.36358506e-01 9.00711368e-01 1.28364606e-01 3.92138150e-01\n",
      "  9.80297990e-01 1.78502390e-01 5.28768575e-01 2.79121725e-01\n",
      "  8.13254854e-04 4.24807252e-01 3.87219215e-01 8.86484766e-01\n",
      "  3.16546562e-01 2.40259564e-01 1.78148963e-01 5.45047468e-01\n",
      "  9.52035289e-02 1.16798917e-01 4.73097677e-01 2.60887105e-02\n",
      "  9.36590489e-01 4.60455768e-01 9.32280502e-01 8.83187208e-01\n",
      "  6.57913186e-02 3.07761818e-01 1.89244355e-01 8.87245191e-01\n",
      "  6.49860958e-01 2.15839348e-01 1.20430404e-01 8.95815824e-01\n",
      "  9.55022587e-01 5.13466979e-01 2.84490766e-01 2.51493095e-01\n",
      "  6.40163140e-01 7.51367649e-01 3.00911043e-01 5.88146273e-01\n",
      "  4.87418229e-01 8.58460373e-01 1.42281442e-01 7.98693655e-01\n",
      "  2.73849309e-01 3.83837409e-01 8.75994498e-01 5.68027801e-01\n",
      "  9.93541838e-01 2.36373053e-01 1.41585232e-01 1.24071398e-01\n",
      "  5.27909885e-01 3.77266999e-01 1.40539598e-02 5.92225441e-01\n",
      "  2.26229887e-01 2.20734254e-01 9.22160503e-01 3.01005029e-01\n",
      "  8.47036397e-01 4.54761783e-01 7.00672394e-01 4.32775052e-01\n",
      "  2.82780878e-01 1.21260371e-02 9.12966132e-01 9.16858374e-02\n",
      "  4.80184390e-01 7.40664819e-01 1.07384731e-01 5.77280458e-01\n",
      "  2.73549065e-01 8.93721030e-01 8.18050301e-01 1.64502584e-01\n",
      "  3.99035955e-01 2.98318148e-01 1.90843254e-02 3.37453765e-01\n",
      "  8.73760997e-01 5.34793386e-01 1.41309965e-02 1.63434245e-01\n",
      "  7.28868972e-01 6.17555743e-01 5.02944897e-01 4.73828011e-01\n",
      "  9.99532044e-01 4.57483480e-02 1.04860280e-03 8.25579203e-01\n",
      "  3.13220003e-01 2.34251921e-02 5.74196239e-01 3.39239081e-01\n",
      "  2.72266708e-01 6.21526054e-01 7.02699391e-01 1.13283545e-01\n",
      "  4.60246946e-01 1.75225276e-01 8.24620788e-01 7.05555524e-02\n",
      "  6.04569905e-01 8.05309090e-01 9.78891617e-01 5.74003942e-01\n",
      "  5.04727848e-01 2.05052476e-01 2.04862992e-01 9.94128112e-01\n",
      "  4.67017283e-02 9.70115659e-01 9.73000562e-01 1.93456354e-01\n",
      "  9.06027511e-01 1.03413788e-01 2.73916143e-01 1.36028116e-01\n",
      "  2.00720372e-01 5.62373004e-01 6.69030986e-01 8.94445737e-01\n",
      "  1.83286794e-01 6.23315874e-01 3.17592922e-01 6.02589804e-01\n",
      "  1.42336792e-01 4.40901526e-01 1.35384244e-01 4.46416681e-01\n",
      "  8.82990491e-01 7.66575392e-01 8.33553049e-01 6.51916087e-01\n",
      "  4.96792143e-01 1.96710661e-01 4.71316013e-01 8.09274854e-01\n",
      "  9.68952779e-01 1.04465226e-01 9.82723689e-02 5.16522266e-02\n",
      "  5.56994378e-01 3.29729247e-01 4.34730177e-01 7.55828653e-01\n",
      "  8.99335035e-01 7.09523426e-01 2.51635623e-01 7.89581954e-01\n",
      "  1.51670669e-01 2.22345789e-01 2.34427910e-01 5.26799193e-01\n",
      "  7.36715711e-01 4.32951056e-01 2.72564963e-02 7.41955734e-01\n",
      "  5.81487322e-01 6.78597081e-01 3.52223897e-01 3.95196415e-01\n",
      "  2.58019121e-01 9.22850473e-01 5.25840818e-01 8.64683665e-01\n",
      "  8.70256667e-02 6.75848827e-01 1.13515272e-01 8.76538644e-01\n",
      "  8.34087487e-01 2.01392091e-01 1.08221943e-01 4.38356415e-01\n",
      "  2.80594738e-01 4.26075359e-01 7.02971154e-01 6.32359125e-01\n",
      "  3.66937559e-01 3.11663127e-01 3.82072553e-01 7.29474227e-01\n",
      "  9.69442753e-03 6.19835500e-02 3.53986484e-01 2.69961342e-01\n",
      "  9.66145944e-01 7.30324847e-01 9.43743542e-01 7.51394178e-01\n",
      "  5.49227683e-01 5.10249147e-01 6.98473057e-01 5.46675373e-01\n",
      "  4.81238669e-01 5.08640085e-01 1.50951768e-01 3.67519848e-01\n",
      "  8.24294916e-01 9.09529623e-01 9.44599851e-02 8.25104170e-01\n",
      "  2.40392678e-01 9.05174756e-01 3.45900116e-01 2.32403858e-01\n",
      "  2.50713364e-01 4.80665980e-02 9.59472712e-01 1.43130147e-01\n",
      "  9.60567140e-01 6.71627928e-01 3.57139289e-01 9.39043178e-01\n",
      "  3.73588067e-01 2.37522335e-01 1.69442308e-01 7.89370148e-01\n",
      "  1.27680295e-01 6.36390518e-01 2.12393454e-01 7.46963058e-01\n",
      "  6.92350127e-01 9.55245753e-01 1.11654590e-01 5.90109542e-01\n",
      "  7.51877214e-02 6.89093813e-01 1.23326890e-01 8.00802173e-01\n",
      "  4.69702700e-01 3.68749512e-01 5.13022529e-01 9.94907071e-01\n",
      "  9.26165102e-01 5.79560694e-01 5.40852816e-01 9.24872461e-01\n",
      "  4.96931285e-01 3.09735411e-01 9.76921323e-01 8.82777133e-01\n",
      "  8.42207621e-01 5.27636824e-02 5.77237523e-01 9.42870435e-01\n",
      "  9.52959064e-01 8.25930390e-02 9.22203653e-01 9.69165707e-02\n",
      "  6.48951994e-01 9.51235287e-01 1.90828532e-01 5.94351862e-01\n",
      "  8.19262621e-01 8.25583620e-01 9.12816401e-01 7.80933331e-01\n",
      "  1.40319874e-01 3.04262643e-01 1.01064348e-01 2.08811746e-01\n",
      "  8.76156816e-01 5.92040626e-01 6.43768867e-02 6.72052982e-02\n",
      "  8.63076461e-02 2.25000701e-01 6.52604743e-01 6.14430068e-01\n",
      "  8.17663755e-01 4.13469307e-01 5.53458546e-01 9.86155712e-01\n",
      "  9.98201625e-01 7.21731455e-01 6.89877395e-01 1.79583980e-01\n",
      "  3.67411063e-01 5.29398421e-01 9.16699079e-01 2.23355176e-01\n",
      "  1.03073867e-01 7.59481549e-01 5.64293583e-01 4.23055021e-01\n",
      "  6.15087600e-01 2.35909943e-01 8.00331762e-01 9.25102398e-01\n",
      "  8.50860643e-01 5.93765767e-01 1.46421615e-02 9.47511120e-01\n",
      "  1.23537768e-01 6.63002627e-01 5.01388065e-01 1.08918946e-01\n",
      "  1.02237405e-01 7.68047903e-01 4.78181431e-01 7.49488591e-02\n",
      "  6.96285277e-01 8.02013413e-02 4.29914689e-01 5.64902693e-01\n",
      "  3.91936543e-01 5.87256914e-01 4.26961154e-01 4.31606245e-01\n",
      "  6.36793780e-02 3.29832743e-01 1.59519605e-01 7.22776384e-01\n",
      "  1.45315670e-01 3.38386033e-01 8.64589367e-01 7.93592466e-02\n",
      "  8.82644166e-01 1.26556657e-01 4.54617479e-01 7.47069946e-01\n",
      "  8.38644923e-01 7.88237004e-01 4.87977710e-01 1.02030252e-01\n",
      "  7.53820906e-01 8.80499785e-01 5.26664096e-01 1.70828352e-01\n",
      "  4.59575328e-01 9.68487401e-01 3.58477682e-01 5.99291171e-01\n",
      "  9.17977141e-01 7.24713367e-01 1.48536513e-01 5.61824380e-01\n",
      "  8.79339587e-01 3.57895658e-01 9.23138332e-01 3.06272759e-01\n",
      "  1.99891656e-01 5.77198083e-01 5.64173030e-01 6.50672422e-01\n",
      "  8.99020130e-01 9.87212876e-01 1.40559156e-01 4.20665380e-01\n",
      "  8.65566781e-01 4.93323769e-01 8.36008190e-02 7.62230953e-01\n",
      "  8.09691194e-01 7.33147805e-01 8.53773604e-01 7.27786750e-01\n",
      "  7.21592544e-01 5.75471888e-01 1.85630820e-01 7.60937351e-01\n",
      "  8.68984608e-01 9.52522819e-01 1.98956214e-01 7.96134657e-01\n",
      "  7.70804347e-02 9.90878269e-01 1.97831950e-01 4.74411868e-02\n",
      "  4.47466551e-01 7.70382149e-01 3.76056552e-01 8.45128401e-01\n",
      "  3.03895985e-01 2.27214423e-01 7.00357482e-02 3.80351542e-01\n",
      "  7.33112023e-02 7.45895763e-01 9.68572815e-01 1.31817862e-01\n",
      "  1.20704972e-03 3.14200136e-01 4.05171427e-01 8.80295553e-01\n",
      "  6.47535678e-01 1.54622300e-01 1.79613736e-01 4.07195188e-01\n",
      "  6.54975810e-01 5.79009482e-01 2.89472852e-01 9.09073839e-01\n",
      "  9.27791718e-01 9.15110637e-01 1.25440465e-01 1.68085477e-01\n",
      "  5.24876491e-01 6.45073797e-01 5.74199320e-02 8.80099788e-01\n",
      "  4.66106699e-01 4.75791044e-01 9.24943840e-01 5.30047962e-01\n",
      "  3.95267212e-01 7.50714484e-01 2.51709098e-01 9.60984625e-01\n",
      "  6.68695260e-01 8.39517788e-01 9.94638080e-01 7.65505423e-01\n",
      "  5.10785461e-01 3.75960643e-01 4.98995698e-01 5.77242632e-02\n",
      "  7.28729015e-01 7.74324282e-01 7.94848533e-01 4.15573788e-01\n",
      "  4.42382044e-01 2.55208973e-01 6.40358928e-01 3.30225770e-01\n",
      "  1.65400174e-01 3.51082963e-01 6.43730412e-01 4.72087469e-01\n",
      "  2.44007531e-01 4.43197169e-01 5.10141077e-01 4.09459078e-01\n",
      "  2.07554767e-01 8.90962904e-01 9.23901296e-01 6.80911843e-01\n",
      "  1.56217064e-01 3.89117634e-01 7.27517634e-01 6.75594067e-01\n",
      "  8.96696316e-01 2.88806771e-01 5.00045448e-01 7.30115422e-01\n",
      "  1.90251496e-01 5.63504212e-01 2.45775542e-01 3.42285706e-01\n",
      "  4.26172229e-01 8.07075247e-01 4.93453005e-01 8.56598360e-02\n",
      "  7.99066655e-01 3.11803222e-01 7.64809010e-01 7.40971219e-01\n",
      "  4.86186624e-01 1.69098308e-01 4.74841935e-01 3.51399658e-01\n",
      "  9.34616816e-01 9.79412928e-01 3.23963480e-01 4.71712544e-01\n",
      "  6.93321200e-02 9.35389162e-01 5.14902972e-01 1.31697498e-01\n",
      "  2.70936370e-01 1.35224968e-01 9.95833089e-01 1.72700773e-01\n",
      "  4.46677345e-01 4.99664611e-01 8.38708706e-01 2.13992508e-01\n",
      "  9.03474105e-01 9.88381857e-01 8.00234631e-01 5.48056220e-01\n",
      "  5.24020443e-01 5.01589974e-01 1.10477421e-01 8.55219043e-01\n",
      "  8.68571368e-01 9.84729102e-01 7.90511312e-01 5.71465503e-01\n",
      "  2.88332595e-01 9.33096818e-01 8.21231111e-01 6.92284427e-01\n",
      "  2.08969295e-01 5.16267767e-01 7.86209328e-02 2.31579212e-01\n",
      "  9.92599405e-01 6.36643718e-01 2.47824547e-01 8.49983851e-01\n",
      "  3.60308396e-01 1.71882760e-01 1.25637203e-01 4.33518174e-01\n",
      "  3.68698703e-01 9.46481642e-01 7.36786319e-01 3.60599095e-01\n",
      "  8.07354323e-01 3.95643632e-01 4.06498783e-01 7.38385335e-01\n",
      "  8.54089630e-01 6.67341637e-01 7.23894808e-01 1.74526794e-01\n",
      "  9.23423552e-02 8.46749942e-01 3.91481950e-01 8.22158207e-01\n",
      "  2.63935391e-01 4.67595665e-01 2.50131721e-01 5.23062762e-01\n",
      "  2.45272418e-01 5.58064869e-01 1.28886934e-01 2.39309746e-02\n",
      "  3.29892837e-01 8.42646230e-01 4.12457657e-01 7.62077081e-01\n",
      "  1.83390158e-01 3.04324886e-01 5.46353185e-01 7.63153182e-01\n",
      "  8.09628413e-01 2.56454570e-01 9.98519738e-01 7.98271626e-02\n",
      "  7.79705184e-01 8.79001775e-01 6.58050105e-01 9.70735791e-01\n",
      "  1.45856802e-01 9.45150498e-01 5.79487799e-02 4.42627634e-01\n",
      "  7.67003643e-01 7.57348386e-02 1.64856742e-01 6.43931789e-01\n",
      "  5.57351221e-01 8.70983427e-01 2.54178944e-01 3.17534063e-02\n",
      "  3.30293845e-01 4.70715570e-02 8.03729223e-01 2.99275233e-01\n",
      "  7.07826414e-01 3.52633024e-01 3.39156690e-01 2.17993313e-01\n",
      "  3.35993513e-01 1.39076678e-02 7.24113120e-01 8.14951195e-01\n",
      "  2.41681691e-01 1.75031063e-01 2.96017393e-02 9.32520512e-01\n",
      "  4.65670432e-01 9.79167484e-01 8.04199338e-01 1.61065782e-01\n",
      "  3.98144627e-01 1.59930963e-01 4.77526349e-01 2.24047569e-02\n",
      "  7.50470390e-01 8.18728128e-01 8.06214595e-01 5.60812400e-01\n",
      "  9.13318179e-01 8.93208001e-01 2.04383574e-01 3.99971729e-01\n",
      "  6.82209618e-01 4.78419299e-01 2.04220235e-01 8.36652414e-01\n",
      "  7.14369934e-01 2.42746479e-01 6.69393784e-01 4.88099211e-01\n",
      "  9.52269656e-01 2.82857298e-01 9.38245422e-01 7.98680959e-01\n",
      "  8.46970759e-01 8.50869395e-01 2.86516001e-03 2.92003485e-01\n",
      "  7.72446261e-01 7.34957595e-01 4.99524833e-01 9.60723061e-01\n",
      "  2.69295288e-01 7.67401710e-02 8.13091669e-01 9.14848527e-01\n",
      "  8.52978717e-01 1.87460455e-01 9.12956388e-01 8.93468241e-01\n",
      "  1.53602455e-01 8.52535850e-01 4.12789285e-01 6.47759641e-01\n",
      "  4.37667437e-01 6.53464282e-01 8.43430201e-01 2.65405978e-01\n",
      "  2.97431975e-01 7.01274564e-01 6.26217272e-01 1.01225550e-01\n",
      "  2.28361124e-01 5.24530415e-01 6.52705403e-01 7.56425907e-01\n",
      "  6.24629146e-01 4.88480608e-01 7.96427222e-01 9.54919357e-01\n",
      "  3.26720031e-01 2.17755577e-01 8.58660019e-01 2.18623709e-01\n",
      "  9.66928718e-01 8.53115682e-01 3.38968485e-01 7.39863838e-01\n",
      "  9.03359286e-01 7.61707119e-02 9.34548604e-01 5.81290303e-02\n",
      "  5.65202584e-01 5.48719182e-01 2.58710518e-01 4.96651646e-01\n",
      "  9.55192847e-01 6.37774206e-01 4.51725844e-01 8.25720415e-01\n",
      "  6.62389948e-01 7.54711050e-01 8.38828397e-01 4.09211939e-01\n",
      "  6.80228680e-01 2.08244518e-01 7.07884581e-02 2.82837051e-01\n",
      "  3.36611437e-01 5.06702046e-01 2.40688790e-01 9.35618144e-01\n",
      "  3.39859684e-02 4.14595131e-01 6.14994361e-01 5.42616407e-01\n",
      "  7.55462008e-01 2.23760320e-01 4.93279055e-01 9.54498938e-01\n",
      "  8.64465489e-02 6.86296478e-01 7.55475062e-01 1.94837740e-01\n",
      "  2.51696128e-01 2.73437295e-01 4.60029875e-01 3.83428409e-01\n",
      "  5.09567615e-01 6.18543629e-01 8.51099360e-01 2.35309813e-01\n",
      "  4.01873977e-01 6.53869437e-01 6.34528600e-01]]\n",
      "Change in theta: 1.5702513842084957e-06\n",
      "---------------------------------------------\n",
      "Iteration number 15 finished, Running time 281.6573739051819, Roo2 iteration 15 = -2434352.306908813\n",
      "Iteration 1:\n",
      "Theta: [[6.82571125e-01 2.04519363e-01 3.44624946e-01 5.75114810e-01\n",
      "  1.67796541e-01 3.75646107e-01 1.16353489e-01 8.83585528e-01\n",
      "  5.22040087e-01 3.47338888e-01 8.63883763e-01 1.62100552e-01\n",
      "  1.26220038e-01 5.95403949e-01 9.41480727e-01 7.64163026e-01\n",
      "  5.77493062e-01 7.73452630e-01 1.63610376e-01 7.90840650e-01\n",
      "  9.35654279e-02 2.68794777e-01 3.84675570e-01 4.97084826e-01\n",
      "  4.94987794e-01 6.36573056e-01 3.65355912e-01 2.17181265e-01\n",
      "  4.22687484e-01 8.48218676e-01 8.36633055e-01 9.86254059e-01\n",
      "  4.69586526e-01 5.17765788e-01 1.62568492e-02 8.68935870e-01\n",
      "  6.89865458e-01 6.26072213e-01 4.87926963e-01 5.22121147e-01\n",
      "  4.13602303e-01 4.41753435e-01 7.98434486e-01 4.11087559e-01\n",
      "  7.66482781e-01 2.83447686e-01 6.01820867e-01 7.80414358e-02\n",
      "  9.99850808e-01 2.20898549e-01 7.66516958e-01 6.18845574e-01\n",
      "  6.52118428e-01 9.73276938e-01 3.28903090e-01 2.16760320e-02\n",
      "  3.20343435e-01 1.12519288e-01 5.44465332e-02 9.88052592e-01\n",
      "  6.05779797e-02 8.64988446e-01 4.53296158e-02 4.81474605e-01\n",
      "  7.91206578e-01 3.88681460e-01 4.32177344e-01 7.85688499e-02\n",
      "  1.92389792e-01 9.14163187e-01 7.07723683e-01 5.39061468e-02\n",
      "  8.76091372e-01 5.85047951e-01 5.29696495e-01 9.12783571e-01\n",
      "  7.89520916e-01 6.30186528e-01 9.55834575e-01 4.74086950e-01\n",
      "  4.29748732e-01 9.38225586e-01 6.08037013e-01 7.48819812e-01\n",
      "  5.09960910e-01 8.82826377e-01 9.07732847e-01 5.37807176e-01\n",
      "  5.87245285e-01 1.60269960e-01 8.63048947e-01 9.99068979e-02\n",
      "  9.15921902e-01 1.18101619e-01 5.79014867e-01 3.95715210e-01\n",
      "  2.48617788e-01 9.28308847e-01 2.19441350e-01 6.00811097e-02\n",
      "  9.26519889e-01 7.02468444e-01 4.80493371e-01 4.96206851e-01\n",
      "  6.64068051e-03 1.45514753e-01 4.15115723e-01 3.65127264e-02\n",
      "  7.29437669e-01 5.36879386e-01 3.39500352e-01 6.57797252e-01\n",
      "  1.25026295e-01 4.34487815e-01 8.53890327e-02 9.71362513e-01\n",
      "  2.27623571e-01 7.88509461e-01 8.18928519e-01 6.22104924e-01\n",
      "  9.20609145e-01 3.11785925e-01 1.56008640e-01 9.24310154e-01\n",
      "  5.82784673e-01 9.49794710e-01 3.92962013e-02 7.11917872e-01\n",
      "  5.12346386e-01 7.59406362e-01 8.43596313e-01 1.22671292e-01\n",
      "  4.76995460e-01 4.99924223e-01 3.46117951e-01 6.76126723e-01\n",
      "  1.99223594e-01 3.46422667e-01 7.67286916e-01 6.21651026e-02\n",
      "  4.65664578e-01 1.35886757e-01 2.75096004e-02 8.14444854e-01\n",
      "  5.12284526e-01 2.89722073e-01 2.26163056e-01 8.59430125e-01\n",
      "  8.31770222e-01 7.21029798e-01 2.36535363e-01 3.34411187e-01\n",
      "  5.18807942e-03 2.74523906e-01 9.88351605e-02 8.88442737e-01\n",
      "  6.15306027e-01 3.40354297e-01 8.85029779e-01 4.82982779e-01\n",
      "  6.52139084e-01 5.35469335e-01 8.15432883e-01 4.69730634e-01\n",
      "  9.84671172e-01 4.41103971e-02 8.85724378e-01 6.94568061e-01\n",
      "  8.33647918e-01 4.62203214e-01 6.11424455e-01 2.26860035e-01\n",
      "  7.93881528e-01 2.29914039e-01 4.08259083e-01 4.88006587e-01\n",
      "  3.36780492e-01 3.71371265e-01 6.59044921e-03 3.56350414e-01\n",
      "  7.18392988e-01 1.38382416e-01 9.48265349e-01 1.24276593e-01\n",
      "  8.18095814e-01 7.58729482e-01 5.16487739e-01 3.77491863e-01\n",
      "  1.32113461e-01 8.55639809e-01 9.48058605e-01 2.25137492e-01\n",
      "  5.16197339e-01 7.49107589e-01 6.40107617e-01 8.66494699e-01\n",
      "  1.47635107e-01 7.42605253e-01 6.45740083e-01 7.58021690e-01\n",
      "  8.00795022e-01 2.21038744e-01 6.04529350e-03 6.27648320e-01\n",
      "  2.51515189e-01 7.70839106e-01 1.97894162e-01 9.04970726e-01\n",
      "  8.38358457e-01 4.17777970e-01 7.56714779e-01 5.77379579e-01\n",
      "  9.48718873e-01 6.54356684e-01 5.91205513e-01 9.88761807e-01\n",
      "  5.36359243e-01 9.00712106e-01 1.28365343e-01 3.92138887e-01\n",
      "  9.80298727e-01 1.78503127e-01 5.28769313e-01 2.79122463e-01\n",
      "  8.13992182e-04 4.24807989e-01 3.87219952e-01 8.86485503e-01\n",
      "  3.16547300e-01 2.40260302e-01 1.78149700e-01 5.45048205e-01\n",
      "  9.52042662e-02 1.16799654e-01 4.73098414e-01 2.60894478e-02\n",
      "  9.36591226e-01 4.60456506e-01 9.32281240e-01 8.83187945e-01\n",
      "  6.57920560e-02 3.07762555e-01 1.89245093e-01 8.87245929e-01\n",
      "  6.49861696e-01 2.15840086e-01 1.20431141e-01 8.95816562e-01\n",
      "  9.55023324e-01 5.13467716e-01 2.84491504e-01 2.51493832e-01\n",
      "  6.40163877e-01 7.51368387e-01 3.00911780e-01 5.88147010e-01\n",
      "  4.87418966e-01 8.58461110e-01 1.42282179e-01 7.98694393e-01\n",
      "  2.73850047e-01 3.83838146e-01 8.75995236e-01 5.68028539e-01\n",
      "  9.93542576e-01 2.36373791e-01 1.41585969e-01 1.24072135e-01\n",
      "  5.27910622e-01 3.77267736e-01 1.40546971e-02 5.92226179e-01\n",
      "  2.26230624e-01 2.20734991e-01 9.22161240e-01 3.01005766e-01\n",
      "  8.47037134e-01 4.54762520e-01 7.00673131e-01 4.32775789e-01\n",
      "  2.82781616e-01 1.21267745e-02 9.12966869e-01 9.16865747e-02\n",
      "  4.80185127e-01 7.40665557e-01 1.07385468e-01 5.77281196e-01\n",
      "  2.73549803e-01 8.93721768e-01 8.18051039e-01 1.64503321e-01\n",
      "  3.99036693e-01 2.98318885e-01 1.90850627e-02 3.37454503e-01\n",
      "  8.73761734e-01 5.34794123e-01 1.41317338e-02 1.63434983e-01\n",
      "  7.28869709e-01 6.17556480e-01 5.02945634e-01 4.73828748e-01\n",
      "  9.99532781e-01 4.57490854e-02 1.04934013e-03 8.25579940e-01\n",
      "  3.13220741e-01 2.34259294e-02 5.74196977e-01 3.39239818e-01\n",
      "  2.72267445e-01 6.21526791e-01 7.02700128e-01 1.13284282e-01\n",
      "  4.60247683e-01 1.75226013e-01 8.24621525e-01 7.05562898e-02\n",
      "  6.04570642e-01 8.05309828e-01 9.78892354e-01 5.74004679e-01\n",
      "  5.04728586e-01 2.05053213e-01 2.04863730e-01 9.94128850e-01\n",
      "  4.67024656e-02 9.70116397e-01 9.73001300e-01 1.93457091e-01\n",
      "  9.06028248e-01 1.03414526e-01 2.73916881e-01 1.36028853e-01\n",
      "  2.00721110e-01 5.62373741e-01 6.69031723e-01 8.94446475e-01\n",
      "  1.83287532e-01 6.23316612e-01 3.17593659e-01 6.02590541e-01\n",
      "  1.42337530e-01 4.40902263e-01 1.35384981e-01 4.46417419e-01\n",
      "  8.82991228e-01 7.66576129e-01 8.33553786e-01 6.51916824e-01\n",
      "  4.96792881e-01 1.96711398e-01 4.71316751e-01 8.09275591e-01\n",
      "  9.68953517e-01 1.04465964e-01 9.82731062e-02 5.16529639e-02\n",
      "  5.56995116e-01 3.29729984e-01 4.34730915e-01 7.55829391e-01\n",
      "  8.99335772e-01 7.09524163e-01 2.51636361e-01 7.89582692e-01\n",
      "  1.51671407e-01 2.22346527e-01 2.34428647e-01 5.26799930e-01\n",
      "  7.36716448e-01 4.32951794e-01 2.72572336e-02 7.41956471e-01\n",
      "  5.81488059e-01 6.78597818e-01 3.52224634e-01 3.95197152e-01\n",
      "  2.58019858e-01 9.22851210e-01 5.25841556e-01 8.64684403e-01\n",
      "  8.70264041e-02 6.75849565e-01 1.13516010e-01 8.76539382e-01\n",
      "  8.34088224e-01 2.01392828e-01 1.08222680e-01 4.38357152e-01\n",
      "  2.80595475e-01 4.26076096e-01 7.02971891e-01 6.32359863e-01\n",
      "  3.66938296e-01 3.11663865e-01 3.82073290e-01 7.29474964e-01\n",
      "  9.69516485e-03 6.19842874e-02 3.53987222e-01 2.69962080e-01\n",
      "  9.66146682e-01 7.30325585e-01 9.43744279e-01 7.51394915e-01\n",
      "  5.49228420e-01 5.10249885e-01 6.98473794e-01 5.46676110e-01\n",
      "  4.81239407e-01 5.08640823e-01 1.50952505e-01 3.67520585e-01\n",
      "  8.24295654e-01 9.09530360e-01 9.44607225e-02 8.25104907e-01\n",
      "  2.40393416e-01 9.05175493e-01 3.45900854e-01 2.32404596e-01\n",
      "  2.50714101e-01 4.80673353e-02 9.59473449e-01 1.43130885e-01\n",
      "  9.60567877e-01 6.71628665e-01 3.57140027e-01 9.39043915e-01\n",
      "  3.73588804e-01 2.37523073e-01 1.69443045e-01 7.89370886e-01\n",
      "  1.27681033e-01 6.36391255e-01 2.12394191e-01 7.46963796e-01\n",
      "  6.92350864e-01 9.55246490e-01 1.11655328e-01 5.90110279e-01\n",
      "  7.51884587e-02 6.89094550e-01 1.23327627e-01 8.00802910e-01\n",
      "  4.69703438e-01 3.68750249e-01 5.13023267e-01 9.94907808e-01\n",
      "  9.26165839e-01 5.79561432e-01 5.40853553e-01 9.24873199e-01\n",
      "  4.96932022e-01 3.09736148e-01 9.76922060e-01 8.82777870e-01\n",
      "  8.42208358e-01 5.27644198e-02 5.77238260e-01 9.42871172e-01\n",
      "  9.52959802e-01 8.25937763e-02 9.22204391e-01 9.69173080e-02\n",
      "  6.48952732e-01 9.51236024e-01 1.90829269e-01 5.94352600e-01\n",
      "  8.19263358e-01 8.25584357e-01 9.12817138e-01 7.80934068e-01\n",
      "  1.40320611e-01 3.04263381e-01 1.01065085e-01 2.08812483e-01\n",
      "  8.76157553e-01 5.92041363e-01 6.43776240e-02 6.72060355e-02\n",
      "  8.63083834e-02 2.25001439e-01 6.52605481e-01 6.14430805e-01\n",
      "  8.17664492e-01 4.13470044e-01 5.53459283e-01 9.86156449e-01\n",
      "  9.98202362e-01 7.21732192e-01 6.89878133e-01 1.79584718e-01\n",
      "  3.67411800e-01 5.29399158e-01 9.16699817e-01 2.23355914e-01\n",
      "  1.03074604e-01 7.59482287e-01 5.64294320e-01 4.23055759e-01\n",
      "  6.15088337e-01 2.35910681e-01 8.00332499e-01 9.25103135e-01\n",
      "  8.50861380e-01 5.93766504e-01 1.46428988e-02 9.47511858e-01\n",
      "  1.23538505e-01 6.63003365e-01 5.01388802e-01 1.08919683e-01\n",
      "  1.02238142e-01 7.68048640e-01 4.78182168e-01 7.49495965e-02\n",
      "  6.96286015e-01 8.02020786e-02 4.29915427e-01 5.64903430e-01\n",
      "  3.91937281e-01 5.87257652e-01 4.26961891e-01 4.31606982e-01\n",
      "  6.36801154e-02 3.29833481e-01 1.59520343e-01 7.22777122e-01\n",
      "  1.45316407e-01 3.38386770e-01 8.64590104e-01 7.93599839e-02\n",
      "  8.82644903e-01 1.26557394e-01 4.54618217e-01 7.47070683e-01\n",
      "  8.38645661e-01 7.88237742e-01 4.87978447e-01 1.02030990e-01\n",
      "  7.53821643e-01 8.80500522e-01 5.26664833e-01 1.70829089e-01\n",
      "  4.59576066e-01 9.68488138e-01 3.58478420e-01 5.99291908e-01\n",
      "  9.17977878e-01 7.24714104e-01 1.48537250e-01 5.61825117e-01\n",
      "  8.79340324e-01 3.57896395e-01 9.23139069e-01 3.06273496e-01\n",
      "  1.99892393e-01 5.77198820e-01 5.64173767e-01 6.50673160e-01\n",
      "  8.99020867e-01 9.87213614e-01 1.40559894e-01 4.20666118e-01\n",
      "  8.65567519e-01 4.93324506e-01 8.36015564e-02 7.62231690e-01\n",
      "  8.09691932e-01 7.33148542e-01 8.53774341e-01 7.27787488e-01\n",
      "  7.21593281e-01 5.75472626e-01 1.85631558e-01 7.60938088e-01\n",
      "  8.68985345e-01 9.52523556e-01 1.98956951e-01 7.96135394e-01\n",
      "  7.70811720e-02 9.90879006e-01 1.97832687e-01 4.74419241e-02\n",
      "  4.47467289e-01 7.70382886e-01 3.76057289e-01 8.45129138e-01\n",
      "  3.03896722e-01 2.27215160e-01 7.00364855e-02 3.80352280e-01\n",
      "  7.33119396e-02 7.45896501e-01 9.68573552e-01 1.31818599e-01\n",
      "  1.20778705e-03 3.14200873e-01 4.05172165e-01 8.80296291e-01\n",
      "  6.47536416e-01 1.54623037e-01 1.79614473e-01 4.07195926e-01\n",
      "  6.54976547e-01 5.79010220e-01 2.89473590e-01 9.09074576e-01\n",
      "  9.27792455e-01 9.15111375e-01 1.25441203e-01 1.68086214e-01\n",
      "  5.24877228e-01 6.45074534e-01 5.74206693e-02 8.80100525e-01\n",
      "  4.66107436e-01 4.75791781e-01 9.24944577e-01 5.30048699e-01\n",
      "  3.95267949e-01 7.50715221e-01 2.51709836e-01 9.60985362e-01\n",
      "  6.68695998e-01 8.39518525e-01 9.94638817e-01 7.65506160e-01\n",
      "  5.10786199e-01 3.75961381e-01 4.98996435e-01 5.77250005e-02\n",
      "  7.28729753e-01 7.74325020e-01 7.94849270e-01 4.15574526e-01\n",
      "  4.42382781e-01 2.55209710e-01 6.40359666e-01 3.30226507e-01\n",
      "  1.65400912e-01 3.51083700e-01 6.43731149e-01 4.72088206e-01\n",
      "  2.44008268e-01 4.43197906e-01 5.10141814e-01 4.09459815e-01\n",
      "  2.07555504e-01 8.90963641e-01 9.23902033e-01 6.80912580e-01\n",
      "  1.56217801e-01 3.89118372e-01 7.27518371e-01 6.75594804e-01\n",
      "  8.96697053e-01 2.88807509e-01 5.00046185e-01 7.30116159e-01\n",
      "  1.90252233e-01 5.63504949e-01 2.45776279e-01 3.42286443e-01\n",
      "  4.26172966e-01 8.07075984e-01 4.93453742e-01 8.56605733e-02\n",
      "  7.99067392e-01 3.11803959e-01 7.64809747e-01 7.40971957e-01\n",
      "  4.86187361e-01 1.69099045e-01 4.74842672e-01 3.51400395e-01\n",
      "  9.34617554e-01 9.79413665e-01 3.23964217e-01 4.71713281e-01\n",
      "  6.93328573e-02 9.35389900e-01 5.14903710e-01 1.31698235e-01\n",
      "  2.70937107e-01 1.35225706e-01 9.95833827e-01 1.72701510e-01\n",
      "  4.46678082e-01 4.99665348e-01 8.38709443e-01 2.13993245e-01\n",
      "  9.03474843e-01 9.88382594e-01 8.00235368e-01 5.48056958e-01\n",
      "  5.24021181e-01 5.01590711e-01 1.10478158e-01 8.55219780e-01\n",
      "  8.68572105e-01 9.84729840e-01 7.90512049e-01 5.71466240e-01\n",
      "  2.88333332e-01 9.33097556e-01 8.21231848e-01 6.92285164e-01\n",
      "  2.08970032e-01 5.16268504e-01 7.86216701e-02 2.31579949e-01\n",
      "  9.92600143e-01 6.36644456e-01 2.47825284e-01 8.49984589e-01\n",
      "  3.60309133e-01 1.71883497e-01 1.25637941e-01 4.33518911e-01\n",
      "  3.68699440e-01 9.46482379e-01 7.36787056e-01 3.60599833e-01\n",
      "  8.07355060e-01 3.95644369e-01 4.06499520e-01 7.38386072e-01\n",
      "  8.54090367e-01 6.67342374e-01 7.23895545e-01 1.74527532e-01\n",
      "  9.23430926e-02 8.46750680e-01 3.91482688e-01 8.22158945e-01\n",
      "  2.63936129e-01 4.67596402e-01 2.50132459e-01 5.23063500e-01\n",
      "  2.45273155e-01 5.58065606e-01 1.28887671e-01 2.39317119e-02\n",
      "  3.29893575e-01 8.42646968e-01 4.12458394e-01 7.62077819e-01\n",
      "  1.83390895e-01 3.04325623e-01 5.46353922e-01 7.63153920e-01\n",
      "  8.09629150e-01 2.56455307e-01 9.98520475e-01 7.98278999e-02\n",
      "  7.79705922e-01 8.79002512e-01 6.58050842e-01 9.70736529e-01\n",
      "  1.45857539e-01 9.45151236e-01 5.79495172e-02 4.42628371e-01\n",
      "  7.67004380e-01 7.57355760e-02 1.64857480e-01 6.43932526e-01\n",
      "  5.57351958e-01 8.70984165e-01 2.54179681e-01 3.17541436e-02\n",
      "  3.30294582e-01 4.70722944e-02 8.03729960e-01 2.99275970e-01\n",
      "  7.07827152e-01 3.52633761e-01 3.39157427e-01 2.17994050e-01\n",
      "  3.35994251e-01 1.39084051e-02 7.24113858e-01 8.14951933e-01\n",
      "  2.41682429e-01 1.75031801e-01 2.96024766e-02 9.32521249e-01\n",
      "  4.65671170e-01 9.79168222e-01 8.04200076e-01 1.61066519e-01\n",
      "  3.98145364e-01 1.59931700e-01 4.77527086e-01 2.24054942e-02\n",
      "  7.50471127e-01 8.18728865e-01 8.06215333e-01 5.60813137e-01\n",
      "  9.13318916e-01 8.93208738e-01 2.04384311e-01 3.99972466e-01\n",
      "  6.82210355e-01 4.78420037e-01 2.04220973e-01 8.36653152e-01\n",
      "  7.14370671e-01 2.42747216e-01 6.69394521e-01 4.88099948e-01\n",
      "  9.52270393e-01 2.82858035e-01 9.38246160e-01 7.98681696e-01\n",
      "  8.46971497e-01 8.50870133e-01 2.86589734e-03 2.92004223e-01\n",
      "  7.72446998e-01 7.34958333e-01 4.99525571e-01 9.60723799e-01\n",
      "  2.69296025e-01 7.67409084e-02 8.13092407e-01 9.14849265e-01\n",
      "  8.52979454e-01 1.87461192e-01 9.12957125e-01 8.93468978e-01\n",
      "  1.53603192e-01 8.52536587e-01 4.12790022e-01 6.47760379e-01\n",
      "  4.37668174e-01 6.53465019e-01 8.43430938e-01 2.65406715e-01\n",
      "  2.97432712e-01 7.01275301e-01 6.26218009e-01 1.01226287e-01\n",
      "  2.28361862e-01 5.24531152e-01 6.52706140e-01 7.56426644e-01\n",
      "  6.24629883e-01 4.88481345e-01 7.96427960e-01 9.54920094e-01\n",
      "  3.26720768e-01 2.17756314e-01 8.58660756e-01 2.18624446e-01\n",
      "  9.66929455e-01 8.53116419e-01 3.38969223e-01 7.39864575e-01\n",
      "  9.03360023e-01 7.61714493e-02 9.34549342e-01 5.81297677e-02\n",
      "  5.65203321e-01 5.48719920e-01 2.58711256e-01 4.96652384e-01\n",
      "  9.55193584e-01 6.37774944e-01 4.51726581e-01 8.25721152e-01\n",
      "  6.62390685e-01 7.54711787e-01 8.38829134e-01 4.09212676e-01\n",
      "  6.80229417e-01 2.08245255e-01 7.07891954e-02 2.82837788e-01\n",
      "  3.36612174e-01 5.06702783e-01 2.40689527e-01 9.35618881e-01\n",
      "  3.39867057e-02 4.14595868e-01 6.14995098e-01 5.42617144e-01\n",
      "  7.55462746e-01 2.23761058e-01 4.93279793e-01 9.54499675e-01\n",
      "  8.64472862e-02 6.86297216e-01 7.55475799e-01 1.94838477e-01\n",
      "  2.51696865e-01 2.73438032e-01 4.60030612e-01 3.83429146e-01\n",
      "  5.09568352e-01 6.18544367e-01 8.51100098e-01 2.35310550e-01\n",
      "  4.01874715e-01 6.53870175e-01 6.34529337e-01]]\n",
      "Change in theta: 1.6265210758175527e-06\n",
      "---------------------------------------------\n",
      "Iteration number 16 finished, Running time 293.77764201164246, Roo2 iteration 16 = -3232940.9331944208\n",
      "Iteration 1:\n",
      "Theta: [[6.82571708e-01 2.04519946e-01 3.44625529e-01 5.75115394e-01\n",
      "  1.67797124e-01 3.75646691e-01 1.16354073e-01 8.83586111e-01\n",
      "  5.22040671e-01 3.47339472e-01 8.63884346e-01 1.62101135e-01\n",
      "  1.26220621e-01 5.95404532e-01 9.41481310e-01 7.64163609e-01\n",
      "  5.77493645e-01 7.73453213e-01 1.63610959e-01 7.90841233e-01\n",
      "  9.35660113e-02 2.68795361e-01 3.84676153e-01 4.97085410e-01\n",
      "  4.94988377e-01 6.36573639e-01 3.65356495e-01 2.17181849e-01\n",
      "  4.22688067e-01 8.48219259e-01 8.36633638e-01 9.86254642e-01\n",
      "  4.69587110e-01 5.17766371e-01 1.62574326e-02 8.68936453e-01\n",
      "  6.89866042e-01 6.26072796e-01 4.87927546e-01 5.22121731e-01\n",
      "  4.13602887e-01 4.41754018e-01 7.98435070e-01 4.11088142e-01\n",
      "  7.66483365e-01 2.83448269e-01 6.01821451e-01 7.80420192e-02\n",
      "  9.99851391e-01 2.20899132e-01 7.66517542e-01 6.18846157e-01\n",
      "  6.52119012e-01 9.73277522e-01 3.28903674e-01 2.16766154e-02\n",
      "  3.20344018e-01 1.12519872e-01 5.44471166e-02 9.88053175e-01\n",
      "  6.05785631e-02 8.64989030e-01 4.53301993e-02 4.81475189e-01\n",
      "  7.91207162e-01 3.88682044e-01 4.32177927e-01 7.85694333e-02\n",
      "  1.92390376e-01 9.14163771e-01 7.07724267e-01 5.39067302e-02\n",
      "  8.76091955e-01 5.85048535e-01 5.29697078e-01 9.12784154e-01\n",
      "  7.89521499e-01 6.30187111e-01 9.55835159e-01 4.74087533e-01\n",
      "  4.29749315e-01 9.38226169e-01 6.08037596e-01 7.48820395e-01\n",
      "  5.09961493e-01 8.82826961e-01 9.07733431e-01 5.37807759e-01\n",
      "  5.87245868e-01 1.60270543e-01 8.63049530e-01 9.99074813e-02\n",
      "  9.15922485e-01 1.18102203e-01 5.79015450e-01 3.95715794e-01\n",
      "  2.48618372e-01 9.28309430e-01 2.19441934e-01 6.00816931e-02\n",
      "  9.26520473e-01 7.02469027e-01 4.80493954e-01 4.96207434e-01\n",
      "  6.64126392e-03 1.45515337e-01 4.15116306e-01 3.65133098e-02\n",
      "  7.29438253e-01 5.36879970e-01 3.39500935e-01 6.57797836e-01\n",
      "  1.25026879e-01 4.34488399e-01 8.53896161e-02 9.71363097e-01\n",
      "  2.27624154e-01 7.88510045e-01 8.18929102e-01 6.22105508e-01\n",
      "  9.20609728e-01 3.11786509e-01 1.56009224e-01 9.24310738e-01\n",
      "  5.82785256e-01 9.49795293e-01 3.92967847e-02 7.11918455e-01\n",
      "  5.12346970e-01 7.59406946e-01 8.43596896e-01 1.22671876e-01\n",
      "  4.76996043e-01 4.99924807e-01 3.46118534e-01 6.76127306e-01\n",
      "  1.99224177e-01 3.46423251e-01 7.67287500e-01 6.21656860e-02\n",
      "  4.65665162e-01 1.35887340e-01 2.75101838e-02 8.14445437e-01\n",
      "  5.12285109e-01 2.89722657e-01 2.26163640e-01 8.59430709e-01\n",
      "  8.31770805e-01 7.21030381e-01 2.36535946e-01 3.34411770e-01\n",
      "  5.18866283e-03 2.74524490e-01 9.88357439e-02 8.88443320e-01\n",
      "  6.15306611e-01 3.40354880e-01 8.85030362e-01 4.82983362e-01\n",
      "  6.52139667e-01 5.35469918e-01 8.15433466e-01 4.69731218e-01\n",
      "  9.84671755e-01 4.41109805e-02 8.85724961e-01 6.94568644e-01\n",
      "  8.33648502e-01 4.62203798e-01 6.11425038e-01 2.26860619e-01\n",
      "  7.93882111e-01 2.29914623e-01 4.08259666e-01 4.88007171e-01\n",
      "  3.36781075e-01 3.71371848e-01 6.59103262e-03 3.56350997e-01\n",
      "  7.18393571e-01 1.38382999e-01 9.48265932e-01 1.24277176e-01\n",
      "  8.18096398e-01 7.58730065e-01 5.16488322e-01 3.77492446e-01\n",
      "  1.32114044e-01 8.55640392e-01 9.48059189e-01 2.25138075e-01\n",
      "  5.16197922e-01 7.49108172e-01 6.40108200e-01 8.66495283e-01\n",
      "  1.47635691e-01 7.42605837e-01 6.45740666e-01 7.58022273e-01\n",
      "  8.00795605e-01 2.21039328e-01 6.04587691e-03 6.27648903e-01\n",
      "  2.51515772e-01 7.70839689e-01 1.97894746e-01 9.04971309e-01\n",
      "  8.38359041e-01 4.17778553e-01 7.56715362e-01 5.77380163e-01\n",
      "  9.48719457e-01 6.54357268e-01 5.91206096e-01 9.88762390e-01\n",
      "  5.36359826e-01 9.00712689e-01 1.28365926e-01 3.92139471e-01\n",
      "  9.80299310e-01 1.78503711e-01 5.28769896e-01 2.79123046e-01\n",
      "  8.14575592e-04 4.24808573e-01 3.87220535e-01 8.86486086e-01\n",
      "  3.16547883e-01 2.40260885e-01 1.78150284e-01 5.45048788e-01\n",
      "  9.52048496e-02 1.16800238e-01 4.73098997e-01 2.60900312e-02\n",
      "  9.36591810e-01 4.60457089e-01 9.32281823e-01 8.83188529e-01\n",
      "  6.57926394e-02 3.07763139e-01 1.89245676e-01 8.87246512e-01\n",
      "  6.49862279e-01 2.15840669e-01 1.20431725e-01 8.95817145e-01\n",
      "  9.55023908e-01 5.13468300e-01 2.84492087e-01 2.51494415e-01\n",
      "  6.40164460e-01 7.51368970e-01 3.00912364e-01 5.88147594e-01\n",
      "  4.87419550e-01 8.58461694e-01 1.42282762e-01 7.98694976e-01\n",
      "  2.73850630e-01 3.83838729e-01 8.75995819e-01 5.68029122e-01\n",
      "  9.93543159e-01 2.36374374e-01 1.41586553e-01 1.24072719e-01\n",
      "  5.27911206e-01 3.77268319e-01 1.40552805e-02 5.92226762e-01\n",
      "  2.26231208e-01 2.20735575e-01 9.22161824e-01 3.01006350e-01\n",
      "  8.47037718e-01 4.54763103e-01 7.00673714e-01 4.32776373e-01\n",
      "  2.82782199e-01 1.21273579e-02 9.12967453e-01 9.16871581e-02\n",
      "  4.80185711e-01 7.40666140e-01 1.07386051e-01 5.77281779e-01\n",
      "  2.73550386e-01 8.93722351e-01 8.18051622e-01 1.64503905e-01\n",
      "  3.99037276e-01 2.98319469e-01 1.90856461e-02 3.37455086e-01\n",
      "  8.73762318e-01 5.34794706e-01 1.41323173e-02 1.63435566e-01\n",
      "  7.28870293e-01 6.17557064e-01 5.02946218e-01 4.73829331e-01\n",
      "  9.99533364e-01 4.57496688e-02 1.04992354e-03 8.25580523e-01\n",
      "  3.13221324e-01 2.34265128e-02 5.74197560e-01 3.39240402e-01\n",
      "  2.72268029e-01 6.21527375e-01 7.02700712e-01 1.13284866e-01\n",
      "  4.60248267e-01 1.75226597e-01 8.24622109e-01 7.05568732e-02\n",
      "  6.04571226e-01 8.05310411e-01 9.78892938e-01 5.74005263e-01\n",
      "  5.04729169e-01 2.05053796e-01 2.04864313e-01 9.94129433e-01\n",
      "  4.67030490e-02 9.70116980e-01 9.73001883e-01 1.93457674e-01\n",
      "  9.06028831e-01 1.03415109e-01 2.73917464e-01 1.36029437e-01\n",
      "  2.00721693e-01 5.62374324e-01 6.69032307e-01 8.94447058e-01\n",
      "  1.83288115e-01 6.23317195e-01 3.17594243e-01 6.02591125e-01\n",
      "  1.42338113e-01 4.40902847e-01 1.35385564e-01 4.46418002e-01\n",
      "  8.82991811e-01 7.66576712e-01 8.33554369e-01 6.51917407e-01\n",
      "  4.96793464e-01 1.96711982e-01 4.71317334e-01 8.09276174e-01\n",
      "  9.68954100e-01 1.04466547e-01 9.82736897e-02 5.16535473e-02\n",
      "  5.56995699e-01 3.29730568e-01 4.34731498e-01 7.55829974e-01\n",
      "  8.99336356e-01 7.09524747e-01 2.51636944e-01 7.89583275e-01\n",
      "  1.51671990e-01 2.22347110e-01 2.34429231e-01 5.26800514e-01\n",
      "  7.36717031e-01 4.32952377e-01 2.72578170e-02 7.41957054e-01\n",
      "  5.81488643e-01 6.78598401e-01 3.52225218e-01 3.95197735e-01\n",
      "  2.58020441e-01 9.22851794e-01 5.25842139e-01 8.64684986e-01\n",
      "  8.70269875e-02 6.75850148e-01 1.13516593e-01 8.76539965e-01\n",
      "  8.34088807e-01 2.01393411e-01 1.08223264e-01 4.38357736e-01\n",
      "  2.80596059e-01 4.26076680e-01 7.02972474e-01 6.32360446e-01\n",
      "  3.66938879e-01 3.11664448e-01 3.82073874e-01 7.29475547e-01\n",
      "  9.69574826e-03 6.19848708e-02 3.53987805e-01 2.69962663e-01\n",
      "  9.66147265e-01 7.30326168e-01 9.43744863e-01 7.51395498e-01\n",
      "  5.49229004e-01 5.10250468e-01 6.98474377e-01 5.46676693e-01\n",
      "  4.81239990e-01 5.08641406e-01 1.50953089e-01 3.67521168e-01\n",
      "  8.24296237e-01 9.09530944e-01 9.44613059e-02 8.25105491e-01\n",
      "  2.40393999e-01 9.05176076e-01 3.45901437e-01 2.32405179e-01\n",
      "  2.50714685e-01 4.80679187e-02 9.59474033e-01 1.43131468e-01\n",
      "  9.60568461e-01 6.71629249e-01 3.57140610e-01 9.39044499e-01\n",
      "  3.73589387e-01 2.37523656e-01 1.69443628e-01 7.89371469e-01\n",
      "  1.27681616e-01 6.36391838e-01 2.12394775e-01 7.46964379e-01\n",
      "  6.92351448e-01 9.55247074e-01 1.11655911e-01 5.90110862e-01\n",
      "  7.51890421e-02 6.89095134e-01 1.23328211e-01 8.00803493e-01\n",
      "  4.69704021e-01 3.68750833e-01 5.13023850e-01 9.94908392e-01\n",
      "  9.26166423e-01 5.79562015e-01 5.40854137e-01 9.24873782e-01\n",
      "  4.96932605e-01 3.09736732e-01 9.76922644e-01 8.82778454e-01\n",
      "  8.42208942e-01 5.27650032e-02 5.77238843e-01 9.42871756e-01\n",
      "  9.52960385e-01 8.25943598e-02 9.22204974e-01 9.69178914e-02\n",
      "  6.48953315e-01 9.51236607e-01 1.90829852e-01 5.94353183e-01\n",
      "  8.19263941e-01 8.25584941e-01 9.12817722e-01 7.80934651e-01\n",
      "  1.40321195e-01 3.04263964e-01 1.01065668e-01 2.08813067e-01\n",
      "  8.76158137e-01 5.92041946e-01 6.43782075e-02 6.72066189e-02\n",
      "  8.63089668e-02 2.25002022e-01 6.52606064e-01 6.14431389e-01\n",
      "  8.17665076e-01 4.13470628e-01 5.53459866e-01 9.86157033e-01\n",
      "  9.98202946e-01 7.21732775e-01 6.89878716e-01 1.79585301e-01\n",
      "  3.67412384e-01 5.29399742e-01 9.16700400e-01 2.23356497e-01\n",
      "  1.03075188e-01 7.59482870e-01 5.64294904e-01 4.23056342e-01\n",
      "  6.15088920e-01 2.35911264e-01 8.00333083e-01 9.25103719e-01\n",
      "  8.50861964e-01 5.93767087e-01 1.46434823e-02 9.47512441e-01\n",
      "  1.23539088e-01 6.63003948e-01 5.01389386e-01 1.08920266e-01\n",
      "  1.02238726e-01 7.68049223e-01 4.78182752e-01 7.49501799e-02\n",
      "  6.96286598e-01 8.02026620e-02 4.29916010e-01 5.64904014e-01\n",
      "  3.91937864e-01 5.87258235e-01 4.26962475e-01 4.31607566e-01\n",
      "  6.36806988e-02 3.29834064e-01 1.59520926e-01 7.22777705e-01\n",
      "  1.45316991e-01 3.38387353e-01 8.64590687e-01 7.93605673e-02\n",
      "  8.82645486e-01 1.26557978e-01 4.54618800e-01 7.47071266e-01\n",
      "  8.38646244e-01 7.88238325e-01 4.87979030e-01 1.02031573e-01\n",
      "  7.53822227e-01 8.80501105e-01 5.26665417e-01 1.70829673e-01\n",
      "  4.59576649e-01 9.68488721e-01 3.58479003e-01 5.99292492e-01\n",
      "  9.17978462e-01 7.24714687e-01 1.48537833e-01 5.61825700e-01\n",
      "  8.79340908e-01 3.57896978e-01 9.23139653e-01 3.06274080e-01\n",
      "  1.99892977e-01 5.77199404e-01 5.64174350e-01 6.50673743e-01\n",
      "  8.99021451e-01 9.87214197e-01 1.40560477e-01 4.20666701e-01\n",
      "  8.65568102e-01 4.93325089e-01 8.36021398e-02 7.62232274e-01\n",
      "  8.09692515e-01 7.33149126e-01 8.53774925e-01 7.27788071e-01\n",
      "  7.21593865e-01 5.75473209e-01 1.85632141e-01 7.60938671e-01\n",
      "  8.68985929e-01 9.52524140e-01 1.98957534e-01 7.96135978e-01\n",
      "  7.70817554e-02 9.90879590e-01 1.97833270e-01 4.74425075e-02\n",
      "  4.47467872e-01 7.70383469e-01 3.76057873e-01 8.45129722e-01\n",
      "  3.03897305e-01 2.27215744e-01 7.00370689e-02 3.80352863e-01\n",
      "  7.33125230e-02 7.45897084e-01 9.68574136e-01 1.31819182e-01\n",
      "  1.20837046e-03 3.14201457e-01 4.05172748e-01 8.80296874e-01\n",
      "  6.47536999e-01 1.54623621e-01 1.79615056e-01 4.07196509e-01\n",
      "  6.54977131e-01 5.79010803e-01 2.89474173e-01 9.09075160e-01\n",
      "  9.27793038e-01 9.15111958e-01 1.25441786e-01 1.68086797e-01\n",
      "  5.24877811e-01 6.45075118e-01 5.74212528e-02 8.80101109e-01\n",
      "  4.66108020e-01 4.75792364e-01 9.24945161e-01 5.30049282e-01\n",
      "  3.95268533e-01 7.50715805e-01 2.51710419e-01 9.60985946e-01\n",
      "  6.68696581e-01 8.39519109e-01 9.94639401e-01 7.65506744e-01\n",
      "  5.10786782e-01 3.75961964e-01 4.98997019e-01 5.77255840e-02\n",
      "  7.28730336e-01 7.74325603e-01 7.94849854e-01 4.15575109e-01\n",
      "  4.42383364e-01 2.55210294e-01 6.40360249e-01 3.30227090e-01\n",
      "  1.65401495e-01 3.51084284e-01 6.43731733e-01 4.72088790e-01\n",
      "  2.44008852e-01 4.43198490e-01 5.10142398e-01 4.09460399e-01\n",
      "  2.07556087e-01 8.90964225e-01 9.23902617e-01 6.80913164e-01\n",
      "  1.56218384e-01 3.89118955e-01 7.27518955e-01 6.75595388e-01\n",
      "  8.96697637e-01 2.88808092e-01 5.00046768e-01 7.30116743e-01\n",
      "  1.90252817e-01 5.63505532e-01 2.45776863e-01 3.42287027e-01\n",
      "  4.26173550e-01 8.07076567e-01 4.93454326e-01 8.56611568e-02\n",
      "  7.99067976e-01 3.11804542e-01 7.64810331e-01 7.40972540e-01\n",
      "  4.86187945e-01 1.69099629e-01 4.74843256e-01 3.51400979e-01\n",
      "  9.34618137e-01 9.79414249e-01 3.23964800e-01 4.71713865e-01\n",
      "  6.93334407e-02 9.35390483e-01 5.14904293e-01 1.31698819e-01\n",
      "  2.70937691e-01 1.35226289e-01 9.95834410e-01 1.72702093e-01\n",
      "  4.46678666e-01 4.99665932e-01 8.38710027e-01 2.13993828e-01\n",
      "  9.03475426e-01 9.88383178e-01 8.00235951e-01 5.48057541e-01\n",
      "  5.24021764e-01 5.01591295e-01 1.10478742e-01 8.55220363e-01\n",
      "  8.68572688e-01 9.84730423e-01 7.90512633e-01 5.71466824e-01\n",
      "  2.88333916e-01 9.33098139e-01 8.21232432e-01 6.92285747e-01\n",
      "  2.08970616e-01 5.16269088e-01 7.86222535e-02 2.31580533e-01\n",
      "  9.92600726e-01 6.36645039e-01 2.47825867e-01 8.49985172e-01\n",
      "  3.60309717e-01 1.71884080e-01 1.25638524e-01 4.33519494e-01\n",
      "  3.68700023e-01 9.46482963e-01 7.36787640e-01 3.60600416e-01\n",
      "  8.07355643e-01 3.95644953e-01 4.06500104e-01 7.38386656e-01\n",
      "  8.54090951e-01 6.67342958e-01 7.23896129e-01 1.74528115e-01\n",
      "  9.23436760e-02 8.46751263e-01 3.91483271e-01 8.22159528e-01\n",
      "  2.63936712e-01 4.67596986e-01 2.50133042e-01 5.23064083e-01\n",
      "  2.45273739e-01 5.58066190e-01 1.28888254e-01 2.39322953e-02\n",
      "  3.29894158e-01 8.42647551e-01 4.12458978e-01 7.62078402e-01\n",
      "  1.83391479e-01 3.04326206e-01 5.46354506e-01 7.63154503e-01\n",
      "  8.09629734e-01 2.56455891e-01 9.98521059e-01 7.98284833e-02\n",
      "  7.79706505e-01 8.79003096e-01 6.58051425e-01 9.70737112e-01\n",
      "  1.45858123e-01 9.45151819e-01 5.79501006e-02 4.42628954e-01\n",
      "  7.67004964e-01 7.57361594e-02 1.64858063e-01 6.43933110e-01\n",
      "  5.57352541e-01 8.70984748e-01 2.54180265e-01 3.17547271e-02\n",
      "  3.30295165e-01 4.70728778e-02 8.03730543e-01 2.99276554e-01\n",
      "  7.07827735e-01 3.52634345e-01 3.39158010e-01 2.17994633e-01\n",
      "  3.35994834e-01 1.39089885e-02 7.24114441e-01 8.14952516e-01\n",
      "  2.41683012e-01 1.75032384e-01 2.96030600e-02 9.32521832e-01\n",
      "  4.65671753e-01 9.79168805e-01 8.04200659e-01 1.61067102e-01\n",
      "  3.98145947e-01 1.59932284e-01 4.77527669e-01 2.24060776e-02\n",
      "  7.50471710e-01 8.18729448e-01 8.06215916e-01 5.60813720e-01\n",
      "  9.13319499e-01 8.93209321e-01 2.04384895e-01 3.99973050e-01\n",
      "  6.82210939e-01 4.78420620e-01 2.04221556e-01 8.36653735e-01\n",
      "  7.14371254e-01 2.42747799e-01 6.69395105e-01 4.88100532e-01\n",
      "  9.52270977e-01 2.82858618e-01 9.38246743e-01 7.98682280e-01\n",
      "  8.46972080e-01 8.50870716e-01 2.86648075e-03 2.92004806e-01\n",
      "  7.72447582e-01 7.34958916e-01 4.99526154e-01 9.60724382e-01\n",
      "  2.69296608e-01 7.67414918e-02 8.13092990e-01 9.14849848e-01\n",
      "  8.52980037e-01 1.87461776e-01 9.12957709e-01 8.93469561e-01\n",
      "  1.53603776e-01 8.52537171e-01 4.12790605e-01 6.47760962e-01\n",
      "  4.37668757e-01 6.53465602e-01 8.43431521e-01 2.65407299e-01\n",
      "  2.97433296e-01 7.01275885e-01 6.26218592e-01 1.01226871e-01\n",
      "  2.28362445e-01 5.24531736e-01 6.52706723e-01 7.56427227e-01\n",
      "  6.24630467e-01 4.88481929e-01 7.96428543e-01 9.54920678e-01\n",
      "  3.26721351e-01 2.17756897e-01 8.58661339e-01 2.18625029e-01\n",
      "  9.66930039e-01 8.53117002e-01 3.38969806e-01 7.39865159e-01\n",
      "  9.03360607e-01 7.61720327e-02 9.34549925e-01 5.81303511e-02\n",
      "  5.65203905e-01 5.48720503e-01 2.58711839e-01 4.96652967e-01\n",
      "  9.55194167e-01 6.37775527e-01 4.51727165e-01 8.25721736e-01\n",
      "  6.62391268e-01 7.54712370e-01 8.38829718e-01 4.09213259e-01\n",
      "  6.80230001e-01 2.08245839e-01 7.07897789e-02 2.82838372e-01\n",
      "  3.36612757e-01 5.06703366e-01 2.40690111e-01 9.35619465e-01\n",
      "  3.39872891e-02 4.14596451e-01 6.14995682e-01 5.42617728e-01\n",
      "  7.55463329e-01 2.23761641e-01 4.93280376e-01 9.54500258e-01\n",
      "  8.64478696e-02 6.86297799e-01 7.55476382e-01 1.94839061e-01\n",
      "  2.51697449e-01 2.73438616e-01 4.60031196e-01 3.83429730e-01\n",
      "  5.09568936e-01 6.18544950e-01 8.51100681e-01 2.35311134e-01\n",
      "  4.01875298e-01 6.53870758e-01 6.34529921e-01]]\n",
      "Change in theta: 1.6717462913748402e-06\n",
      "---------------------------------------------\n",
      "Iteration number 17 finished, Running time 298.0147593021393, Roo2 iteration 17 = -13033456.612919923\n",
      "Iteration 1:\n",
      "Theta: [[6.82572192e-01 2.04520429e-01 3.44626012e-01 5.75115877e-01\n",
      "  1.67797608e-01 3.75647174e-01 1.16354556e-01 8.83586594e-01\n",
      "  5.22041154e-01 3.47339955e-01 8.63884830e-01 1.62101619e-01\n",
      "  1.26221105e-01 5.95405016e-01 9.41481794e-01 7.64164092e-01\n",
      "  5.77494129e-01 7.73453696e-01 1.63611442e-01 7.90841717e-01\n",
      "  9.35664947e-02 2.68795844e-01 3.84676636e-01 4.97085893e-01\n",
      "  4.94988861e-01 6.36574123e-01 3.65356978e-01 2.17182332e-01\n",
      "  4.22688551e-01 8.48219742e-01 8.36634121e-01 9.86255126e-01\n",
      "  4.69587593e-01 5.17766855e-01 1.62579160e-02 8.68936936e-01\n",
      "  6.89866525e-01 6.26073280e-01 4.87928030e-01 5.22122214e-01\n",
      "  4.13603370e-01 4.41754502e-01 7.98435553e-01 4.11088625e-01\n",
      "  7.66483848e-01 2.83448753e-01 6.01821934e-01 7.80425025e-02\n",
      "  9.99851875e-01 2.20899616e-01 7.66518025e-01 6.18846641e-01\n",
      "  6.52119495e-01 9.73278005e-01 3.28904157e-01 2.16770987e-02\n",
      "  3.20344502e-01 1.12520355e-01 5.44475999e-02 9.88053658e-01\n",
      "  6.05790465e-02 8.64989513e-01 4.53306826e-02 4.81475672e-01\n",
      "  7.91207645e-01 3.88682527e-01 4.32178411e-01 7.85699166e-02\n",
      "  1.92390859e-01 9.14164254e-01 7.07724750e-01 5.39072136e-02\n",
      "  8.76092438e-01 5.85049018e-01 5.29697562e-01 9.12784638e-01\n",
      "  7.89521983e-01 6.30187595e-01 9.55835642e-01 4.74088017e-01\n",
      "  4.29749798e-01 9.38226653e-01 6.08038080e-01 7.48820879e-01\n",
      "  5.09961977e-01 8.82827444e-01 9.07733914e-01 5.37808243e-01\n",
      "  5.87246352e-01 1.60271026e-01 8.63050014e-01 9.99079646e-02\n",
      "  9.15922968e-01 1.18102686e-01 5.79015934e-01 3.95716277e-01\n",
      "  2.48618855e-01 9.28309913e-01 2.19442417e-01 6.00821764e-02\n",
      "  9.26520956e-01 7.02469510e-01 4.80494438e-01 4.96207917e-01\n",
      "  6.64174724e-03 1.45515820e-01 4.15116789e-01 3.65137932e-02\n",
      "  7.29438736e-01 5.36880453e-01 3.39501418e-01 6.57798319e-01\n",
      "  1.25027362e-01 4.34488882e-01 8.53900994e-02 9.71363580e-01\n",
      "  2.27624637e-01 7.88510528e-01 8.18929586e-01 6.22105991e-01\n",
      "  9.20610212e-01 3.11786992e-01 1.56009707e-01 9.24311221e-01\n",
      "  5.82785739e-01 9.49795776e-01 3.92972681e-02 7.11918939e-01\n",
      "  5.12347453e-01 7.59407429e-01 8.43597380e-01 1.22672359e-01\n",
      "  4.76996526e-01 4.99925290e-01 3.46119017e-01 6.76127789e-01\n",
      "  1.99224661e-01 3.46423734e-01 7.67287983e-01 6.21661694e-02\n",
      "  4.65665645e-01 1.35887823e-01 2.75106671e-02 8.14445921e-01\n",
      "  5.12285593e-01 2.89723140e-01 2.26164123e-01 8.59431192e-01\n",
      "  8.31771289e-01 7.21030864e-01 2.36536429e-01 3.34412254e-01\n",
      "  5.18914615e-03 2.74524973e-01 9.88362272e-02 8.88443803e-01\n",
      "  6.15307094e-01 3.40355363e-01 8.85030846e-01 4.82983845e-01\n",
      "  6.52140150e-01 5.35470402e-01 8.15433950e-01 4.69731701e-01\n",
      "  9.84672238e-01 4.41114638e-02 8.85725445e-01 6.94569128e-01\n",
      "  8.33648985e-01 4.62204281e-01 6.11425522e-01 2.26861102e-01\n",
      "  7.93882595e-01 2.29915106e-01 4.08260150e-01 4.88007654e-01\n",
      "  3.36781559e-01 3.71372332e-01 6.59151595e-03 3.56351480e-01\n",
      "  7.18394054e-01 1.38383483e-01 9.48266416e-01 1.24277659e-01\n",
      "  8.18096881e-01 7.58730548e-01 5.16488806e-01 3.77492929e-01\n",
      "  1.32114528e-01 8.55640875e-01 9.48059672e-01 2.25138559e-01\n",
      "  5.16198405e-01 7.49108656e-01 6.40108684e-01 8.66495766e-01\n",
      "  1.47636174e-01 7.42606320e-01 6.45741149e-01 7.58022757e-01\n",
      "  8.00796089e-01 2.21039811e-01 6.04636023e-03 6.27649387e-01\n",
      "  2.51516255e-01 7.70840173e-01 1.97895229e-01 9.04971793e-01\n",
      "  8.38359524e-01 4.17779037e-01 7.56715845e-01 5.77380646e-01\n",
      "  9.48719940e-01 6.54357751e-01 5.91206580e-01 9.88762874e-01\n",
      "  5.36360310e-01 9.00713172e-01 1.28366410e-01 3.92139954e-01\n",
      "  9.80299794e-01 1.78504194e-01 5.28770380e-01 2.79123529e-01\n",
      "  8.15058917e-04 4.24809056e-01 3.87221019e-01 8.86486570e-01\n",
      "  3.16548366e-01 2.40261368e-01 1.78150767e-01 5.45049272e-01\n",
      "  9.52053329e-02 1.16800721e-01 4.73099481e-01 2.60905146e-02\n",
      "  9.36592293e-01 4.60457572e-01 9.32282306e-01 8.83189012e-01\n",
      "  6.57931227e-02 3.07763622e-01 1.89246159e-01 8.87246996e-01\n",
      "  6.49862763e-01 2.15841152e-01 1.20432208e-01 8.95817628e-01\n",
      "  9.55024391e-01 5.13468783e-01 2.84492570e-01 2.51494899e-01\n",
      "  6.40164944e-01 7.51369453e-01 3.00912847e-01 5.88148077e-01\n",
      "  4.87420033e-01 8.58462177e-01 1.42283246e-01 7.98695459e-01\n",
      "  2.73851113e-01 3.83839213e-01 8.75996302e-01 5.68029605e-01\n",
      "  9.93543642e-01 2.36374857e-01 1.41587036e-01 1.24073202e-01\n",
      "  5.27911689e-01 3.77268803e-01 1.40557639e-02 5.92227245e-01\n",
      "  2.26231691e-01 2.20736058e-01 9.22162307e-01 3.01006833e-01\n",
      "  8.47038201e-01 4.54763587e-01 7.00674198e-01 4.32776856e-01\n",
      "  2.82782682e-01 1.21278412e-02 9.12967936e-01 9.16876415e-02\n",
      "  4.80186194e-01 7.40666624e-01 1.07386535e-01 5.77282262e-01\n",
      "  2.73550869e-01 8.93722834e-01 8.18052105e-01 1.64504388e-01\n",
      "  3.99037760e-01 2.98319952e-01 1.90861294e-02 3.37455570e-01\n",
      "  8.73762801e-01 5.34795190e-01 1.41328006e-02 1.63436049e-01\n",
      "  7.28870776e-01 6.17557547e-01 5.02946701e-01 4.73829815e-01\n",
      "  9.99533848e-01 4.57501521e-02 1.05040686e-03 8.25581007e-01\n",
      "  3.13221807e-01 2.34269962e-02 5.74198043e-01 3.39240885e-01\n",
      "  2.72268512e-01 6.21527858e-01 7.02701195e-01 1.13285349e-01\n",
      "  4.60248750e-01 1.75227080e-01 8.24622592e-01 7.05573565e-02\n",
      "  6.04571709e-01 8.05310895e-01 9.78893421e-01 5.74005746e-01\n",
      "  5.04729652e-01 2.05054280e-01 2.04864796e-01 9.94129916e-01\n",
      "  4.67035323e-02 9.70117464e-01 9.73002367e-01 1.93458158e-01\n",
      "  9.06029315e-01 1.03415592e-01 2.73917947e-01 1.36029920e-01\n",
      "  2.00722176e-01 5.62374808e-01 6.69032790e-01 8.94447542e-01\n",
      "  1.83288598e-01 6.23317678e-01 3.17594726e-01 6.02591608e-01\n",
      "  1.42338596e-01 4.40903330e-01 1.35386048e-01 4.46418485e-01\n",
      "  8.82992295e-01 7.66577196e-01 8.33554853e-01 6.51917891e-01\n",
      "  4.96793947e-01 1.96712465e-01 4.71317817e-01 8.09276658e-01\n",
      "  9.68954583e-01 1.04467030e-01 9.82741730e-02 5.16540306e-02\n",
      "  5.56996182e-01 3.29731051e-01 4.34731981e-01 7.55830457e-01\n",
      "  8.99336839e-01 7.09525230e-01 2.51637428e-01 7.89583758e-01\n",
      "  1.51672473e-01 2.22347593e-01 2.34429714e-01 5.26800997e-01\n",
      "  7.36717515e-01 4.32952860e-01 2.72583004e-02 7.41957538e-01\n",
      "  5.81489126e-01 6.78598885e-01 3.52225701e-01 3.95198219e-01\n",
      "  2.58020925e-01 9.22852277e-01 5.25842623e-01 8.64685469e-01\n",
      "  8.70274708e-02 6.75850632e-01 1.13517077e-01 8.76540448e-01\n",
      "  8.34089291e-01 2.01393895e-01 1.08223747e-01 4.38358219e-01\n",
      "  2.80596542e-01 4.26077163e-01 7.02972958e-01 6.32360929e-01\n",
      "  3.66939363e-01 3.11664931e-01 3.82074357e-01 7.29476031e-01\n",
      "  9.69623159e-03 6.19853541e-02 3.53988288e-01 2.69963146e-01\n",
      "  9.66147748e-01 7.30326652e-01 9.43745346e-01 7.51395982e-01\n",
      "  5.49229487e-01 5.10250952e-01 6.98474861e-01 5.46677177e-01\n",
      "  4.81240473e-01 5.08641889e-01 1.50953572e-01 3.67521652e-01\n",
      "  8.24296720e-01 9.09531427e-01 9.44617892e-02 8.25105974e-01\n",
      "  2.40394482e-01 9.05176560e-01 3.45901920e-01 2.32405662e-01\n",
      "  2.50715168e-01 4.80684020e-02 9.59474516e-01 1.43131951e-01\n",
      "  9.60568944e-01 6.71629732e-01 3.57141093e-01 9.39044982e-01\n",
      "  3.73589871e-01 2.37524139e-01 1.69444112e-01 7.89371953e-01\n",
      "  1.27682100e-01 6.36392322e-01 2.12395258e-01 7.46964862e-01\n",
      "  6.92351931e-01 9.55247557e-01 1.11656394e-01 5.90111346e-01\n",
      "  7.51895255e-02 6.89095617e-01 1.23328694e-01 8.00803977e-01\n",
      "  4.69704505e-01 3.68751316e-01 5.13024333e-01 9.94908875e-01\n",
      "  9.26166906e-01 5.79562498e-01 5.40854620e-01 9.24874266e-01\n",
      "  4.96933089e-01 3.09737215e-01 9.76923127e-01 8.82778937e-01\n",
      "  8.42209425e-01 5.27654865e-02 5.77239327e-01 9.42872239e-01\n",
      "  9.52960868e-01 8.25948431e-02 9.22205458e-01 9.69183747e-02\n",
      "  6.48953798e-01 9.51237091e-01 1.90830336e-01 5.94353667e-01\n",
      "  8.19264425e-01 8.25585424e-01 9.12818205e-01 7.80935135e-01\n",
      "  1.40321678e-01 3.04264448e-01 1.01066152e-01 2.08813550e-01\n",
      "  8.76158620e-01 5.92042430e-01 6.43786908e-02 6.72071022e-02\n",
      "  8.63094501e-02 2.25002505e-01 6.52606547e-01 6.14431872e-01\n",
      "  8.17665559e-01 4.13471111e-01 5.53460350e-01 9.86157516e-01\n",
      "  9.98203429e-01 7.21733259e-01 6.89879199e-01 1.79585784e-01\n",
      "  3.67412867e-01 5.29400225e-01 9.16700884e-01 2.23356981e-01\n",
      "  1.03075671e-01 7.59483354e-01 5.64295387e-01 4.23056825e-01\n",
      "  6.15089404e-01 2.35911748e-01 8.00333566e-01 9.25104202e-01\n",
      "  8.50862447e-01 5.93767571e-01 1.46439656e-02 9.47512924e-01\n",
      "  1.23539572e-01 6.63004431e-01 5.01389869e-01 1.08920750e-01\n",
      "  1.02239209e-01 7.68049707e-01 4.78183235e-01 7.49506632e-02\n",
      "  6.96287081e-01 8.02031454e-02 4.29916493e-01 5.64904497e-01\n",
      "  3.91938347e-01 5.87258718e-01 4.26962958e-01 4.31608049e-01\n",
      "  6.36811821e-02 3.29834547e-01 1.59521409e-01 7.22778188e-01\n",
      "  1.45317474e-01 3.38387837e-01 8.64591171e-01 7.93610506e-02\n",
      "  8.82645970e-01 1.26558461e-01 4.54619283e-01 7.47071750e-01\n",
      "  8.38646727e-01 7.88238808e-01 4.87979514e-01 1.02032056e-01\n",
      "  7.53822710e-01 8.80501589e-01 5.26665900e-01 1.70830156e-01\n",
      "  4.59577132e-01 9.68489205e-01 3.58479486e-01 5.99292975e-01\n",
      "  9.17978945e-01 7.24715171e-01 1.48538317e-01 5.61826184e-01\n",
      "  8.79341391e-01 3.57897462e-01 9.23140136e-01 3.06274563e-01\n",
      "  1.99893460e-01 5.77199887e-01 5.64174834e-01 6.50674226e-01\n",
      "  8.99021934e-01 9.87214680e-01 1.40560960e-01 4.20667184e-01\n",
      "  8.65568585e-01 4.93325573e-01 8.36026231e-02 7.62232757e-01\n",
      "  8.09692998e-01 7.33149609e-01 8.53775408e-01 7.27788554e-01\n",
      "  7.21594348e-01 5.75473692e-01 1.85632624e-01 7.60939155e-01\n",
      "  8.68986412e-01 9.52524623e-01 1.98958018e-01 7.96136461e-01\n",
      "  7.70822388e-02 9.90880073e-01 1.97833754e-01 4.74429908e-02\n",
      "  4.47468356e-01 7.70383953e-01 3.76058356e-01 8.45130205e-01\n",
      "  3.03897789e-01 2.27216227e-01 7.00375522e-02 3.80353346e-01\n",
      "  7.33130064e-02 7.45897567e-01 9.68574619e-01 1.31819666e-01\n",
      "  1.20885378e-03 3.14201940e-01 4.05173232e-01 8.80297357e-01\n",
      "  6.47537482e-01 1.54624104e-01 1.79615540e-01 4.07196992e-01\n",
      "  6.54977614e-01 5.79011286e-01 2.89474656e-01 9.09075643e-01\n",
      "  9.27793522e-01 9.15112441e-01 1.25442269e-01 1.68087281e-01\n",
      "  5.24878295e-01 6.45075601e-01 5.74217361e-02 8.80101592e-01\n",
      "  4.66108503e-01 4.75792848e-01 9.24945644e-01 5.30049766e-01\n",
      "  3.95269016e-01 7.50716288e-01 2.51710902e-01 9.60986429e-01\n",
      "  6.68697064e-01 8.39519592e-01 9.94639884e-01 7.65507227e-01\n",
      "  5.10787265e-01 3.75962447e-01 4.98997502e-01 5.77260673e-02\n",
      "  7.28730819e-01 7.74326086e-01 7.94850337e-01 4.15575592e-01\n",
      "  4.42383848e-01 2.55210777e-01 6.40360732e-01 3.30227574e-01\n",
      "  1.65401978e-01 3.51084767e-01 6.43732216e-01 4.72089273e-01\n",
      "  2.44009335e-01 4.43198973e-01 5.10142881e-01 4.09460882e-01\n",
      "  2.07556571e-01 8.90964708e-01 9.23903100e-01 6.80913647e-01\n",
      "  1.56218868e-01 3.89119438e-01 7.27519438e-01 6.75595871e-01\n",
      "  8.96698120e-01 2.88808575e-01 5.00047252e-01 7.30117226e-01\n",
      "  1.90253300e-01 5.63506016e-01 2.45777346e-01 3.42287510e-01\n",
      "  4.26174033e-01 8.07077051e-01 4.93454809e-01 8.56616401e-02\n",
      "  7.99068459e-01 3.11805026e-01 7.64810814e-01 7.40973024e-01\n",
      "  4.86188428e-01 1.69100112e-01 4.74843739e-01 3.51401462e-01\n",
      "  9.34618621e-01 9.79414732e-01 3.23965284e-01 4.71714348e-01\n",
      "  6.93339240e-02 9.35390966e-01 5.14904776e-01 1.31699302e-01\n",
      "  2.70938174e-01 1.35226773e-01 9.95834893e-01 1.72702577e-01\n",
      "  4.46679149e-01 4.99666415e-01 8.38710510e-01 2.13994312e-01\n",
      "  9.03475909e-01 9.88383661e-01 8.00236435e-01 5.48058024e-01\n",
      "  5.24022247e-01 5.01591778e-01 1.10479225e-01 8.55220847e-01\n",
      "  8.68573172e-01 9.84730906e-01 7.90513116e-01 5.71467307e-01\n",
      "  2.88334399e-01 9.33098623e-01 8.21232915e-01 6.92286231e-01\n",
      "  2.08971099e-01 5.16269571e-01 7.86227369e-02 2.31581016e-01\n",
      "  9.92601209e-01 6.36645522e-01 2.47826351e-01 8.49985655e-01\n",
      "  3.60310200e-01 1.71884564e-01 1.25639008e-01 4.33519978e-01\n",
      "  3.68700507e-01 9.46483446e-01 7.36788123e-01 3.60600900e-01\n",
      "  8.07356127e-01 3.95645436e-01 4.06500587e-01 7.38387139e-01\n",
      "  8.54091434e-01 6.67343441e-01 7.23896612e-01 1.74528598e-01\n",
      "  9.23441593e-02 8.46751746e-01 3.91483754e-01 8.22160011e-01\n",
      "  2.63937195e-01 4.67597469e-01 2.50133525e-01 5.23064566e-01\n",
      "  2.45274222e-01 5.58066673e-01 1.28888738e-01 2.39327787e-02\n",
      "  3.29894641e-01 8.42648035e-01 4.12459461e-01 7.62078885e-01\n",
      "  1.83391962e-01 3.04326690e-01 5.46354989e-01 7.63154986e-01\n",
      "  8.09630217e-01 2.56456374e-01 9.98521542e-01 7.98289667e-02\n",
      "  7.79706988e-01 8.79003579e-01 6.58051909e-01 9.70737595e-01\n",
      "  1.45858606e-01 9.45152302e-01 5.79505839e-02 4.42629438e-01\n",
      "  7.67005447e-01 7.57366427e-02 1.64858546e-01 6.43933593e-01\n",
      "  5.57353025e-01 8.70985231e-01 2.54180748e-01 3.17552104e-02\n",
      "  3.30295649e-01 4.70733611e-02 8.03731027e-01 2.99277037e-01\n",
      "  7.07828218e-01 3.52634828e-01 3.39158494e-01 2.17995117e-01\n",
      "  3.35995317e-01 1.39094719e-02 7.24114924e-01 8.14952999e-01\n",
      "  2.41683496e-01 1.75032867e-01 2.96035434e-02 9.32522316e-01\n",
      "  4.65672236e-01 9.79169288e-01 8.04201142e-01 1.61067586e-01\n",
      "  3.98146431e-01 1.59932767e-01 4.77528153e-01 2.24065609e-02\n",
      "  7.50472194e-01 8.18729932e-01 8.06216399e-01 5.60814204e-01\n",
      "  9.13319983e-01 8.93209805e-01 2.04385378e-01 3.99973533e-01\n",
      "  6.82211422e-01 4.78421103e-01 2.04222040e-01 8.36654218e-01\n",
      "  7.14371738e-01 2.42748283e-01 6.69395588e-01 4.88101015e-01\n",
      "  9.52271460e-01 2.82859102e-01 9.38247226e-01 7.98682763e-01\n",
      "  8.46972563e-01 8.50871199e-01 2.86696408e-03 2.92005289e-01\n",
      "  7.72448065e-01 7.34959399e-01 4.99526637e-01 9.60724865e-01\n",
      "  2.69297092e-01 7.67419751e-02 8.13093474e-01 9.14850332e-01\n",
      "  8.52980521e-01 1.87462259e-01 9.12958192e-01 8.93470045e-01\n",
      "  1.53604259e-01 8.52537654e-01 4.12791089e-01 6.47761445e-01\n",
      "  4.37669241e-01 6.53466086e-01 8.43432005e-01 2.65407782e-01\n",
      "  2.97433779e-01 7.01276368e-01 6.26219076e-01 1.01227354e-01\n",
      "  2.28362928e-01 5.24532219e-01 6.52707207e-01 7.56427711e-01\n",
      "  6.24630950e-01 4.88482412e-01 7.96429026e-01 9.54921161e-01\n",
      "  3.26721835e-01 2.17757381e-01 8.58661823e-01 2.18625513e-01\n",
      "  9.66930522e-01 8.53117486e-01 3.38970289e-01 7.39865642e-01\n",
      "  9.03361090e-01 7.61725160e-02 9.34550409e-01 5.81308344e-02\n",
      "  5.65204388e-01 5.48720986e-01 2.58712322e-01 4.96653450e-01\n",
      "  9.55194651e-01 6.37776010e-01 4.51727648e-01 8.25722219e-01\n",
      "  6.62391752e-01 7.54712854e-01 8.38830201e-01 4.09213743e-01\n",
      "  6.80230484e-01 2.08246322e-01 7.07902622e-02 2.82838855e-01\n",
      "  3.36613241e-01 5.06703850e-01 2.40690594e-01 9.35619948e-01\n",
      "  3.39877724e-02 4.14596935e-01 6.14996165e-01 5.42618211e-01\n",
      "  7.55463812e-01 2.23762124e-01 4.93280860e-01 9.54500742e-01\n",
      "  8.64483529e-02 6.86298282e-01 7.55476866e-01 1.94839544e-01\n",
      "  2.51697932e-01 2.73439099e-01 4.60031679e-01 3.83430213e-01\n",
      "  5.09569419e-01 6.18545433e-01 8.51101164e-01 2.35311617e-01\n",
      "  4.01875781e-01 6.53871242e-01 6.34530404e-01]]\n",
      "Change in theta: 1.7096826847000359e-06\n",
      "---------------------------------------------\n",
      "Iteration number 18 finished, Running time 331.71964597702026, Roo2 iteration 18 = -38674802.16022274\n",
      "Iteration 1:\n",
      "Theta: [[6.82572589e-01 2.04520826e-01 3.44626409e-01 5.75116274e-01\n",
      "  1.67798005e-01 3.75647571e-01 1.16354953e-01 8.83586991e-01\n",
      "  5.22041551e-01 3.47340352e-01 8.63885226e-01 1.62102016e-01\n",
      "  1.26221502e-01 5.95405413e-01 9.41482191e-01 7.64164489e-01\n",
      "  5.77494526e-01 7.73454093e-01 1.63611839e-01 7.90842114e-01\n",
      "  9.35668916e-02 2.68796241e-01 3.84677033e-01 4.97086290e-01\n",
      "  4.94989258e-01 6.36574520e-01 3.65357375e-01 2.17182729e-01\n",
      "  4.22688948e-01 8.48220139e-01 8.36634518e-01 9.86255523e-01\n",
      "  4.69587990e-01 5.17767252e-01 1.62583129e-02 8.68937333e-01\n",
      "  6.89866922e-01 6.26073676e-01 4.87928426e-01 5.22122611e-01\n",
      "  4.13603767e-01 4.41754899e-01 7.98435950e-01 4.11089022e-01\n",
      "  7.66484245e-01 2.83449149e-01 6.01822331e-01 7.80428995e-02\n",
      "  9.99852272e-01 2.20900013e-01 7.66518422e-01 6.18847038e-01\n",
      "  6.52119892e-01 9.73278402e-01 3.28904554e-01 2.16774957e-02\n",
      "  3.20344898e-01 1.12520752e-01 5.44479969e-02 9.88054055e-01\n",
      "  6.05794434e-02 8.64989910e-01 4.53310795e-02 4.81476069e-01\n",
      "  7.91208042e-01 3.88682924e-01 4.32178808e-01 7.85703136e-02\n",
      "  1.92391256e-01 9.14164651e-01 7.07725147e-01 5.39076105e-02\n",
      "  8.76092835e-01 5.85049415e-01 5.29697959e-01 9.12785035e-01\n",
      "  7.89522380e-01 6.30187992e-01 9.55836039e-01 4.74088414e-01\n",
      "  4.29750195e-01 9.38227049e-01 6.08038477e-01 7.48821276e-01\n",
      "  5.09962374e-01 8.82827841e-01 9.07734311e-01 5.37808640e-01\n",
      "  5.87246749e-01 1.60271423e-01 8.63050411e-01 9.99083616e-02\n",
      "  9.15923365e-01 1.18103083e-01 5.79016331e-01 3.95716674e-01\n",
      "  2.48619252e-01 9.28310310e-01 2.19442814e-01 6.00825734e-02\n",
      "  9.26521353e-01 7.02469907e-01 4.80494835e-01 4.96208314e-01\n",
      "  6.64214419e-03 1.45516217e-01 4.15117186e-01 3.65141901e-02\n",
      "  7.29439133e-01 5.36880850e-01 3.39501815e-01 6.57798716e-01\n",
      "  1.25027759e-01 4.34489279e-01 8.53904963e-02 9.71363977e-01\n",
      "  2.27625034e-01 7.88510925e-01 8.18929983e-01 6.22106388e-01\n",
      "  9.20610609e-01 3.11787389e-01 1.56010104e-01 9.24311618e-01\n",
      "  5.82786136e-01 9.49796173e-01 3.92976650e-02 7.11919336e-01\n",
      "  5.12347850e-01 7.59407826e-01 8.43597776e-01 1.22672756e-01\n",
      "  4.76996923e-01 4.99925687e-01 3.46119414e-01 6.76128186e-01\n",
      "  1.99225058e-01 3.46424131e-01 7.67288380e-01 6.21665663e-02\n",
      "  4.65666042e-01 1.35888220e-01 2.75110640e-02 8.14446318e-01\n",
      "  5.12285990e-01 2.89723537e-01 2.26164520e-01 8.59431589e-01\n",
      "  8.31771686e-01 7.21031261e-01 2.36536826e-01 3.34412651e-01\n",
      "  5.18954310e-03 2.74525370e-01 9.88366242e-02 8.88444200e-01\n",
      "  6.15307491e-01 3.40355760e-01 8.85031243e-01 4.82984242e-01\n",
      "  6.52140547e-01 5.35470799e-01 8.15434346e-01 4.69732098e-01\n",
      "  9.84672635e-01 4.41118607e-02 8.85725841e-01 6.94569525e-01\n",
      "  8.33649382e-01 4.62204678e-01 6.11425919e-01 2.26861499e-01\n",
      "  7.93882992e-01 2.29915503e-01 4.08260546e-01 4.88008051e-01\n",
      "  3.36781956e-01 3.71372728e-01 6.59191290e-03 3.56351877e-01\n",
      "  7.18394451e-01 1.38383880e-01 9.48266812e-01 1.24278056e-01\n",
      "  8.18097278e-01 7.58730945e-01 5.16489203e-01 3.77493326e-01\n",
      "  1.32114925e-01 8.55641272e-01 9.48060069e-01 2.25138956e-01\n",
      "  5.16198802e-01 7.49109053e-01 6.40109081e-01 8.66496163e-01\n",
      "  1.47636571e-01 7.42606717e-01 6.45741546e-01 7.58023154e-01\n",
      "  8.00796486e-01 2.21040208e-01 6.04675718e-03 6.27649784e-01\n",
      "  2.51516652e-01 7.70840570e-01 1.97895626e-01 9.04972190e-01\n",
      "  8.38359921e-01 4.17779434e-01 7.56716242e-01 5.77381043e-01\n",
      "  9.48720337e-01 6.54358148e-01 5.91206977e-01 9.88763270e-01\n",
      "  5.36360707e-01 9.00713569e-01 1.28366807e-01 3.92140351e-01\n",
      "  9.80300191e-01 1.78504591e-01 5.28770776e-01 2.79123926e-01\n",
      "  8.15455868e-04 4.24809453e-01 3.87221416e-01 8.86486967e-01\n",
      "  3.16548763e-01 2.40261765e-01 1.78151164e-01 5.45049669e-01\n",
      "  9.52057299e-02 1.16801118e-01 4.73099878e-01 2.60909115e-02\n",
      "  9.36592690e-01 4.60457969e-01 9.32282703e-01 8.83189409e-01\n",
      "  6.57935197e-02 3.07764019e-01 1.89246556e-01 8.87247392e-01\n",
      "  6.49863159e-01 2.15841549e-01 1.20432605e-01 8.95818025e-01\n",
      "  9.55024788e-01 5.13469180e-01 2.84492967e-01 2.51495296e-01\n",
      "  6.40165341e-01 7.51369850e-01 3.00913244e-01 5.88148474e-01\n",
      "  4.87420430e-01 8.58462574e-01 1.42283643e-01 7.98695856e-01\n",
      "  2.73851510e-01 3.83839610e-01 8.75996699e-01 5.68030002e-01\n",
      "  9.93544039e-01 2.36375254e-01 1.41587433e-01 1.24073599e-01\n",
      "  5.27912086e-01 3.77269200e-01 1.40561608e-02 5.92227642e-01\n",
      "  2.26232088e-01 2.20736455e-01 9.22162704e-01 3.01007230e-01\n",
      "  8.47038598e-01 4.54763984e-01 7.00674595e-01 4.32777253e-01\n",
      "  2.82783079e-01 1.21282381e-02 9.12968333e-01 9.16880384e-02\n",
      "  4.80186591e-01 7.40667020e-01 1.07386932e-01 5.77282659e-01\n",
      "  2.73551266e-01 8.93723231e-01 8.18052502e-01 1.64504785e-01\n",
      "  3.99038157e-01 2.98320349e-01 1.90865264e-02 3.37455966e-01\n",
      "  8.73763198e-01 5.34795587e-01 1.41331975e-02 1.63436446e-01\n",
      "  7.28871173e-01 6.17557944e-01 5.02947098e-01 4.73830212e-01\n",
      "  9.99534245e-01 4.57505490e-02 1.05080381e-03 8.25581404e-01\n",
      "  3.13222204e-01 2.34273931e-02 5.74198440e-01 3.39241282e-01\n",
      "  2.72268909e-01 6.21528255e-01 7.02701592e-01 1.13285746e-01\n",
      "  4.60249147e-01 1.75227477e-01 8.24622989e-01 7.05577535e-02\n",
      "  6.04572106e-01 8.05311291e-01 9.78893818e-01 5.74006143e-01\n",
      "  5.04730049e-01 2.05054677e-01 2.04865193e-01 9.94130313e-01\n",
      "  4.67039293e-02 9.70117861e-01 9.73002763e-01 1.93458555e-01\n",
      "  9.06029712e-01 1.03415989e-01 2.73918344e-01 1.36030317e-01\n",
      "  2.00722573e-01 5.62375205e-01 6.69033187e-01 8.94447939e-01\n",
      "  1.83288995e-01 6.23318075e-01 3.17595123e-01 6.02592005e-01\n",
      "  1.42338993e-01 4.40903727e-01 1.35386445e-01 4.46418882e-01\n",
      "  8.82992692e-01 7.66577593e-01 8.33555250e-01 6.51918288e-01\n",
      "  4.96794344e-01 1.96712862e-01 4.71318214e-01 8.09277055e-01\n",
      "  9.68954980e-01 1.04467427e-01 9.82745699e-02 5.16544276e-02\n",
      "  5.56996579e-01 3.29731448e-01 4.34732378e-01 7.55830854e-01\n",
      "  8.99337236e-01 7.09525627e-01 2.51637824e-01 7.89584155e-01\n",
      "  1.51672870e-01 2.22347990e-01 2.34430111e-01 5.26801394e-01\n",
      "  7.36717912e-01 4.32953257e-01 2.72586973e-02 7.41957935e-01\n",
      "  5.81489523e-01 6.78599282e-01 3.52226098e-01 3.95198616e-01\n",
      "  2.58021322e-01 9.22852674e-01 5.25843019e-01 8.64685866e-01\n",
      "  8.70278677e-02 6.75851029e-01 1.13517473e-01 8.76540845e-01\n",
      "  8.34089688e-01 2.01394292e-01 1.08224144e-01 4.38358616e-01\n",
      "  2.80596939e-01 4.26077560e-01 7.02973355e-01 6.32361326e-01\n",
      "  3.66939760e-01 3.11665328e-01 3.82074754e-01 7.29476428e-01\n",
      "  9.69662854e-03 6.19857511e-02 3.53988685e-01 2.69963543e-01\n",
      "  9.66148145e-01 7.30327048e-01 9.43745743e-01 7.51396379e-01\n",
      "  5.49229884e-01 5.10251349e-01 6.98475258e-01 5.46677574e-01\n",
      "  4.81240870e-01 5.08642286e-01 1.50953969e-01 3.67522049e-01\n",
      "  8.24297117e-01 9.09531824e-01 9.44621862e-02 8.25106371e-01\n",
      "  2.40394879e-01 9.05176957e-01 3.45902317e-01 2.32406059e-01\n",
      "  2.50715565e-01 4.80687990e-02 9.59474913e-01 1.43132348e-01\n",
      "  9.60569341e-01 6.71630129e-01 3.57141490e-01 9.39045379e-01\n",
      "  3.73590268e-01 2.37524536e-01 1.69444509e-01 7.89372349e-01\n",
      "  1.27682496e-01 6.36392719e-01 2.12395655e-01 7.46965259e-01\n",
      "  6.92352328e-01 9.55247954e-01 1.11656791e-01 5.90111743e-01\n",
      "  7.51899224e-02 6.89096014e-01 1.23329091e-01 8.00804374e-01\n",
      "  4.69704902e-01 3.68751713e-01 5.13024730e-01 9.94909272e-01\n",
      "  9.26167303e-01 5.79562895e-01 5.40855017e-01 9.24874663e-01\n",
      "  4.96933486e-01 3.09737612e-01 9.76923524e-01 8.82779334e-01\n",
      "  8.42209822e-01 5.27658835e-02 5.77239724e-01 9.42872636e-01\n",
      "  9.52961265e-01 8.25952400e-02 9.22205854e-01 9.69187717e-02\n",
      "  6.48954195e-01 9.51237488e-01 1.90830733e-01 5.94354063e-01\n",
      "  8.19264822e-01 8.25585821e-01 9.12818602e-01 7.80935532e-01\n",
      "  1.40322075e-01 3.04264844e-01 1.01066549e-01 2.08813947e-01\n",
      "  8.76159017e-01 5.92042827e-01 6.43790877e-02 6.72074992e-02\n",
      "  8.63098471e-02 2.25002902e-01 6.52606944e-01 6.14432269e-01\n",
      "  8.17665956e-01 4.13471508e-01 5.53460747e-01 9.86157913e-01\n",
      "  9.98203826e-01 7.21733656e-01 6.89879596e-01 1.79586181e-01\n",
      "  3.67413264e-01 5.29400622e-01 9.16701280e-01 2.23357377e-01\n",
      "  1.03076068e-01 7.59483750e-01 5.64295784e-01 4.23057222e-01\n",
      "  6.15089801e-01 2.35912144e-01 8.00333963e-01 9.25104599e-01\n",
      "  8.50862844e-01 5.93767968e-01 1.46443625e-02 9.47513321e-01\n",
      "  1.23539969e-01 6.63004828e-01 5.01390266e-01 1.08921147e-01\n",
      "  1.02239606e-01 7.68050104e-01 4.78183632e-01 7.49510601e-02\n",
      "  6.96287478e-01 8.02035423e-02 4.29916890e-01 5.64904894e-01\n",
      "  3.91938744e-01 5.87259115e-01 4.26963355e-01 4.31608446e-01\n",
      "  6.36815791e-02 3.29834944e-01 1.59521806e-01 7.22778585e-01\n",
      "  1.45317871e-01 3.38388234e-01 8.64591568e-01 7.93614476e-02\n",
      "  8.82646367e-01 1.26558858e-01 4.54619680e-01 7.47072147e-01\n",
      "  8.38647124e-01 7.88239205e-01 4.87979911e-01 1.02032453e-01\n",
      "  7.53823107e-01 8.80501986e-01 5.26666297e-01 1.70830553e-01\n",
      "  4.59577529e-01 9.68489602e-01 3.58479883e-01 5.99293372e-01\n",
      "  9.17979342e-01 7.24715568e-01 1.48538714e-01 5.61826581e-01\n",
      "  8.79341788e-01 3.57897859e-01 9.23140533e-01 3.06274960e-01\n",
      "  1.99893857e-01 5.77200284e-01 5.64175231e-01 6.50674623e-01\n",
      "  8.99022331e-01 9.87215077e-01 1.40561357e-01 4.20667581e-01\n",
      "  8.65568982e-01 4.93325970e-01 8.36030200e-02 7.62233154e-01\n",
      "  8.09693395e-01 7.33150006e-01 8.53775805e-01 7.27788951e-01\n",
      "  7.21594745e-01 5.75474089e-01 1.85633021e-01 7.60939552e-01\n",
      "  8.68986809e-01 9.52525020e-01 1.98958415e-01 7.96136858e-01\n",
      "  7.70826357e-02 9.90880470e-01 1.97834151e-01 4.74433878e-02\n",
      "  4.47468752e-01 7.70384350e-01 3.76058753e-01 8.45130602e-01\n",
      "  3.03898186e-01 2.27216624e-01 7.00379492e-02 3.80353743e-01\n",
      "  7.33134033e-02 7.45897964e-01 9.68575016e-01 1.31820063e-01\n",
      "  1.20925073e-03 3.14202337e-01 4.05173628e-01 8.80297754e-01\n",
      "  6.47537879e-01 1.54624501e-01 1.79615937e-01 4.07197389e-01\n",
      "  6.54978011e-01 5.79011683e-01 2.89475053e-01 9.09076040e-01\n",
      "  9.27793919e-01 9.15112838e-01 1.25442666e-01 1.68087678e-01\n",
      "  5.24878692e-01 6.45075998e-01 5.74221330e-02 8.80101989e-01\n",
      "  4.66108900e-01 4.75793245e-01 9.24946041e-01 5.30050163e-01\n",
      "  3.95269413e-01 7.50716685e-01 2.51711299e-01 9.60986826e-01\n",
      "  6.68697461e-01 8.39519989e-01 9.94640281e-01 7.65507624e-01\n",
      "  5.10787662e-01 3.75962844e-01 4.98997899e-01 5.77264642e-02\n",
      "  7.28731216e-01 7.74326483e-01 7.94850734e-01 4.15575989e-01\n",
      "  4.42384245e-01 2.55211174e-01 6.40361129e-01 3.30227971e-01\n",
      "  1.65402375e-01 3.51085164e-01 6.43732613e-01 4.72089670e-01\n",
      "  2.44009732e-01 4.43199370e-01 5.10143278e-01 4.09461279e-01\n",
      "  2.07556968e-01 8.90965105e-01 9.23903497e-01 6.80914044e-01\n",
      "  1.56219265e-01 3.89119835e-01 7.27519835e-01 6.75596268e-01\n",
      "  8.96698517e-01 2.88808972e-01 5.00047649e-01 7.30117623e-01\n",
      "  1.90253697e-01 5.63506413e-01 2.45777743e-01 3.42287907e-01\n",
      "  4.26174430e-01 8.07077448e-01 4.93455206e-01 8.56620370e-02\n",
      "  7.99068856e-01 3.11805423e-01 7.64811211e-01 7.40973420e-01\n",
      "  4.86188825e-01 1.69100509e-01 4.74844136e-01 3.51401859e-01\n",
      "  9.34619017e-01 9.79415129e-01 3.23965681e-01 4.71714745e-01\n",
      "  6.93343210e-02 9.35391363e-01 5.14905173e-01 1.31699699e-01\n",
      "  2.70938571e-01 1.35227169e-01 9.95835290e-01 1.72702974e-01\n",
      "  4.46679546e-01 4.99666812e-01 8.38710907e-01 2.13994709e-01\n",
      "  9.03476306e-01 9.88384058e-01 8.00236832e-01 5.48058421e-01\n",
      "  5.24022644e-01 5.01592175e-01 1.10479622e-01 8.55221244e-01\n",
      "  8.68573569e-01 9.84731303e-01 7.90513513e-01 5.71467704e-01\n",
      "  2.88334796e-01 9.33099019e-01 8.21233312e-01 6.92286628e-01\n",
      "  2.08971496e-01 5.16269968e-01 7.86231338e-02 2.31581413e-01\n",
      "  9.92601606e-01 6.36645919e-01 2.47826748e-01 8.49986052e-01\n",
      "  3.60310597e-01 1.71884961e-01 1.25639404e-01 4.33520375e-01\n",
      "  3.68700904e-01 9.46483843e-01 7.36788520e-01 3.60601297e-01\n",
      "  8.07356524e-01 3.95645833e-01 4.06500984e-01 7.38387536e-01\n",
      "  8.54091831e-01 6.67343838e-01 7.23897009e-01 1.74528995e-01\n",
      "  9.23445563e-02 8.46752143e-01 3.91484151e-01 8.22160408e-01\n",
      "  2.63937592e-01 4.67597866e-01 2.50133922e-01 5.23064963e-01\n",
      "  2.45274619e-01 5.58067070e-01 1.28889135e-01 2.39331756e-02\n",
      "  3.29895038e-01 8.42648431e-01 4.12459858e-01 7.62079282e-01\n",
      "  1.83392359e-01 3.04327087e-01 5.46355386e-01 7.63155383e-01\n",
      "  8.09630614e-01 2.56456771e-01 9.98521939e-01 7.98293636e-02\n",
      "  7.79707385e-01 8.79003976e-01 6.58052306e-01 9.70737992e-01\n",
      "  1.45859003e-01 9.45152699e-01 5.79509809e-02 4.42629835e-01\n",
      "  7.67005844e-01 7.57370397e-02 1.64858943e-01 6.43933990e-01\n",
      "  5.57353422e-01 8.70985628e-01 2.54181145e-01 3.17556073e-02\n",
      "  3.30296046e-01 4.70737581e-02 8.03731424e-01 2.99277434e-01\n",
      "  7.07828615e-01 3.52635225e-01 3.39158891e-01 2.17995514e-01\n",
      "  3.35995714e-01 1.39098688e-02 7.24115321e-01 8.14953396e-01\n",
      "  2.41683892e-01 1.75033264e-01 2.96039403e-02 9.32522713e-01\n",
      "  4.65672633e-01 9.79169685e-01 8.04201539e-01 1.61067983e-01\n",
      "  3.98146828e-01 1.59933164e-01 4.77528550e-01 2.24069579e-02\n",
      "  7.50472591e-01 8.18730329e-01 8.06216796e-01 5.60814601e-01\n",
      "  9.13320380e-01 8.93210202e-01 2.04385775e-01 3.99973930e-01\n",
      "  6.82211819e-01 4.78421500e-01 2.04222436e-01 8.36654615e-01\n",
      "  7.14372135e-01 2.42748680e-01 6.69395985e-01 4.88101412e-01\n",
      "  9.52271857e-01 2.82859499e-01 9.38247623e-01 7.98683160e-01\n",
      "  8.46972960e-01 8.50871596e-01 2.86736103e-03 2.92005686e-01\n",
      "  7.72448462e-01 7.34959796e-01 4.99527034e-01 9.60725262e-01\n",
      "  2.69297489e-01 7.67423720e-02 8.13093871e-01 9.14850728e-01\n",
      "  8.52980918e-01 1.87462656e-01 9.12958589e-01 8.93470442e-01\n",
      "  1.53604656e-01 8.52538051e-01 4.12791486e-01 6.47761842e-01\n",
      "  4.37669638e-01 6.53466483e-01 8.43432402e-01 2.65408179e-01\n",
      "  2.97434176e-01 7.01276765e-01 6.26219473e-01 1.01227751e-01\n",
      "  2.28363325e-01 5.24532616e-01 6.52707604e-01 7.56428108e-01\n",
      "  6.24631347e-01 4.88482809e-01 7.96429423e-01 9.54921558e-01\n",
      "  3.26722232e-01 2.17757778e-01 8.58662220e-01 2.18625910e-01\n",
      "  9.66930919e-01 8.53117883e-01 3.38970686e-01 7.39866039e-01\n",
      "  9.03361487e-01 7.61729129e-02 9.34550806e-01 5.81312314e-02\n",
      "  5.65204785e-01 5.48721383e-01 2.58712719e-01 4.96653847e-01\n",
      "  9.55195048e-01 6.37776407e-01 4.51728045e-01 8.25722616e-01\n",
      "  6.62392149e-01 7.54713251e-01 8.38830598e-01 4.09214140e-01\n",
      "  6.80230881e-01 2.08246719e-01 7.07906591e-02 2.82839252e-01\n",
      "  3.36613638e-01 5.06704247e-01 2.40690991e-01 9.35620345e-01\n",
      "  3.39881694e-02 4.14597332e-01 6.14996562e-01 5.42618608e-01\n",
      "  7.55464209e-01 2.23762521e-01 4.93281256e-01 9.54501139e-01\n",
      "  8.64487499e-02 6.86298679e-01 7.55477263e-01 1.94839941e-01\n",
      "  2.51698329e-01 2.73439496e-01 4.60032076e-01 3.83430610e-01\n",
      "  5.09569816e-01 6.18545830e-01 8.51101561e-01 2.35312014e-01\n",
      "  4.01876178e-01 6.53871638e-01 6.34530801e-01]]\n",
      "Change in theta: 1.7411579249550913e-06\n",
      "---------------------------------------------\n",
      "Iteration number 19 finished, Running time 408.72991394996643, Roo2 iteration 19 = -14010856.486886462\n",
      "Iteration 1:\n",
      "Theta: [[6.82572959e-01 2.04521197e-01 3.44626780e-01 5.75116645e-01\n",
      "  1.67798375e-01 3.75647942e-01 1.16355324e-01 8.83587362e-01\n",
      "  5.22041922e-01 3.47340723e-01 8.63885597e-01 1.62102386e-01\n",
      "  1.26221872e-01 5.95405784e-01 9.41482561e-01 7.64164860e-01\n",
      "  5.77494896e-01 7.73454464e-01 1.63612210e-01 7.90842485e-01\n",
      "  9.35672624e-02 2.68796612e-01 3.84677404e-01 4.97086661e-01\n",
      "  4.94989628e-01 6.36574891e-01 3.65357746e-01 2.17183100e-01\n",
      "  4.22689318e-01 8.48220510e-01 8.36634889e-01 9.86255893e-01\n",
      "  4.69588361e-01 5.17767622e-01 1.62586837e-02 8.68937704e-01\n",
      "  6.89867293e-01 6.26074047e-01 4.87928797e-01 5.22122982e-01\n",
      "  4.13604138e-01 4.41755269e-01 7.98436321e-01 4.11089393e-01\n",
      "  7.66484616e-01 2.83449520e-01 6.01822702e-01 7.80432702e-02\n",
      "  9.99852642e-01 2.20900383e-01 7.66518793e-01 6.18847408e-01\n",
      "  6.52120263e-01 9.73278773e-01 3.28904925e-01 2.16778665e-02\n",
      "  3.20345269e-01 1.12521123e-01 5.44483677e-02 9.88054426e-01\n",
      "  6.05798142e-02 8.64990281e-01 4.53314503e-02 4.81476440e-01\n",
      "  7.91208413e-01 3.88683295e-01 4.32179178e-01 7.85706843e-02\n",
      "  1.92391627e-01 9.14165022e-01 7.07725518e-01 5.39079813e-02\n",
      "  8.76093206e-01 5.85049786e-01 5.29698329e-01 9.12785405e-01\n",
      "  7.89522750e-01 6.30188362e-01 9.55836410e-01 4.74088784e-01\n",
      "  4.29750566e-01 9.38227420e-01 6.08038847e-01 7.48821646e-01\n",
      "  5.09962744e-01 8.82828212e-01 9.07734682e-01 5.37809010e-01\n",
      "  5.87247119e-01 1.60271794e-01 8.63050781e-01 9.99087324e-02\n",
      "  9.15923736e-01 1.18103454e-01 5.79016701e-01 3.95717045e-01\n",
      "  2.48619623e-01 9.28310681e-01 2.19443185e-01 6.00829442e-02\n",
      "  9.26521724e-01 7.02470278e-01 4.80495206e-01 4.96208685e-01\n",
      "  6.64251497e-03 1.45516588e-01 4.15117557e-01 3.65145609e-02\n",
      "  7.29439504e-01 5.36881221e-01 3.39502186e-01 6.57799087e-01\n",
      "  1.25028130e-01 4.34489650e-01 8.53908671e-02 9.71364348e-01\n",
      "  2.27625405e-01 7.88511296e-01 8.18930353e-01 6.22106759e-01\n",
      "  9.20610979e-01 3.11787760e-01 1.56010475e-01 9.24311989e-01\n",
      "  5.82786507e-01 9.49796544e-01 3.92980358e-02 7.11919706e-01\n",
      "  5.12348221e-01 7.59408197e-01 8.43598147e-01 1.22673127e-01\n",
      "  4.76997294e-01 4.99926058e-01 3.46119785e-01 6.76128557e-01\n",
      "  1.99225429e-01 3.46424502e-01 7.67288751e-01 6.21669371e-02\n",
      "  4.65666413e-01 1.35888591e-01 2.75114348e-02 8.14446688e-01\n",
      "  5.12286360e-01 2.89723908e-01 2.26164891e-01 8.59431960e-01\n",
      "  8.31772056e-01 7.21031632e-01 2.36537197e-01 3.34413021e-01\n",
      "  5.18991388e-03 2.74525741e-01 9.88369950e-02 8.88444571e-01\n",
      "  6.15307862e-01 3.40356131e-01 8.85031613e-01 4.82984613e-01\n",
      "  6.52140918e-01 5.35471169e-01 8.15434717e-01 4.69732469e-01\n",
      "  9.84673006e-01 4.41122315e-02 8.85726212e-01 6.94569895e-01\n",
      "  8.33649753e-01 4.62205049e-01 6.11426289e-01 2.26861870e-01\n",
      "  7.93883362e-01 2.29915874e-01 4.08260917e-01 4.88008422e-01\n",
      "  3.36782326e-01 3.71373099e-01 6.59228367e-03 3.56352248e-01\n",
      "  7.18394822e-01 1.38384250e-01 9.48267183e-01 1.24278427e-01\n",
      "  8.18097649e-01 7.58731316e-01 5.16489574e-01 3.77493697e-01\n",
      "  1.32115295e-01 8.55641643e-01 9.48060440e-01 2.25139326e-01\n",
      "  5.16199173e-01 7.49109424e-01 6.40109451e-01 8.66496534e-01\n",
      "  1.47636942e-01 7.42607088e-01 6.45741917e-01 7.58023524e-01\n",
      "  8.00796857e-01 2.21040579e-01 6.04712795e-03 6.27650155e-01\n",
      "  2.51517023e-01 7.70840940e-01 1.97895997e-01 9.04972560e-01\n",
      "  8.38360292e-01 4.17779805e-01 7.56716613e-01 5.77381414e-01\n",
      "  9.48720708e-01 6.54358519e-01 5.91207348e-01 9.88763641e-01\n",
      "  5.36361077e-01 9.00713940e-01 1.28367177e-01 3.92140722e-01\n",
      "  9.80300561e-01 1.78504962e-01 5.28771147e-01 2.79124297e-01\n",
      "  8.15826641e-04 4.24809824e-01 3.87221786e-01 8.86487337e-01\n",
      "  3.16549134e-01 2.40262136e-01 1.78151535e-01 5.45050039e-01\n",
      "  9.52061006e-02 1.16801489e-01 4.73100248e-01 2.60912823e-02\n",
      "  9.36593061e-01 4.60458340e-01 9.32283074e-01 8.83189780e-01\n",
      "  6.57938904e-02 3.07764390e-01 1.89246927e-01 8.87247763e-01\n",
      "  6.49863530e-01 2.15841920e-01 1.20432976e-01 8.95818396e-01\n",
      "  9.55025159e-01 5.13469551e-01 2.84493338e-01 2.51495666e-01\n",
      "  6.40165711e-01 7.51370221e-01 3.00913615e-01 5.88148845e-01\n",
      "  4.87420801e-01 8.58462945e-01 1.42284013e-01 7.98696227e-01\n",
      "  2.73851881e-01 3.83839980e-01 8.75997070e-01 5.68030373e-01\n",
      "  9.93544410e-01 2.36375625e-01 1.41587804e-01 1.24073970e-01\n",
      "  5.27912457e-01 3.77269570e-01 1.40565316e-02 5.92228013e-01\n",
      "  2.26232459e-01 2.20736826e-01 9.22163075e-01 3.01007601e-01\n",
      "  8.47038969e-01 4.54764354e-01 7.00674965e-01 4.32777624e-01\n",
      "  2.82783450e-01 1.21286089e-02 9.12968704e-01 9.16884092e-02\n",
      "  4.80186962e-01 7.40667391e-01 1.07387302e-01 5.77283030e-01\n",
      "  2.73551637e-01 8.93723602e-01 8.18052873e-01 1.64505156e-01\n",
      "  3.99038527e-01 2.98320720e-01 1.90868971e-02 3.37456337e-01\n",
      "  8.73763569e-01 5.34795957e-01 1.41335683e-02 1.63436817e-01\n",
      "  7.28871544e-01 6.17558315e-01 5.02947469e-01 4.73830582e-01\n",
      "  9.99534616e-01 4.57509198e-02 1.05117459e-03 8.25581774e-01\n",
      "  3.13222575e-01 2.34277639e-02 5.74198811e-01 3.39241653e-01\n",
      "  2.72269280e-01 6.21528626e-01 7.02701963e-01 1.13286117e-01\n",
      "  4.60249518e-01 1.75227848e-01 8.24623360e-01 7.05581242e-02\n",
      "  6.04572477e-01 8.05311662e-01 9.78894189e-01 5.74006514e-01\n",
      "  5.04730420e-01 2.05055047e-01 2.04865564e-01 9.94130684e-01\n",
      "  4.67043001e-02 9.70118231e-01 9.73003134e-01 1.93458925e-01\n",
      "  9.06030082e-01 1.03416360e-01 2.73918715e-01 1.36030688e-01\n",
      "  2.00722944e-01 5.62375575e-01 6.69033558e-01 8.94448309e-01\n",
      "  1.83289366e-01 6.23318446e-01 3.17595494e-01 6.02592376e-01\n",
      "  1.42339364e-01 4.40904098e-01 1.35386816e-01 4.46419253e-01\n",
      "  8.82993062e-01 7.66577963e-01 8.33555620e-01 6.51918659e-01\n",
      "  4.96794715e-01 1.96713233e-01 4.71318585e-01 8.09277426e-01\n",
      "  9.68955351e-01 1.04467798e-01 9.82749407e-02 5.16547983e-02\n",
      "  5.56996950e-01 3.29731819e-01 4.34732749e-01 7.55831225e-01\n",
      "  8.99337607e-01 7.09525998e-01 2.51638195e-01 7.89584526e-01\n",
      "  1.51673241e-01 2.22348361e-01 2.34430482e-01 5.26801765e-01\n",
      "  7.36718282e-01 4.32953628e-01 2.72590681e-02 7.41958306e-01\n",
      "  5.81489894e-01 6.78599652e-01 3.52226469e-01 3.95198986e-01\n",
      "  2.58021692e-01 9.22853045e-01 5.25843390e-01 8.64686237e-01\n",
      "  8.70282385e-02 6.75851399e-01 1.13517844e-01 8.76541216e-01\n",
      "  8.34090059e-01 2.01394662e-01 1.08224515e-01 4.38358987e-01\n",
      "  2.80597310e-01 4.26077931e-01 7.02973725e-01 6.32361697e-01\n",
      "  3.66940130e-01 3.11665699e-01 3.82075125e-01 7.29476798e-01\n",
      "  9.69699931e-03 6.19861218e-02 3.53989056e-01 2.69963914e-01\n",
      "  9.66148516e-01 7.30327419e-01 9.43746114e-01 7.51396750e-01\n",
      "  5.49230255e-01 5.10251719e-01 6.98475628e-01 5.46677945e-01\n",
      "  4.81241241e-01 5.08642657e-01 1.50954340e-01 3.67522419e-01\n",
      "  8.24297488e-01 9.09532195e-01 9.44625569e-02 8.25106742e-01\n",
      "  2.40395250e-01 9.05177327e-01 3.45902688e-01 2.32406430e-01\n",
      "  2.50715936e-01 4.80691698e-02 9.59475284e-01 1.43132719e-01\n",
      "  9.60569712e-01 6.71630500e-01 3.57141861e-01 9.39045750e-01\n",
      "  3.73590638e-01 2.37524907e-01 1.69444879e-01 7.89372720e-01\n",
      "  1.27682867e-01 6.36393089e-01 2.12396026e-01 7.46965630e-01\n",
      "  6.92352699e-01 9.55248325e-01 1.11657162e-01 5.90112114e-01\n",
      "  7.51902932e-02 6.89096385e-01 1.23329462e-01 8.00804744e-01\n",
      "  4.69705272e-01 3.68752084e-01 5.13025101e-01 9.94909643e-01\n",
      "  9.26167674e-01 5.79563266e-01 5.40855388e-01 9.24875033e-01\n",
      "  4.96933856e-01 3.09737983e-01 9.76923895e-01 8.82779705e-01\n",
      "  8.42210193e-01 5.27662542e-02 5.77240094e-01 9.42873007e-01\n",
      "  9.52961636e-01 8.25956108e-02 9.22206225e-01 9.69191424e-02\n",
      "  6.48954566e-01 9.51237858e-01 1.90831104e-01 5.94354434e-01\n",
      "  8.19265192e-01 8.25586192e-01 9.12818973e-01 7.80935902e-01\n",
      "  1.40322446e-01 3.04265215e-01 1.01066919e-01 2.08814318e-01\n",
      "  8.76159388e-01 5.92043197e-01 6.43794585e-02 6.72078700e-02\n",
      "  8.63102179e-02 2.25003273e-01 6.52607315e-01 6.14432640e-01\n",
      "  8.17666327e-01 4.13471879e-01 5.53461117e-01 9.86158284e-01\n",
      "  9.98204197e-01 7.21734026e-01 6.89879967e-01 1.79586552e-01\n",
      "  3.67413635e-01 5.29400993e-01 9.16701651e-01 2.23357748e-01\n",
      "  1.03076439e-01 7.59484121e-01 5.64296155e-01 4.23057593e-01\n",
      "  6.15090171e-01 2.35912515e-01 8.00334334e-01 9.25104970e-01\n",
      "  8.50863215e-01 5.93768338e-01 1.46447333e-02 9.47513692e-01\n",
      "  1.23540339e-01 6.63005199e-01 5.01390637e-01 1.08921518e-01\n",
      "  1.02239977e-01 7.68050474e-01 4.78184003e-01 7.49514309e-02\n",
      "  6.96287849e-01 8.02039131e-02 4.29917261e-01 5.64905265e-01\n",
      "  3.91939115e-01 5.87259486e-01 4.26963726e-01 4.31608817e-01\n",
      "  6.36819498e-02 3.29835315e-01 1.59522177e-01 7.22778956e-01\n",
      "  1.45318242e-01 3.38388604e-01 8.64591938e-01 7.93618183e-02\n",
      "  8.82646737e-01 1.26559229e-01 4.54620051e-01 7.47072517e-01\n",
      "  8.38647495e-01 7.88239576e-01 4.87980281e-01 1.02032824e-01\n",
      "  7.53823478e-01 8.80502356e-01 5.26666668e-01 1.70830924e-01\n",
      "  4.59577900e-01 9.68489973e-01 3.58480254e-01 5.99293743e-01\n",
      "  9.17979713e-01 7.24715938e-01 1.48539084e-01 5.61826951e-01\n",
      "  8.79342159e-01 3.57898229e-01 9.23140904e-01 3.06275331e-01\n",
      "  1.99894228e-01 5.77200655e-01 5.64175601e-01 6.50674994e-01\n",
      "  8.99022702e-01 9.87215448e-01 1.40561728e-01 4.20667952e-01\n",
      "  8.65569353e-01 4.93326340e-01 8.36033908e-02 7.62233525e-01\n",
      "  8.09693766e-01 7.33150377e-01 8.53776176e-01 7.27789322e-01\n",
      "  7.21595116e-01 5.75474460e-01 1.85633392e-01 7.60939922e-01\n",
      "  8.68987180e-01 9.52525391e-01 1.98958785e-01 7.96137229e-01\n",
      "  7.70830065e-02 9.90880841e-01 1.97834521e-01 4.74437586e-02\n",
      "  4.47469123e-01 7.70384720e-01 3.76059124e-01 8.45130973e-01\n",
      "  3.03898556e-01 2.27216995e-01 7.00383199e-02 3.80354114e-01\n",
      "  7.33137741e-02 7.45898335e-01 9.68575387e-01 1.31820433e-01\n",
      "  1.20962151e-03 3.14202708e-01 4.05173999e-01 8.80298125e-01\n",
      "  6.47538250e-01 1.54624872e-01 1.79616307e-01 4.07197760e-01\n",
      "  6.54978382e-01 5.79012054e-01 2.89475424e-01 9.09076411e-01\n",
      "  9.27794289e-01 9.15113209e-01 1.25443037e-01 1.68088048e-01\n",
      "  5.24879062e-01 6.45076369e-01 5.74225038e-02 8.80102360e-01\n",
      "  4.66109271e-01 4.75793616e-01 9.24946412e-01 5.30050533e-01\n",
      "  3.95269784e-01 7.50717056e-01 2.51711670e-01 9.60987197e-01\n",
      "  6.68697832e-01 8.39520360e-01 9.94640652e-01 7.65507995e-01\n",
      "  5.10788033e-01 3.75963215e-01 4.98998270e-01 5.77268350e-02\n",
      "  7.28731587e-01 7.74326854e-01 7.94851105e-01 4.15576360e-01\n",
      "  4.42384615e-01 2.55211545e-01 6.40361500e-01 3.30228342e-01\n",
      "  1.65402746e-01 3.51085535e-01 6.43732984e-01 4.72090041e-01\n",
      "  2.44010103e-01 4.43199741e-01 5.10143649e-01 4.09461650e-01\n",
      "  2.07557338e-01 8.90965476e-01 9.23903868e-01 6.80914415e-01\n",
      "  1.56219635e-01 3.89120206e-01 7.27520206e-01 6.75596639e-01\n",
      "  8.96698888e-01 2.88809343e-01 5.00048020e-01 7.30117994e-01\n",
      "  1.90254068e-01 5.63506783e-01 2.45778114e-01 3.42288278e-01\n",
      "  4.26174801e-01 8.07077818e-01 4.93455577e-01 8.56624078e-02\n",
      "  7.99069227e-01 3.11805793e-01 7.64811582e-01 7.40973791e-01\n",
      "  4.86189196e-01 1.69100880e-01 4.74844507e-01 3.51402230e-01\n",
      "  9.34619388e-01 9.79415500e-01 3.23966051e-01 4.71715116e-01\n",
      "  6.93346918e-02 9.35391734e-01 5.14905544e-01 1.31700070e-01\n",
      "  2.70938942e-01 1.35227540e-01 9.95835661e-01 1.72703344e-01\n",
      "  4.46679917e-01 4.99667183e-01 8.38711278e-01 2.13995079e-01\n",
      "  9.03476677e-01 9.88384429e-01 8.00237202e-01 5.48058792e-01\n",
      "  5.24023015e-01 5.01592546e-01 1.10479993e-01 8.55221614e-01\n",
      "  8.68573939e-01 9.84731674e-01 7.90513884e-01 5.71468075e-01\n",
      "  2.88335167e-01 9.33099390e-01 8.21233683e-01 6.92286998e-01\n",
      "  2.08971867e-01 5.16270339e-01 7.86235046e-02 2.31581784e-01\n",
      "  9.92601977e-01 6.36646290e-01 2.47827118e-01 8.49986423e-01\n",
      "  3.60310968e-01 1.71885331e-01 1.25639775e-01 4.33520746e-01\n",
      "  3.68701274e-01 9.46484214e-01 7.36788891e-01 3.60601667e-01\n",
      "  8.07356895e-01 3.95646204e-01 4.06501355e-01 7.38387907e-01\n",
      "  8.54092202e-01 6.67344209e-01 7.23897380e-01 1.74529366e-01\n",
      "  9.23449270e-02 8.46752514e-01 3.91484522e-01 8.22160779e-01\n",
      "  2.63937963e-01 4.67598237e-01 2.50134293e-01 5.23065334e-01\n",
      "  2.45274990e-01 5.58067441e-01 1.28889505e-01 2.39335464e-02\n",
      "  3.29895409e-01 8.42648802e-01 4.12460229e-01 7.62079653e-01\n",
      "  1.83392730e-01 3.04327457e-01 5.46355757e-01 7.63155754e-01\n",
      "  8.09630985e-01 2.56457142e-01 9.98522310e-01 7.98297344e-02\n",
      "  7.79707756e-01 8.79004347e-01 6.58052676e-01 9.70738363e-01\n",
      "  1.45859374e-01 9.45153070e-01 5.79513516e-02 4.42630205e-01\n",
      "  7.67006215e-01 7.57374104e-02 1.64859314e-01 6.43934361e-01\n",
      "  5.57353792e-01 8.70985999e-01 2.54181516e-01 3.17559781e-02\n",
      "  3.30296416e-01 4.70741288e-02 8.03731794e-01 2.99277805e-01\n",
      "  7.07828986e-01 3.52635596e-01 3.39159261e-01 2.17995884e-01\n",
      "  3.35996085e-01 1.39102396e-02 7.24115692e-01 8.14953767e-01\n",
      "  2.41684263e-01 1.75033635e-01 2.96043111e-02 9.32523083e-01\n",
      "  4.65673004e-01 9.79170056e-01 8.04201910e-01 1.61068353e-01\n",
      "  3.98147198e-01 1.59933535e-01 4.77528920e-01 2.24073287e-02\n",
      "  7.50472962e-01 8.18730699e-01 8.06217167e-01 5.60814971e-01\n",
      "  9.13320750e-01 8.93210572e-01 2.04386146e-01 3.99974301e-01\n",
      "  6.82212190e-01 4.78421871e-01 2.04222807e-01 8.36654986e-01\n",
      "  7.14372505e-01 2.42749050e-01 6.69396356e-01 4.88101783e-01\n",
      "  9.52272228e-01 2.82859869e-01 9.38247994e-01 7.98683531e-01\n",
      "  8.46973331e-01 8.50871967e-01 2.86773180e-03 2.92006057e-01\n",
      "  7.72448833e-01 7.34960167e-01 4.99527405e-01 9.60725633e-01\n",
      "  2.69297859e-01 7.67427428e-02 8.13094241e-01 9.14851099e-01\n",
      "  8.52981288e-01 1.87463027e-01 9.12958960e-01 8.93470812e-01\n",
      "  1.53605027e-01 8.52538422e-01 4.12791856e-01 6.47762213e-01\n",
      "  4.37670008e-01 6.53466853e-01 8.43432772e-01 2.65408550e-01\n",
      "  2.97434547e-01 7.01277136e-01 6.26219843e-01 1.01228122e-01\n",
      "  2.28363696e-01 5.24532987e-01 6.52707974e-01 7.56428478e-01\n",
      "  6.24631718e-01 4.88483180e-01 7.96429794e-01 9.54921929e-01\n",
      "  3.26722602e-01 2.17758148e-01 8.58662590e-01 2.18626280e-01\n",
      "  9.66931290e-01 8.53118254e-01 3.38971057e-01 7.39866410e-01\n",
      "  9.03361858e-01 7.61732837e-02 9.34551176e-01 5.81316021e-02\n",
      "  5.65205156e-01 5.48721754e-01 2.58713090e-01 4.96654218e-01\n",
      "  9.55195418e-01 6.37776778e-01 4.51728416e-01 8.25722987e-01\n",
      "  6.62392519e-01 7.54713621e-01 8.38830969e-01 4.09214510e-01\n",
      "  6.80231252e-01 2.08247090e-01 7.07910299e-02 2.82839623e-01\n",
      "  3.36614009e-01 5.06704617e-01 2.40691362e-01 9.35620716e-01\n",
      "  3.39885402e-02 4.14597702e-01 6.14996933e-01 5.42618979e-01\n",
      "  7.55464580e-01 2.23762892e-01 4.93281627e-01 9.54501509e-01\n",
      "  8.64491207e-02 6.86299050e-01 7.55477633e-01 1.94840312e-01\n",
      "  2.51698700e-01 2.73439867e-01 4.60032447e-01 3.83430981e-01\n",
      "  5.09570187e-01 6.18546201e-01 8.51101932e-01 2.35312385e-01\n",
      "  4.01876549e-01 6.53872009e-01 6.34531172e-01]]\n",
      "Change in theta: 1.7708168241602795e-06\n",
      "---------------------------------------------\n",
      "Iteration number 20 finished, Running time 546.4501509666443, Roo2 iteration 20 = -10395849.42620507\n",
      "Iteration 1:\n",
      "Theta: [[6.82573335e-01 2.04521573e-01 3.44627156e-01 5.75117020e-01\n",
      "  1.67798751e-01 3.75648317e-01 1.16355699e-01 8.83587738e-01\n",
      "  5.22042298e-01 3.47341099e-01 8.63885973e-01 1.62102762e-01\n",
      "  1.26222248e-01 5.95406159e-01 9.41482937e-01 7.64165236e-01\n",
      "  5.77495272e-01 7.73454840e-01 1.63612586e-01 7.90842860e-01\n",
      "  9.35676382e-02 2.68796987e-01 3.84677780e-01 4.97087036e-01\n",
      "  4.94990004e-01 6.36575266e-01 3.65358122e-01 2.17183475e-01\n",
      "  4.22689694e-01 8.48220886e-01 8.36635265e-01 9.86256269e-01\n",
      "  4.69588737e-01 5.17767998e-01 1.62590595e-02 8.68938080e-01\n",
      "  6.89867669e-01 6.26074423e-01 4.87929173e-01 5.22123358e-01\n",
      "  4.13604514e-01 4.41755645e-01 7.98436697e-01 4.11089769e-01\n",
      "  7.66484992e-01 2.83449896e-01 6.01823078e-01 7.80436460e-02\n",
      "  9.99853018e-01 2.20900759e-01 7.66519169e-01 6.18847784e-01\n",
      "  6.52120639e-01 9.73279148e-01 3.28905301e-01 2.16782423e-02\n",
      "  3.20345645e-01 1.12521498e-01 5.44487435e-02 9.88054802e-01\n",
      "  6.05801900e-02 8.64990656e-01 4.53318261e-02 4.81476816e-01\n",
      "  7.91208789e-01 3.88683671e-01 4.32179554e-01 7.85710601e-02\n",
      "  1.92392003e-01 9.14165398e-01 7.07725893e-01 5.39083571e-02\n",
      "  8.76093582e-01 5.85050161e-01 5.29698705e-01 9.12785781e-01\n",
      "  7.89523126e-01 6.30188738e-01 9.55836786e-01 4.74089160e-01\n",
      "  4.29750942e-01 9.38227796e-01 6.08039223e-01 7.48822022e-01\n",
      "  5.09963120e-01 8.82828588e-01 9.07735058e-01 5.37809386e-01\n",
      "  5.87247495e-01 1.60272170e-01 8.63051157e-01 9.99091082e-02\n",
      "  9.15924112e-01 1.18103830e-01 5.79017077e-01 3.95717421e-01\n",
      "  2.48619999e-01 9.28311057e-01 2.19443561e-01 6.00833200e-02\n",
      "  9.26522100e-01 7.02470654e-01 4.80495581e-01 4.96209061e-01\n",
      "  6.64289078e-03 1.45516963e-01 4.15117933e-01 3.65149367e-02\n",
      "  7.29439880e-01 5.36881597e-01 3.39502562e-01 6.57799463e-01\n",
      "  1.25028506e-01 4.34490026e-01 8.53912429e-02 9.71364724e-01\n",
      "  2.27625781e-01 7.88511671e-01 8.18930729e-01 6.22107135e-01\n",
      "  9.20611355e-01 3.11788136e-01 1.56010851e-01 9.24312364e-01\n",
      "  5.82786883e-01 9.49796920e-01 3.92984116e-02 7.11920082e-01\n",
      "  5.12348597e-01 7.59408573e-01 8.43598523e-01 1.22673502e-01\n",
      "  4.76997670e-01 4.99926433e-01 3.46120161e-01 6.76128933e-01\n",
      "  1.99225804e-01 3.46424878e-01 7.67289127e-01 6.21673129e-02\n",
      "  4.65666788e-01 1.35888967e-01 2.75118106e-02 8.14447064e-01\n",
      "  5.12286736e-01 2.89724284e-01 2.26165267e-01 8.59432335e-01\n",
      "  8.31772432e-01 7.21032008e-01 2.36537573e-01 3.34413397e-01\n",
      "  5.19028969e-03 2.74526116e-01 9.88373708e-02 8.88444947e-01\n",
      "  6.15308238e-01 3.40356507e-01 8.85031989e-01 4.82984989e-01\n",
      "  6.52141294e-01 5.35471545e-01 8.15435093e-01 4.69732844e-01\n",
      "  9.84673382e-01 4.41126073e-02 8.85726588e-01 6.94570271e-01\n",
      "  8.33650128e-01 4.62205424e-01 6.11426665e-01 2.26862246e-01\n",
      "  7.93883738e-01 2.29916250e-01 4.08261293e-01 4.88008798e-01\n",
      "  3.36782702e-01 3.71373475e-01 6.59265948e-03 3.56352624e-01\n",
      "  7.18395198e-01 1.38384626e-01 9.48267559e-01 1.24278803e-01\n",
      "  8.18098024e-01 7.58731692e-01 5.16489949e-01 3.77494073e-01\n",
      "  1.32115671e-01 8.55642019e-01 9.48060816e-01 2.25139702e-01\n",
      "  5.16199549e-01 7.49109799e-01 6.40109827e-01 8.66496909e-01\n",
      "  1.47637317e-01 7.42607463e-01 6.45742293e-01 7.58023900e-01\n",
      "  8.00797232e-01 2.21040955e-01 6.04750377e-03 6.27650530e-01\n",
      "  2.51517399e-01 7.70841316e-01 1.97896372e-01 9.04972936e-01\n",
      "  8.38360668e-01 4.17780180e-01 7.56716989e-01 5.77381790e-01\n",
      "  9.48721083e-01 6.54358894e-01 5.91207723e-01 9.88764017e-01\n",
      "  5.36361453e-01 9.00714316e-01 1.28367553e-01 3.92141098e-01\n",
      "  9.80300937e-01 1.78505338e-01 5.28771523e-01 2.79124673e-01\n",
      "  8.16202453e-04 4.24810200e-01 3.87222162e-01 8.86487713e-01\n",
      "  3.16549510e-01 2.40262512e-01 1.78151910e-01 5.45050415e-01\n",
      "  9.52064765e-02 1.16801865e-01 4.73100624e-01 2.60916581e-02\n",
      "  9.36593437e-01 4.60458716e-01 9.32283450e-01 8.83190156e-01\n",
      "  6.57942662e-02 3.07764765e-01 1.89247303e-01 8.87248139e-01\n",
      "  6.49863906e-01 2.15842296e-01 1.20433352e-01 8.95818772e-01\n",
      "  9.55025534e-01 5.13469927e-01 2.84493714e-01 2.51496042e-01\n",
      "  6.40166087e-01 7.51370597e-01 3.00913990e-01 5.88149221e-01\n",
      "  4.87421177e-01 8.58463321e-01 1.42284389e-01 7.98696603e-01\n",
      "  2.73852257e-01 3.83840356e-01 8.75997446e-01 5.68030749e-01\n",
      "  9.93544786e-01 2.36376001e-01 1.41588180e-01 1.24074346e-01\n",
      "  5.27912833e-01 3.77269946e-01 1.40569074e-02 5.92228389e-01\n",
      "  2.26232835e-01 2.20737201e-01 9.22163450e-01 3.01007976e-01\n",
      "  8.47039345e-01 4.54764730e-01 7.00675341e-01 4.32778000e-01\n",
      "  2.82783826e-01 1.21289847e-02 9.12969080e-01 9.16887850e-02\n",
      "  4.80187338e-01 7.40667767e-01 1.07387678e-01 5.77283406e-01\n",
      "  2.73552013e-01 8.93723978e-01 8.18053249e-01 1.64505532e-01\n",
      "  3.99038903e-01 2.98321096e-01 1.90872730e-02 3.37456713e-01\n",
      "  8.73763945e-01 5.34796333e-01 1.41339441e-02 1.63437193e-01\n",
      "  7.28871920e-01 6.17558691e-01 5.02947845e-01 4.73830958e-01\n",
      "  9.99534991e-01 4.57512956e-02 1.05155040e-03 8.25582150e-01\n",
      "  3.13222951e-01 2.34281397e-02 5.74199187e-01 3.39242029e-01\n",
      "  2.72269656e-01 6.21529001e-01 7.02702339e-01 1.13286493e-01\n",
      "  4.60249894e-01 1.75228223e-01 8.24623735e-01 7.05585000e-02\n",
      "  6.04572853e-01 8.05312038e-01 9.78894565e-01 5.74006890e-01\n",
      "  5.04730796e-01 2.05055423e-01 2.04865940e-01 9.94131060e-01\n",
      "  4.67046759e-02 9.70118607e-01 9.73003510e-01 1.93459301e-01\n",
      "  9.06030458e-01 1.03416736e-01 2.73919091e-01 1.36031064e-01\n",
      "  2.00723320e-01 5.62375951e-01 6.69033933e-01 8.94448685e-01\n",
      "  1.83289742e-01 6.23318822e-01 3.17595869e-01 6.02592751e-01\n",
      "  1.42339740e-01 4.40904474e-01 1.35387191e-01 4.46419629e-01\n",
      "  8.82993438e-01 7.66578339e-01 8.33555996e-01 6.51919034e-01\n",
      "  4.96795091e-01 1.96713608e-01 4.71318961e-01 8.09277801e-01\n",
      "  9.68955727e-01 1.04468174e-01 9.82753165e-02 5.16551742e-02\n",
      "  5.56997326e-01 3.29732195e-01 4.34733125e-01 7.55831601e-01\n",
      "  8.99337983e-01 7.09526374e-01 2.51638571e-01 7.89584902e-01\n",
      "  1.51673617e-01 2.22348737e-01 2.34430858e-01 5.26802141e-01\n",
      "  7.36718658e-01 4.32954004e-01 2.72594439e-02 7.41958681e-01\n",
      "  5.81490270e-01 6.78600028e-01 3.52226845e-01 3.95199362e-01\n",
      "  2.58022068e-01 9.22853421e-01 5.25843766e-01 8.64686613e-01\n",
      "  8.70286143e-02 6.75851775e-01 1.13518220e-01 8.76541592e-01\n",
      "  8.34090434e-01 2.01395038e-01 1.08224891e-01 4.38359362e-01\n",
      "  2.80597685e-01 4.26078307e-01 7.02974101e-01 6.32362073e-01\n",
      "  3.66940506e-01 3.11666075e-01 3.82075500e-01 7.29477174e-01\n",
      "  9.69737512e-03 6.19864976e-02 3.53989432e-01 2.69964290e-01\n",
      "  9.66148892e-01 7.30327795e-01 9.43746489e-01 7.51397125e-01\n",
      "  5.49230631e-01 5.10252095e-01 6.98476004e-01 5.46678320e-01\n",
      "  4.81241617e-01 5.08643033e-01 1.50954715e-01 3.67522795e-01\n",
      "  8.24297864e-01 9.09532570e-01 9.44629327e-02 8.25107118e-01\n",
      "  2.40395626e-01 9.05177703e-01 3.45903064e-01 2.32406806e-01\n",
      "  2.50716311e-01 4.80695456e-02 9.59475660e-01 1.43133095e-01\n",
      "  9.60570088e-01 6.71630876e-01 3.57142237e-01 9.39046126e-01\n",
      "  3.73591014e-01 2.37525283e-01 1.69445255e-01 7.89373096e-01\n",
      "  1.27683243e-01 6.36393465e-01 2.12396402e-01 7.46966006e-01\n",
      "  6.92353075e-01 9.55248700e-01 1.11657538e-01 5.90112489e-01\n",
      "  7.51906690e-02 6.89096761e-01 1.23329838e-01 8.00805120e-01\n",
      "  4.69705648e-01 3.68752460e-01 5.13025477e-01 9.94910019e-01\n",
      "  9.26168049e-01 5.79563642e-01 5.40855763e-01 9.24875409e-01\n",
      "  4.96934232e-01 3.09738358e-01 9.76924270e-01 8.82780081e-01\n",
      "  8.42210569e-01 5.27666300e-02 5.77240470e-01 9.42873383e-01\n",
      "  9.52962012e-01 8.25959866e-02 9.22206601e-01 9.69195183e-02\n",
      "  6.48954942e-01 9.51238234e-01 1.90831479e-01 5.94354810e-01\n",
      "  8.19265568e-01 8.25586568e-01 9.12819349e-01 7.80936278e-01\n",
      "  1.40322821e-01 3.04265591e-01 1.01067295e-01 2.08814694e-01\n",
      "  8.76159763e-01 5.92043573e-01 6.43798343e-02 6.72082458e-02\n",
      "  8.63105937e-02 2.25003649e-01 6.52607691e-01 6.14433016e-01\n",
      "  8.17666702e-01 4.13472255e-01 5.53461493e-01 9.86158660e-01\n",
      "  9.98204573e-01 7.21734402e-01 6.89880343e-01 1.79586928e-01\n",
      "  3.67414011e-01 5.29401369e-01 9.16702027e-01 2.23358124e-01\n",
      "  1.03076814e-01 7.59484497e-01 5.64296531e-01 4.23057969e-01\n",
      "  6.15090547e-01 2.35912891e-01 8.00334710e-01 9.25105345e-01\n",
      "  8.50863591e-01 5.93768714e-01 1.46451091e-02 9.47514068e-01\n",
      "  1.23540715e-01 6.63005575e-01 5.01391012e-01 1.08921893e-01\n",
      "  1.02240353e-01 7.68050850e-01 4.78184379e-01 7.49518067e-02\n",
      "  6.96288225e-01 8.02042889e-02 4.29917637e-01 5.64905640e-01\n",
      "  3.91939491e-01 5.87259862e-01 4.26964102e-01 4.31609192e-01\n",
      "  6.36823256e-02 3.29835691e-01 1.59522553e-01 7.22779332e-01\n",
      "  1.45318618e-01 3.38388980e-01 8.64592314e-01 7.93621942e-02\n",
      "  8.82647113e-01 1.26559605e-01 4.54620427e-01 7.47072893e-01\n",
      "  8.38647871e-01 7.88239952e-01 4.87980657e-01 1.02033200e-01\n",
      "  7.53823854e-01 8.80502732e-01 5.26667044e-01 1.70831300e-01\n",
      "  4.59578276e-01 9.68490348e-01 3.58480630e-01 5.99294119e-01\n",
      "  9.17980088e-01 7.24716314e-01 1.48539460e-01 5.61827327e-01\n",
      "  8.79342534e-01 3.57898605e-01 9.23141280e-01 3.06275706e-01\n",
      "  1.99894603e-01 5.77201031e-01 5.64175977e-01 6.50675370e-01\n",
      "  8.99023078e-01 9.87215824e-01 1.40562104e-01 4.20668328e-01\n",
      "  8.65569729e-01 4.93326716e-01 8.36037666e-02 7.62233900e-01\n",
      "  8.09694142e-01 7.33150752e-01 8.53776552e-01 7.27789698e-01\n",
      "  7.21595492e-01 5.75474836e-01 1.85633768e-01 7.60940298e-01\n",
      "  8.68987555e-01 9.52525767e-01 1.98959161e-01 7.96137604e-01\n",
      "  7.70833823e-02 9.90881217e-01 1.97834897e-01 4.74441344e-02\n",
      "  4.47469499e-01 7.70385096e-01 3.76059500e-01 8.45131349e-01\n",
      "  3.03898932e-01 2.27217370e-01 7.00386958e-02 3.80354490e-01\n",
      "  7.33141499e-02 7.45898711e-01 9.68575763e-01 1.31820809e-01\n",
      "  1.20999732e-03 3.14203083e-01 4.05174375e-01 8.80298501e-01\n",
      "  6.47538626e-01 1.54625248e-01 1.79616683e-01 4.07198136e-01\n",
      "  6.54978757e-01 5.79012430e-01 2.89475800e-01 9.09076787e-01\n",
      "  9.27794665e-01 9.15113585e-01 1.25443413e-01 1.68088424e-01\n",
      "  5.24879438e-01 6.45076745e-01 5.74228796e-02 8.80102736e-01\n",
      "  4.66109647e-01 4.75793991e-01 9.24946788e-01 5.30050909e-01\n",
      "  3.95270159e-01 7.50717432e-01 2.51712046e-01 9.60987573e-01\n",
      "  6.68698208e-01 8.39520736e-01 9.94641028e-01 7.65508371e-01\n",
      "  5.10788409e-01 3.75963591e-01 4.98998646e-01 5.77272108e-02\n",
      "  7.28731963e-01 7.74327230e-01 7.94851481e-01 4.15576736e-01\n",
      "  4.42384991e-01 2.55211920e-01 6.40361876e-01 3.30228717e-01\n",
      "  1.65403122e-01 3.51085911e-01 6.43733359e-01 4.72090417e-01\n",
      "  2.44010479e-01 4.43200116e-01 5.10144025e-01 4.09462026e-01\n",
      "  2.07557714e-01 8.90965852e-01 9.23904244e-01 6.80914791e-01\n",
      "  1.56220011e-01 3.89120582e-01 7.27520582e-01 6.75597014e-01\n",
      "  8.96699263e-01 2.88809719e-01 5.00048395e-01 7.30118369e-01\n",
      "  1.90254444e-01 5.63507159e-01 2.45778490e-01 3.42288654e-01\n",
      "  4.26175177e-01 8.07078194e-01 4.93455953e-01 8.56627836e-02\n",
      "  7.99069602e-01 3.11806169e-01 7.64811958e-01 7.40974167e-01\n",
      "  4.86189571e-01 1.69101255e-01 4.74844883e-01 3.51402605e-01\n",
      "  9.34619764e-01 9.79415876e-01 3.23966427e-01 4.71715491e-01\n",
      "  6.93350676e-02 9.35392110e-01 5.14905920e-01 1.31700446e-01\n",
      "  2.70939318e-01 1.35227916e-01 9.95836037e-01 1.72703720e-01\n",
      "  4.46680293e-01 4.99667559e-01 8.38711653e-01 2.13995455e-01\n",
      "  9.03477053e-01 9.88384804e-01 8.00237578e-01 5.48059168e-01\n",
      "  5.24023391e-01 5.01592921e-01 1.10480368e-01 8.55221990e-01\n",
      "  8.68574315e-01 9.84732050e-01 7.90514259e-01 5.71468450e-01\n",
      "  2.88335542e-01 9.33099766e-01 8.21234059e-01 6.92287374e-01\n",
      "  2.08972242e-01 5.16270715e-01 7.86238804e-02 2.31582160e-01\n",
      "  9.92602353e-01 6.36646666e-01 2.47827494e-01 8.49986799e-01\n",
      "  3.60311343e-01 1.71885707e-01 1.25640151e-01 4.33521121e-01\n",
      "  3.68701650e-01 9.46484589e-01 7.36789267e-01 3.60602043e-01\n",
      "  8.07357270e-01 3.95646580e-01 4.06501731e-01 7.38388283e-01\n",
      "  8.54092577e-01 6.67344585e-01 7.23897756e-01 1.74529742e-01\n",
      "  9.23453028e-02 8.46752890e-01 3.91484898e-01 8.22161155e-01\n",
      "  2.63938339e-01 4.67598612e-01 2.50134669e-01 5.23065710e-01\n",
      "  2.45275366e-01 5.58067816e-01 1.28889881e-01 2.39339222e-02\n",
      "  3.29895785e-01 8.42649178e-01 4.12460605e-01 7.62080029e-01\n",
      "  1.83393106e-01 3.04327833e-01 5.46356133e-01 7.63156130e-01\n",
      "  8.09631360e-01 2.56457518e-01 9.98522685e-01 7.98301102e-02\n",
      "  7.79708132e-01 8.79004723e-01 6.58053052e-01 9.70738739e-01\n",
      "  1.45859749e-01 9.45153446e-01 5.79517275e-02 4.42630581e-01\n",
      "  7.67006591e-01 7.57377862e-02 1.64859690e-01 6.43934737e-01\n",
      "  5.57354168e-01 8.70986375e-01 2.54181892e-01 3.17563539e-02\n",
      "  3.30296792e-01 4.70745046e-02 8.03732170e-01 2.99278181e-01\n",
      "  7.07829362e-01 3.52635971e-01 3.39159637e-01 2.17996260e-01\n",
      "  3.35996461e-01 1.39106154e-02 7.24116068e-01 8.14954143e-01\n",
      "  2.41684639e-01 1.75034011e-01 2.96046869e-02 9.32523459e-01\n",
      "  4.65673380e-01 9.79170432e-01 8.04202286e-01 1.61068729e-01\n",
      "  3.98147574e-01 1.59933911e-01 4.77529296e-01 2.24077045e-02\n",
      "  7.50473337e-01 8.18731075e-01 8.06217543e-01 5.60815347e-01\n",
      "  9.13321126e-01 8.93210948e-01 2.04386521e-01 3.99974677e-01\n",
      "  6.82212566e-01 4.78422247e-01 2.04223183e-01 8.36655362e-01\n",
      "  7.14372881e-01 2.42749426e-01 6.69396731e-01 4.88102158e-01\n",
      "  9.52272604e-01 2.82860245e-01 9.38248370e-01 7.98683906e-01\n",
      "  8.46973707e-01 8.50872343e-01 2.86810761e-03 2.92006433e-01\n",
      "  7.72449209e-01 7.34960543e-01 4.99527781e-01 9.60726009e-01\n",
      "  2.69298235e-01 7.67431186e-02 8.13094617e-01 9.14851475e-01\n",
      "  8.52981664e-01 1.87463402e-01 9.12959336e-01 8.93471188e-01\n",
      "  1.53605403e-01 8.52538797e-01 4.12792232e-01 6.47762589e-01\n",
      "  4.37670384e-01 6.53467229e-01 8.43433148e-01 2.65408925e-01\n",
      "  2.97434923e-01 7.01277511e-01 6.26220219e-01 1.01228498e-01\n",
      "  2.28364072e-01 5.24533363e-01 6.52708350e-01 7.56428854e-01\n",
      "  6.24632093e-01 4.88483556e-01 7.96430170e-01 9.54922305e-01\n",
      "  3.26722978e-01 2.17758524e-01 8.58662966e-01 2.18626656e-01\n",
      "  9.66931666e-01 8.53118629e-01 3.38971433e-01 7.39866786e-01\n",
      "  9.03362234e-01 7.61736595e-02 9.34551552e-01 5.81319779e-02\n",
      "  5.65205532e-01 5.48722130e-01 2.58713466e-01 4.96654594e-01\n",
      "  9.55195794e-01 6.37777154e-01 4.51728792e-01 8.25723362e-01\n",
      "  6.62392895e-01 7.54713997e-01 8.38831345e-01 4.09214886e-01\n",
      "  6.80231628e-01 2.08247466e-01 7.07914057e-02 2.82839999e-01\n",
      "  3.36614384e-01 5.06704993e-01 2.40691738e-01 9.35621092e-01\n",
      "  3.39889160e-02 4.14598078e-01 6.14997309e-01 5.42619355e-01\n",
      "  7.55464956e-01 2.23763268e-01 4.93282003e-01 9.54501885e-01\n",
      "  8.64494965e-02 6.86299426e-01 7.55478009e-01 1.94840687e-01\n",
      "  2.51699075e-01 2.73440243e-01 4.60032822e-01 3.83431356e-01\n",
      "  5.09570563e-01 6.18546577e-01 8.51102308e-01 2.35312761e-01\n",
      "  4.01876925e-01 6.53872385e-01 6.34531547e-01]]\n",
      "Change in theta: 1.8011343231957147e-06\n",
      "---------------------------------------------\n",
      "Iteration number 21 finished, Running time 524.0762338638306, Roo2 iteration 21 = -7171444.502799218\n",
      "Iteration 1:\n",
      "Theta: [[6.82573713e-01 2.04521951e-01 3.44627534e-01 5.75117399e-01\n",
      "  1.67799129e-01 3.75648696e-01 1.16356078e-01 8.83588116e-01\n",
      "  5.22042676e-01 3.47341477e-01 8.63886351e-01 1.62103140e-01\n",
      "  1.26222626e-01 5.95406538e-01 9.41483315e-01 7.64165614e-01\n",
      "  5.77495650e-01 7.73455218e-01 1.63612964e-01 7.90843239e-01\n",
      "  9.35680164e-02 2.68797366e-01 3.84678158e-01 4.97087415e-01\n",
      "  4.94990382e-01 6.36575645e-01 3.65358500e-01 2.17183854e-01\n",
      "  4.22690072e-01 8.48221264e-01 8.36635643e-01 9.86256647e-01\n",
      "  4.69589115e-01 5.17768376e-01 1.62594377e-02 8.68938458e-01\n",
      "  6.89868047e-01 6.26074801e-01 4.87929551e-01 5.22123736e-01\n",
      "  4.13604892e-01 4.41756023e-01 7.98437075e-01 4.11090147e-01\n",
      "  7.66485370e-01 2.83450274e-01 6.01823456e-01 7.80440242e-02\n",
      "  9.99853396e-01 2.20901137e-01 7.66519547e-01 6.18848162e-01\n",
      "  6.52121017e-01 9.73279527e-01 3.28905679e-01 2.16786204e-02\n",
      "  3.20346023e-01 1.12521877e-01 5.44491217e-02 9.88055180e-01\n",
      "  6.05805682e-02 8.64991035e-01 4.53322043e-02 4.81477194e-01\n",
      "  7.91209167e-01 3.88684049e-01 4.32179932e-01 7.85714383e-02\n",
      "  1.92392381e-01 9.14165776e-01 7.07726272e-01 5.39087353e-02\n",
      "  8.76093960e-01 5.85050540e-01 5.29699083e-01 9.12786159e-01\n",
      "  7.89523504e-01 6.30189116e-01 9.55837164e-01 4.74089538e-01\n",
      "  4.29751320e-01 9.38228174e-01 6.08039601e-01 7.48822400e-01\n",
      "  5.09963498e-01 8.82828966e-01 9.07735436e-01 5.37809764e-01\n",
      "  5.87247873e-01 1.60272548e-01 8.63051535e-01 9.99094864e-02\n",
      "  9.15924490e-01 1.18104208e-01 5.79017455e-01 3.95717799e-01\n",
      "  2.48620377e-01 9.28311435e-01 2.19443939e-01 6.00836982e-02\n",
      "  9.26522478e-01 7.02471032e-01 4.80495960e-01 4.96209439e-01\n",
      "  6.64326896e-03 1.45517342e-01 4.15118311e-01 3.65153149e-02\n",
      "  7.29440258e-01 5.36881975e-01 3.39502940e-01 6.57799841e-01\n",
      "  1.25028884e-01 4.34490404e-01 8.53916211e-02 9.71365102e-01\n",
      "  2.27626159e-01 7.88512050e-01 8.18931107e-01 6.22107513e-01\n",
      "  9.20611733e-01 3.11788514e-01 1.56011229e-01 9.24312743e-01\n",
      "  5.82787261e-01 9.49797298e-01 3.92987898e-02 7.11920460e-01\n",
      "  5.12348975e-01 7.59408951e-01 8.43598901e-01 1.22673881e-01\n",
      "  4.76998048e-01 4.99926812e-01 3.46120539e-01 6.76129311e-01\n",
      "  1.99226183e-01 3.46425256e-01 7.67289505e-01 6.21676911e-02\n",
      "  4.65667167e-01 1.35889345e-01 2.75121888e-02 8.14447442e-01\n",
      "  5.12287114e-01 2.89724662e-01 2.26165645e-01 8.59432714e-01\n",
      "  8.31772810e-01 7.21032386e-01 2.36537951e-01 3.34413775e-01\n",
      "  5.19066787e-03 2.74526495e-01 9.88377490e-02 8.88445325e-01\n",
      "  6.15308616e-01 3.40356885e-01 8.85032367e-01 4.82985367e-01\n",
      "  6.52141672e-01 5.35471923e-01 8.15435471e-01 4.69733223e-01\n",
      "  9.84673760e-01 4.41129855e-02 8.85726966e-01 6.94570649e-01\n",
      "  8.33650507e-01 4.62205803e-01 6.11427043e-01 2.26862624e-01\n",
      "  7.93884116e-01 2.29916628e-01 4.08261671e-01 4.88009176e-01\n",
      "  3.36783080e-01 3.71373853e-01 6.59303767e-03 3.56353002e-01\n",
      "  7.18395576e-01 1.38385004e-01 9.48267937e-01 1.24279181e-01\n",
      "  8.18098403e-01 7.58732070e-01 5.16490328e-01 3.77494451e-01\n",
      "  1.32116049e-01 8.55642397e-01 9.48061194e-01 2.25140080e-01\n",
      "  5.16199927e-01 7.49110178e-01 6.40110205e-01 8.66497288e-01\n",
      "  1.47637696e-01 7.42607842e-01 6.45742671e-01 7.58024278e-01\n",
      "  8.00797611e-01 2.21041333e-01 6.04788195e-03 6.27650909e-01\n",
      "  2.51517777e-01 7.70841694e-01 1.97896751e-01 9.04973314e-01\n",
      "  8.38361046e-01 4.17780558e-01 7.56717367e-01 5.77382168e-01\n",
      "  9.48721462e-01 6.54359273e-01 5.91208102e-01 9.88764395e-01\n",
      "  5.36361831e-01 9.00714694e-01 1.28367931e-01 3.92141476e-01\n",
      "  9.80301315e-01 1.78505716e-01 5.28771901e-01 2.79125051e-01\n",
      "  8.16580640e-04 4.24810578e-01 3.87222540e-01 8.86488091e-01\n",
      "  3.16549888e-01 2.40262890e-01 1.78152289e-01 5.45050793e-01\n",
      "  9.52068546e-02 1.16802243e-01 4.73101002e-01 2.60920363e-02\n",
      "  9.36593815e-01 4.60459094e-01 9.32283828e-01 8.83190534e-01\n",
      "  6.57946444e-02 3.07765144e-01 1.89247681e-01 8.87248517e-01\n",
      "  6.49864284e-01 2.15842674e-01 1.20433730e-01 8.95819150e-01\n",
      "  9.55025913e-01 5.13470305e-01 2.84494092e-01 2.51496420e-01\n",
      "  6.40166465e-01 7.51370975e-01 3.00914369e-01 5.88149599e-01\n",
      "  4.87421555e-01 8.58463699e-01 1.42284767e-01 7.98696981e-01\n",
      "  2.73852635e-01 3.83840734e-01 8.75997824e-01 5.68031127e-01\n",
      "  9.93545164e-01 2.36376379e-01 1.41588558e-01 1.24074724e-01\n",
      "  5.27913211e-01 3.77270324e-01 1.40572856e-02 5.92228767e-01\n",
      "  2.26233213e-01 2.20737580e-01 9.22163829e-01 3.01008355e-01\n",
      "  8.47039723e-01 4.54765108e-01 7.00675719e-01 4.32778378e-01\n",
      "  2.82784204e-01 1.21293629e-02 9.12969458e-01 9.16891632e-02\n",
      "  4.80187716e-01 7.40668145e-01 1.07388056e-01 5.77283784e-01\n",
      "  2.73552391e-01 8.93724356e-01 8.18053627e-01 1.64505910e-01\n",
      "  3.99039281e-01 2.98321474e-01 1.90876511e-02 3.37457091e-01\n",
      "  8.73764323e-01 5.34796711e-01 1.41343223e-02 1.63437571e-01\n",
      "  7.28872298e-01 6.17559069e-01 5.02948223e-01 4.73831336e-01\n",
      "  9.99535370e-01 4.57516738e-02 1.05192858e-03 8.25582528e-01\n",
      "  3.13223329e-01 2.34285179e-02 5.74199565e-01 3.39242407e-01\n",
      "  2.72270034e-01 6.21529380e-01 7.02702717e-01 1.13286871e-01\n",
      "  4.60250272e-01 1.75228602e-01 8.24624114e-01 7.05588782e-02\n",
      "  6.04573231e-01 8.05312416e-01 9.78894943e-01 5.74007268e-01\n",
      "  5.04731174e-01 2.05055801e-01 2.04866318e-01 9.94131438e-01\n",
      "  4.67050541e-02 9.70118985e-01 9.73003888e-01 1.93459679e-01\n",
      "  9.06030836e-01 1.03417114e-01 2.73919469e-01 1.36031442e-01\n",
      "  2.00723698e-01 5.62376329e-01 6.69034312e-01 8.94449063e-01\n",
      "  1.83290120e-01 6.23319200e-01 3.17596248e-01 6.02593130e-01\n",
      "  1.42340118e-01 4.40904852e-01 1.35387570e-01 4.46420007e-01\n",
      "  8.82993816e-01 7.66578717e-01 8.33556374e-01 6.51919413e-01\n",
      "  4.96795469e-01 1.96713987e-01 4.71319339e-01 8.09278180e-01\n",
      "  9.68956105e-01 1.04468552e-01 9.82756947e-02 5.16555523e-02\n",
      "  5.56997704e-01 3.29732573e-01 4.34733503e-01 7.55831979e-01\n",
      "  8.99338361e-01 7.09526752e-01 2.51638949e-01 7.89585280e-01\n",
      "  1.51673995e-01 2.22349115e-01 2.34431236e-01 5.26802519e-01\n",
      "  7.36719036e-01 4.32954382e-01 2.72598221e-02 7.41959060e-01\n",
      "  5.81490648e-01 6.78600406e-01 3.52227223e-01 3.95199740e-01\n",
      "  2.58022446e-01 9.22853799e-01 5.25844144e-01 8.64686991e-01\n",
      "  8.70289925e-02 6.75852153e-01 1.13518598e-01 8.76541970e-01\n",
      "  8.34090813e-01 2.01395416e-01 1.08225269e-01 4.38359741e-01\n",
      "  2.80598064e-01 4.26078685e-01 7.02974479e-01 6.32362451e-01\n",
      "  3.66940884e-01 3.11666453e-01 3.82075879e-01 7.29477552e-01\n",
      "  9.69775331e-03 6.19868758e-02 3.53989810e-01 2.69964668e-01\n",
      "  9.66149270e-01 7.30328173e-01 9.43746868e-01 7.51397504e-01\n",
      "  5.49231009e-01 5.10252473e-01 6.98476382e-01 5.46678699e-01\n",
      "  4.81241995e-01 5.08643411e-01 1.50955094e-01 3.67523173e-01\n",
      "  8.24298242e-01 9.09532949e-01 9.44633109e-02 8.25107496e-01\n",
      "  2.40396004e-01 9.05178081e-01 3.45903442e-01 2.32407184e-01\n",
      "  2.50716690e-01 4.80699238e-02 9.59476038e-01 1.43133473e-01\n",
      "  9.60570466e-01 6.71631254e-01 3.57142615e-01 9.39046504e-01\n",
      "  3.73591392e-01 2.37525661e-01 1.69445633e-01 7.89373474e-01\n",
      "  1.27683621e-01 6.36393843e-01 2.12396780e-01 7.46966384e-01\n",
      "  6.92353453e-01 9.55249079e-01 1.11657916e-01 5.90112868e-01\n",
      "  7.51910472e-02 6.89097139e-01 1.23330216e-01 8.00805498e-01\n",
      "  4.69706026e-01 3.68752838e-01 5.13025855e-01 9.94910397e-01\n",
      "  9.26168428e-01 5.79564020e-01 5.40856142e-01 9.24875787e-01\n",
      "  4.96934610e-01 3.09738737e-01 9.76924649e-01 8.82780459e-01\n",
      "  8.42210947e-01 5.27670082e-02 5.77240848e-01 9.42873761e-01\n",
      "  9.52962390e-01 8.25963648e-02 9.22206979e-01 9.69198964e-02\n",
      "  6.48955320e-01 9.51238612e-01 1.90831858e-01 5.94355188e-01\n",
      "  8.19265946e-01 8.25586946e-01 9.12819727e-01 7.80936656e-01\n",
      "  1.40323200e-01 3.04265969e-01 1.01067673e-01 2.08815072e-01\n",
      "  8.76160142e-01 5.92043951e-01 6.43802125e-02 6.72086240e-02\n",
      "  8.63109719e-02 2.25004027e-01 6.52608069e-01 6.14433394e-01\n",
      "  8.17667081e-01 4.13472633e-01 5.53461871e-01 9.86159038e-01\n",
      "  9.98204951e-01 7.21734780e-01 6.89880721e-01 1.79587306e-01\n",
      "  3.67414389e-01 5.29401747e-01 9.16702405e-01 2.23358502e-01\n",
      "  1.03077193e-01 7.59484875e-01 5.64296909e-01 4.23058347e-01\n",
      "  6.15090925e-01 2.35913269e-01 8.00335088e-01 9.25105724e-01\n",
      "  8.50863969e-01 5.93769092e-01 1.46454873e-02 9.47514446e-01\n",
      "  1.23541093e-01 6.63005953e-01 5.01391391e-01 1.08922272e-01\n",
      "  1.02240731e-01 7.68051228e-01 4.78184757e-01 7.49521849e-02\n",
      "  6.96288603e-01 8.02046671e-02 4.29918015e-01 5.64906019e-01\n",
      "  3.91939869e-01 5.87260240e-01 4.26964480e-01 4.31609571e-01\n",
      "  6.36827038e-02 3.29836069e-01 1.59522931e-01 7.22779710e-01\n",
      "  1.45318996e-01 3.38389358e-01 8.64592692e-01 7.93625723e-02\n",
      "  8.82647491e-01 1.26559983e-01 4.54620805e-01 7.47073271e-01\n",
      "  8.38648249e-01 7.88240330e-01 4.87981035e-01 1.02033578e-01\n",
      "  7.53824232e-01 8.80503110e-01 5.26667422e-01 1.70831678e-01\n",
      "  4.59578654e-01 9.68490727e-01 3.58481008e-01 5.99294497e-01\n",
      "  9.17980467e-01 7.24716692e-01 1.48539838e-01 5.61827705e-01\n",
      "  8.79342913e-01 3.57898983e-01 9.23141658e-01 3.06276085e-01\n",
      "  1.99894982e-01 5.77201409e-01 5.64176355e-01 6.50675748e-01\n",
      "  8.99023456e-01 9.87216202e-01 1.40562482e-01 4.20668706e-01\n",
      "  8.65570107e-01 4.93327094e-01 8.36041448e-02 7.62234279e-01\n",
      "  8.09694520e-01 7.33151131e-01 8.53776930e-01 7.27790076e-01\n",
      "  7.21595870e-01 5.75475214e-01 1.85634146e-01 7.60940676e-01\n",
      "  8.68987934e-01 9.52526145e-01 1.98959539e-01 7.96137983e-01\n",
      "  7.70837605e-02 9.90881595e-01 1.97835275e-01 4.74445126e-02\n",
      "  4.47469877e-01 7.70385474e-01 3.76059878e-01 8.45131727e-01\n",
      "  3.03899310e-01 2.27217749e-01 7.00390739e-02 3.80354868e-01\n",
      "  7.33145281e-02 7.45899089e-01 9.68576141e-01 1.31821187e-01\n",
      "  1.21037551e-03 3.14203462e-01 4.05174753e-01 8.80298879e-01\n",
      "  6.47539004e-01 1.54625626e-01 1.79617061e-01 4.07198514e-01\n",
      "  6.54979136e-01 5.79012808e-01 2.89476178e-01 9.09077165e-01\n",
      "  9.27795043e-01 9.15113963e-01 1.25443791e-01 1.68088802e-01\n",
      "  5.24879816e-01 6.45077123e-01 5.74232578e-02 8.80103114e-01\n",
      "  4.66110025e-01 4.75794370e-01 9.24947166e-01 5.30051287e-01\n",
      "  3.95270538e-01 7.50717810e-01 2.51712424e-01 9.60987951e-01\n",
      "  6.68698586e-01 8.39521114e-01 9.94641406e-01 7.65508749e-01\n",
      "  5.10788787e-01 3.75963969e-01 4.98999024e-01 5.77275890e-02\n",
      "  7.28732341e-01 7.74327608e-01 7.94851859e-01 4.15577114e-01\n",
      "  4.42385369e-01 2.55212299e-01 6.40362254e-01 3.30229096e-01\n",
      "  1.65403500e-01 3.51086289e-01 6.43733738e-01 4.72090795e-01\n",
      "  2.44010857e-01 4.43200495e-01 5.10144403e-01 4.09462404e-01\n",
      "  2.07558092e-01 8.90966230e-01 9.23904622e-01 6.80915169e-01\n",
      "  1.56220389e-01 3.89120960e-01 7.27520960e-01 6.75597393e-01\n",
      "  8.96699642e-01 2.88810097e-01 5.00048774e-01 7.30118748e-01\n",
      "  1.90254822e-01 5.63507537e-01 2.45778868e-01 3.42289032e-01\n",
      "  4.26175555e-01 8.07078572e-01 4.93456331e-01 8.56631618e-02\n",
      "  7.99069981e-01 3.11806547e-01 7.64812336e-01 7.40974545e-01\n",
      "  4.86189950e-01 1.69101634e-01 4.74845261e-01 3.51402984e-01\n",
      "  9.34620142e-01 9.79416254e-01 3.23966805e-01 4.71715870e-01\n",
      "  6.93354458e-02 9.35392488e-01 5.14906298e-01 1.31700824e-01\n",
      "  2.70939696e-01 1.35228294e-01 9.95836415e-01 1.72704098e-01\n",
      "  4.46680671e-01 4.99667937e-01 8.38712032e-01 2.13995833e-01\n",
      "  9.03477431e-01 9.88385183e-01 8.00237956e-01 5.48059546e-01\n",
      "  5.24023769e-01 5.01593300e-01 1.10480747e-01 8.55222368e-01\n",
      "  8.68574693e-01 9.84732428e-01 7.90514638e-01 5.71468829e-01\n",
      "  2.88335921e-01 9.33100144e-01 8.21234437e-01 6.92287752e-01\n",
      "  2.08972621e-01 5.16271093e-01 7.86242586e-02 2.31582538e-01\n",
      "  9.92602731e-01 6.36647044e-01 2.47827872e-01 8.49987177e-01\n",
      "  3.60311722e-01 1.71886085e-01 1.25640529e-01 4.33521500e-01\n",
      "  3.68702028e-01 9.46484968e-01 7.36789645e-01 3.60602421e-01\n",
      "  8.07357649e-01 3.95646958e-01 4.06502109e-01 7.38388661e-01\n",
      "  8.54092956e-01 6.67344963e-01 7.23898134e-01 1.74530120e-01\n",
      "  9.23456810e-02 8.46753268e-01 3.91485276e-01 8.22161533e-01\n",
      "  2.63938717e-01 4.67598991e-01 2.50135047e-01 5.23066088e-01\n",
      "  2.45275744e-01 5.58068195e-01 1.28890259e-01 2.39343004e-02\n",
      "  3.29896163e-01 8.42649556e-01 4.12460983e-01 7.62080407e-01\n",
      "  1.83393484e-01 3.04328211e-01 5.46356511e-01 7.63156508e-01\n",
      "  8.09631739e-01 2.56457896e-01 9.98523064e-01 7.98304884e-02\n",
      "  7.79708510e-01 8.79005101e-01 6.58053430e-01 9.70739117e-01\n",
      "  1.45860128e-01 9.45153824e-01 5.79521056e-02 4.42630959e-01\n",
      "  7.67006969e-01 7.57381644e-02 1.64860068e-01 6.43935115e-01\n",
      "  5.57354546e-01 8.70986753e-01 2.54182270e-01 3.17567321e-02\n",
      "  3.30297170e-01 4.70748828e-02 8.03732548e-01 2.99278559e-01\n",
      "  7.07829740e-01 3.52636350e-01 3.39160015e-01 2.17996638e-01\n",
      "  3.35996839e-01 1.39109936e-02 7.24116446e-01 8.14954521e-01\n",
      "  2.41685017e-01 1.75034389e-01 2.96050651e-02 9.32523837e-01\n",
      "  4.65673758e-01 9.79170810e-01 8.04202664e-01 1.61069107e-01\n",
      "  3.98147952e-01 1.59934289e-01 4.77529674e-01 2.24080827e-02\n",
      "  7.50473716e-01 8.18731453e-01 8.06217921e-01 5.60815725e-01\n",
      "  9.13321504e-01 8.93211326e-01 2.04386900e-01 3.99975055e-01\n",
      "  6.82212944e-01 4.78422625e-01 2.04223561e-01 8.36655740e-01\n",
      "  7.14373259e-01 2.42749804e-01 6.69397110e-01 4.88102537e-01\n",
      "  9.52272982e-01 2.82860623e-01 9.38248748e-01 7.98684285e-01\n",
      "  8.46974085e-01 8.50872721e-01 2.86848580e-03 2.92006811e-01\n",
      "  7.72449587e-01 7.34960921e-01 4.99528159e-01 9.60726387e-01\n",
      "  2.69298613e-01 7.67434968e-02 8.13094995e-01 9.14851853e-01\n",
      "  8.52982042e-01 1.87463781e-01 9.12959714e-01 8.93471566e-01\n",
      "  1.53605781e-01 8.52539176e-01 4.12792610e-01 6.47762967e-01\n",
      "  4.37670762e-01 6.53467607e-01 8.43433526e-01 2.65409304e-01\n",
      "  2.97435301e-01 7.01277890e-01 6.26220597e-01 1.01228876e-01\n",
      "  2.28364450e-01 5.24533741e-01 6.52708728e-01 7.56429232e-01\n",
      "  6.24632472e-01 4.88483934e-01 7.96430548e-01 9.54922683e-01\n",
      "  3.26723356e-01 2.17758902e-01 8.58663344e-01 2.18627034e-01\n",
      "  9.66932044e-01 8.53119008e-01 3.38971811e-01 7.39867164e-01\n",
      "  9.03362612e-01 7.61740377e-02 9.34551930e-01 5.81323561e-02\n",
      "  5.65205910e-01 5.48722508e-01 2.58713844e-01 4.96654972e-01\n",
      "  9.55196172e-01 6.37777532e-01 4.51729170e-01 8.25723741e-01\n",
      "  6.62393273e-01 7.54714375e-01 8.38831723e-01 4.09215264e-01\n",
      "  6.80232006e-01 2.08247844e-01 7.07917839e-02 2.82840377e-01\n",
      "  3.36614763e-01 5.06705371e-01 2.40692116e-01 9.35621470e-01\n",
      "  3.39892942e-02 4.14598456e-01 6.14997687e-01 5.42619733e-01\n",
      "  7.55465334e-01 2.23763646e-01 4.93282381e-01 9.54502263e-01\n",
      "  8.64498747e-02 6.86299804e-01 7.55478387e-01 1.94841066e-01\n",
      "  2.51699454e-01 2.73440621e-01 4.60033201e-01 3.83431735e-01\n",
      "  5.09570941e-01 6.18546955e-01 8.51102686e-01 2.35313139e-01\n",
      "  4.01877303e-01 6.53872763e-01 6.34531926e-01]]\n",
      "Change in theta: 1.8319032013249056e-06\n",
      "---------------------------------------------\n",
      "Iteration number 22 finished, Running time 516.1294620037079, Roo2 iteration 22 = -2245554.783374777\n",
      "Iteration 1:\n",
      "Theta: [[6.82574058e-01 2.04522295e-01 3.44627878e-01 5.75117743e-01\n",
      "  1.67799474e-01 3.75649040e-01 1.16356422e-01 8.83588460e-01\n",
      "  5.22043020e-01 3.47341821e-01 8.63886696e-01 1.62103485e-01\n",
      "  1.26222971e-01 5.95406882e-01 9.41483660e-01 7.64165958e-01\n",
      "  5.77495995e-01 7.73455562e-01 1.63613309e-01 7.90843583e-01\n",
      "  9.35683607e-02 2.68797710e-01 3.84678503e-01 4.97087759e-01\n",
      "  4.94990727e-01 6.36575989e-01 3.65358844e-01 2.17184198e-01\n",
      "  4.22690417e-01 8.48221608e-01 8.36635987e-01 9.86256992e-01\n",
      "  4.69589459e-01 5.17768721e-01 1.62597820e-02 8.68938802e-01\n",
      "  6.89868391e-01 6.26075146e-01 4.87929896e-01 5.22124080e-01\n",
      "  4.13605236e-01 4.41756368e-01 7.98437419e-01 4.11090491e-01\n",
      "  7.66485714e-01 2.83450619e-01 6.01823800e-01 7.80443686e-02\n",
      "  9.99853741e-01 2.20901482e-01 7.66519891e-01 6.18848507e-01\n",
      "  6.52121361e-01 9.73279871e-01 3.28906023e-01 2.16789648e-02\n",
      "  3.20346368e-01 1.12522221e-01 5.44494660e-02 9.88055524e-01\n",
      "  6.05809125e-02 8.64991379e-01 4.53325486e-02 4.81477538e-01\n",
      "  7.91209511e-01 3.88684393e-01 4.32180277e-01 7.85717827e-02\n",
      "  1.92392725e-01 9.14166120e-01 7.07726616e-01 5.39090796e-02\n",
      "  8.76094304e-01 5.85050884e-01 5.29699428e-01 9.12786504e-01\n",
      "  7.89523849e-01 6.30189461e-01 9.55837508e-01 4.74089883e-01\n",
      "  4.29751664e-01 9.38228519e-01 6.08039946e-01 7.48822745e-01\n",
      "  5.09963843e-01 8.82829310e-01 9.07735780e-01 5.37810109e-01\n",
      "  5.87248218e-01 1.60272892e-01 8.63051880e-01 9.99098307e-02\n",
      "  9.15924834e-01 1.18104552e-01 5.79017800e-01 3.95718143e-01\n",
      "  2.48620721e-01 9.28311779e-01 2.19444283e-01 6.00840425e-02\n",
      "  9.26522822e-01 7.02471376e-01 4.80496304e-01 4.96209783e-01\n",
      "  6.64361329e-03 1.45517686e-01 4.15118655e-01 3.65156592e-02\n",
      "  7.29440602e-01 5.36882319e-01 3.39503285e-01 6.57800185e-01\n",
      "  1.25029228e-01 4.34490748e-01 8.53919654e-02 9.71365446e-01\n",
      "  2.27626503e-01 7.88512394e-01 8.18931452e-01 6.22107857e-01\n",
      "  9.20612078e-01 3.11788858e-01 1.56011573e-01 9.24313087e-01\n",
      "  5.82787605e-01 9.49797642e-01 3.92991341e-02 7.11920805e-01\n",
      "  5.12349319e-01 7.59409295e-01 8.43599246e-01 1.22674225e-01\n",
      "  4.76998392e-01 4.99927156e-01 3.46120883e-01 6.76129655e-01\n",
      "  1.99226527e-01 3.46425600e-01 7.67289849e-01 6.21680354e-02\n",
      "  4.65667511e-01 1.35889689e-01 2.75125331e-02 8.14447787e-01\n",
      "  5.12287459e-01 2.89725006e-01 2.26165989e-01 8.59433058e-01\n",
      "  8.31773155e-01 7.21032730e-01 2.36538295e-01 3.34414120e-01\n",
      "  5.19101220e-03 2.74526839e-01 9.88380933e-02 8.88445669e-01\n",
      "  6.15308960e-01 3.40357229e-01 8.85032712e-01 4.82985711e-01\n",
      "  6.52142017e-01 5.35472268e-01 8.15435816e-01 4.69733567e-01\n",
      "  9.84674104e-01 4.41133298e-02 8.85727311e-01 6.94570994e-01\n",
      "  8.33650851e-01 4.62206147e-01 6.11427388e-01 2.26862968e-01\n",
      "  7.93884461e-01 2.29916972e-01 4.08262016e-01 4.88009520e-01\n",
      "  3.36783425e-01 3.71374198e-01 6.59338200e-03 3.56353346e-01\n",
      "  7.18395920e-01 1.38385349e-01 9.48268282e-01 1.24279525e-01\n",
      "  8.18098747e-01 7.58732414e-01 5.16490672e-01 3.77494795e-01\n",
      "  1.32116394e-01 8.55642741e-01 9.48061538e-01 2.25140425e-01\n",
      "  5.16200271e-01 7.49110522e-01 6.40110550e-01 8.66497632e-01\n",
      "  1.47638040e-01 7.42608186e-01 6.45743015e-01 7.58024623e-01\n",
      "  8.00797955e-01 2.21041677e-01 6.04822628e-03 6.27651253e-01\n",
      "  2.51518121e-01 7.70842039e-01 1.97897095e-01 9.04973659e-01\n",
      "  8.38361390e-01 4.17780903e-01 7.56717711e-01 5.77382512e-01\n",
      "  9.48721806e-01 6.54359617e-01 5.91208446e-01 9.88764740e-01\n",
      "  5.36362176e-01 9.00715038e-01 1.28368276e-01 3.92141820e-01\n",
      "  9.80301660e-01 1.78506060e-01 5.28772246e-01 2.79125395e-01\n",
      "  8.16924967e-04 4.24810922e-01 3.87222885e-01 8.86488436e-01\n",
      "  3.16550232e-01 2.40263234e-01 1.78152633e-01 5.45051138e-01\n",
      "  9.52071990e-02 1.16802587e-01 4.73101347e-01 2.60923806e-02\n",
      "  9.36594159e-01 4.60459438e-01 9.32284172e-01 8.83190878e-01\n",
      "  6.57949888e-02 3.07765488e-01 1.89248025e-01 8.87248862e-01\n",
      "  6.49864629e-01 2.15843019e-01 1.20434074e-01 8.95819494e-01\n",
      "  9.55026257e-01 5.13470649e-01 2.84494436e-01 2.51496765e-01\n",
      "  6.40166810e-01 7.51371320e-01 3.00914713e-01 5.88149943e-01\n",
      "  4.87421899e-01 8.58464043e-01 1.42285112e-01 7.98697325e-01\n",
      "  2.73852980e-01 3.83841079e-01 8.75998169e-01 5.68031471e-01\n",
      "  9.93545508e-01 2.36376724e-01 1.41588902e-01 1.24075068e-01\n",
      "  5.27913555e-01 3.77270669e-01 1.40576299e-02 5.92229112e-01\n",
      "  2.26233557e-01 2.20737924e-01 9.22164173e-01 3.01008699e-01\n",
      "  8.47040067e-01 4.54765453e-01 7.00676064e-01 4.32778722e-01\n",
      "  2.82784548e-01 1.21297072e-02 9.12969802e-01 9.16895075e-02\n",
      "  4.80188060e-01 7.40668490e-01 1.07388401e-01 5.77284128e-01\n",
      "  2.73552736e-01 8.93724701e-01 8.18053971e-01 1.64506254e-01\n",
      "  3.99039626e-01 2.98321818e-01 1.90879955e-02 3.37457436e-01\n",
      "  8.73764667e-01 5.34797056e-01 1.41346666e-02 1.63437916e-01\n",
      "  7.28872642e-01 6.17559413e-01 5.02948567e-01 4.73831681e-01\n",
      "  9.99535714e-01 4.57520181e-02 1.05227291e-03 8.25582873e-01\n",
      "  3.13223673e-01 2.34288622e-02 5.74199909e-01 3.39242751e-01\n",
      "  2.72270378e-01 6.21529724e-01 7.02703061e-01 1.13287215e-01\n",
      "  4.60250616e-01 1.75228946e-01 8.24624458e-01 7.05592226e-02\n",
      "  6.04573575e-01 8.05312761e-01 9.78895287e-01 5.74007612e-01\n",
      "  5.04731518e-01 2.05056146e-01 2.04866662e-01 9.94131782e-01\n",
      "  4.67053984e-02 9.70119330e-01 9.73004233e-01 1.93460024e-01\n",
      "  9.06031181e-01 1.03417458e-01 2.73919813e-01 1.36031786e-01\n",
      "  2.00724042e-01 5.62376674e-01 6.69034656e-01 8.94449408e-01\n",
      "  1.83290464e-01 6.23319544e-01 3.17596592e-01 6.02593474e-01\n",
      "  1.42340463e-01 4.40905196e-01 1.35387914e-01 4.46420352e-01\n",
      "  8.82994161e-01 7.66579062e-01 8.33556719e-01 6.51919757e-01\n",
      "  4.96795813e-01 1.96714331e-01 4.71319683e-01 8.09278524e-01\n",
      "  9.68956450e-01 1.04468896e-01 9.82760390e-02 5.16558967e-02\n",
      "  5.56998049e-01 3.29732917e-01 4.34733848e-01 7.55832323e-01\n",
      "  8.99338705e-01 7.09527096e-01 2.51639294e-01 7.89585624e-01\n",
      "  1.51674339e-01 2.22349460e-01 2.34431580e-01 5.26802863e-01\n",
      "  7.36719381e-01 4.32954726e-01 2.72601664e-02 7.41959404e-01\n",
      "  5.81490992e-01 6.78600751e-01 3.52227567e-01 3.95200085e-01\n",
      "  2.58022791e-01 9.22854143e-01 5.25844489e-01 8.64687335e-01\n",
      "  8.70293368e-02 6.75852498e-01 1.13518943e-01 8.76542314e-01\n",
      "  8.34091157e-01 2.01395761e-01 1.08225613e-01 4.38360085e-01\n",
      "  2.80598408e-01 4.26079029e-01 7.02974824e-01 6.32362795e-01\n",
      "  3.66941229e-01 3.11666797e-01 3.82076223e-01 7.29477897e-01\n",
      "  9.69809764e-03 6.19872202e-02 3.53990154e-01 2.69965012e-01\n",
      "  9.66149614e-01 7.30328518e-01 9.43747212e-01 7.51397848e-01\n",
      "  5.49231353e-01 5.10252818e-01 6.98476727e-01 5.46679043e-01\n",
      "  4.81242339e-01 5.08643755e-01 1.50955438e-01 3.67523518e-01\n",
      "  8.24298586e-01 9.09533293e-01 9.44636553e-02 8.25107840e-01\n",
      "  2.40396348e-01 9.05178426e-01 3.45903786e-01 2.32407528e-01\n",
      "  2.50717034e-01 4.80702681e-02 9.59476382e-01 1.43133817e-01\n",
      "  9.60570810e-01 6.71631598e-01 3.57142959e-01 9.39046848e-01\n",
      "  3.73591737e-01 2.37526005e-01 1.69445978e-01 7.89373819e-01\n",
      "  1.27683966e-01 6.36394188e-01 2.12397124e-01 7.46966728e-01\n",
      "  6.92353797e-01 9.55249423e-01 1.11658260e-01 5.90113212e-01\n",
      "  7.51913915e-02 6.89097483e-01 1.23330560e-01 8.00805843e-01\n",
      "  4.69706371e-01 3.68753182e-01 5.13026199e-01 9.94910741e-01\n",
      "  9.26168772e-01 5.79564364e-01 5.40856486e-01 9.24876132e-01\n",
      "  4.96934955e-01 3.09739081e-01 9.76924993e-01 8.82780803e-01\n",
      "  8.42211291e-01 5.27673526e-02 5.77241193e-01 9.42874105e-01\n",
      "  9.52962735e-01 8.25967091e-02 9.22207324e-01 9.69202408e-02\n",
      "  6.48955664e-01 9.51238957e-01 1.90832202e-01 5.94355533e-01\n",
      "  8.19266291e-01 8.25587290e-01 9.12820071e-01 7.80937001e-01\n",
      "  1.40323544e-01 3.04266314e-01 1.01068018e-01 2.08815416e-01\n",
      "  8.76160486e-01 5.92044296e-01 6.43805568e-02 6.72089683e-02\n",
      "  8.63113162e-02 2.25004371e-01 6.52608413e-01 6.14433738e-01\n",
      "  8.17667425e-01 4.13472977e-01 5.53462216e-01 9.86159382e-01\n",
      "  9.98205295e-01 7.21735125e-01 6.89881066e-01 1.79587651e-01\n",
      "  3.67414733e-01 5.29402091e-01 9.16702750e-01 2.23358847e-01\n",
      "  1.03077537e-01 7.59485220e-01 5.64297253e-01 4.23058691e-01\n",
      "  6.15091270e-01 2.35913614e-01 8.00335432e-01 9.25106068e-01\n",
      "  8.50864313e-01 5.93769437e-01 1.46458316e-02 9.47514790e-01\n",
      "  1.23541438e-01 6.63006297e-01 5.01391735e-01 1.08922616e-01\n",
      "  1.02241075e-01 7.68051573e-01 4.78185101e-01 7.49525292e-02\n",
      "  6.96288948e-01 8.02050114e-02 4.29918359e-01 5.64906363e-01\n",
      "  3.91940213e-01 5.87260584e-01 4.26964824e-01 4.31609915e-01\n",
      "  6.36830482e-02 3.29836414e-01 1.59523276e-01 7.22780054e-01\n",
      "  1.45319340e-01 3.38389703e-01 8.64593037e-01 7.93629167e-02\n",
      "  8.82647836e-01 1.26560327e-01 4.54621149e-01 7.47073616e-01\n",
      "  8.38648594e-01 7.88240674e-01 4.87981380e-01 1.02033922e-01\n",
      "  7.53824576e-01 8.80503455e-01 5.26667766e-01 1.70832022e-01\n",
      "  4.59578999e-01 9.68491071e-01 3.58481352e-01 5.99294841e-01\n",
      "  9.17980811e-01 7.24717037e-01 1.48540183e-01 5.61828050e-01\n",
      "  8.79343257e-01 3.57899328e-01 9.23142002e-01 3.06276429e-01\n",
      "  1.99895326e-01 5.77201753e-01 5.64176700e-01 6.50676092e-01\n",
      "  8.99023800e-01 9.87216547e-01 1.40562826e-01 4.20669050e-01\n",
      "  8.65570451e-01 4.93327439e-01 8.36044891e-02 7.62234623e-01\n",
      "  8.09694865e-01 7.33151475e-01 8.53777274e-01 7.27790421e-01\n",
      "  7.21596214e-01 5.75475558e-01 1.85634490e-01 7.60941021e-01\n",
      "  8.68988278e-01 9.52526489e-01 1.98959884e-01 7.96138327e-01\n",
      "  7.70841048e-02 9.90881939e-01 1.97835620e-01 4.74448569e-02\n",
      "  4.47470222e-01 7.70385819e-01 3.76060222e-01 8.45132071e-01\n",
      "  3.03899655e-01 2.27218093e-01 7.00394183e-02 3.80355212e-01\n",
      "  7.33148724e-02 7.45899433e-01 9.68576485e-01 1.31821532e-01\n",
      "  1.21071983e-03 3.14203806e-01 4.05175098e-01 8.80299223e-01\n",
      "  6.47539348e-01 1.54625970e-01 1.79617406e-01 4.07198858e-01\n",
      "  6.54979480e-01 5.79013152e-01 2.89476522e-01 9.09077509e-01\n",
      "  9.27795388e-01 9.15114307e-01 1.25444135e-01 1.68089147e-01\n",
      "  5.24880161e-01 6.45077467e-01 5.74236021e-02 8.80103458e-01\n",
      "  4.66110369e-01 4.75794714e-01 9.24947510e-01 5.30051632e-01\n",
      "  3.95270882e-01 7.50718154e-01 2.51712768e-01 9.60988295e-01\n",
      "  6.68698930e-01 8.39521458e-01 9.94641750e-01 7.65509093e-01\n",
      "  5.10789131e-01 3.75964314e-01 4.98999368e-01 5.77279333e-02\n",
      "  7.28732686e-01 7.74327952e-01 7.94852203e-01 4.15577458e-01\n",
      "  4.42385714e-01 2.55212643e-01 6.40362598e-01 3.30229440e-01\n",
      "  1.65403844e-01 3.51086633e-01 6.43734082e-01 4.72091139e-01\n",
      "  2.44011201e-01 4.43200839e-01 5.10144747e-01 4.09462748e-01\n",
      "  2.07558437e-01 8.90966574e-01 9.23904966e-01 6.80915513e-01\n",
      "  1.56220734e-01 3.89121304e-01 7.27521304e-01 6.75597737e-01\n",
      "  8.96699986e-01 2.88810441e-01 5.00049118e-01 7.30119092e-01\n",
      "  1.90255166e-01 5.63507882e-01 2.45779212e-01 3.42289376e-01\n",
      "  4.26175899e-01 8.07078917e-01 4.93456675e-01 8.56635061e-02\n",
      "  7.99070325e-01 3.11806892e-01 7.64812680e-01 7.40974890e-01\n",
      "  4.86190294e-01 1.69101978e-01 4.74845605e-01 3.51403328e-01\n",
      "  9.34620487e-01 9.79416598e-01 3.23967150e-01 4.71716214e-01\n",
      "  6.93357901e-02 9.35392832e-01 5.14906643e-01 1.31701168e-01\n",
      "  2.70940040e-01 1.35228639e-01 9.95836760e-01 1.72704443e-01\n",
      "  4.46681015e-01 4.99668281e-01 8.38712376e-01 2.13996178e-01\n",
      "  9.03477775e-01 9.88385527e-01 8.00238301e-01 5.48059890e-01\n",
      "  5.24024113e-01 5.01593644e-01 1.10481091e-01 8.55222713e-01\n",
      "  8.68575038e-01 9.84732773e-01 7.90514982e-01 5.71469173e-01\n",
      "  2.88336265e-01 9.33100489e-01 8.21234781e-01 6.92288097e-01\n",
      "  2.08972965e-01 5.16271437e-01 7.86246029e-02 2.31582882e-01\n",
      "  9.92603076e-01 6.36647388e-01 2.47828217e-01 8.49987521e-01\n",
      "  3.60312066e-01 1.71886430e-01 1.25640874e-01 4.33521844e-01\n",
      "  3.68702373e-01 9.46485312e-01 7.36789989e-01 3.60602766e-01\n",
      "  8.07357993e-01 3.95647302e-01 4.06502453e-01 7.38389005e-01\n",
      "  8.54093300e-01 6.67345307e-01 7.23898478e-01 1.74530464e-01\n",
      "  9.23460254e-02 8.46753613e-01 3.91485620e-01 8.22161877e-01\n",
      "  2.63939061e-01 4.67599335e-01 2.50135391e-01 5.23066433e-01\n",
      "  2.45276088e-01 5.58068539e-01 1.28890604e-01 2.39346447e-02\n",
      "  3.29896507e-01 8.42649901e-01 4.12461327e-01 7.62080751e-01\n",
      "  1.83393828e-01 3.04328556e-01 5.46356855e-01 7.63156853e-01\n",
      "  8.09632083e-01 2.56458240e-01 9.98523408e-01 7.98308327e-02\n",
      "  7.79708854e-01 8.79005445e-01 6.58053775e-01 9.70739461e-01\n",
      "  1.45860472e-01 9.45154169e-01 5.79524500e-02 4.42631304e-01\n",
      "  7.67007313e-01 7.57385088e-02 1.64860413e-01 6.43935459e-01\n",
      "  5.57354891e-01 8.70987098e-01 2.54182614e-01 3.17570764e-02\n",
      "  3.30297515e-01 4.70752272e-02 8.03732893e-01 2.99278903e-01\n",
      "  7.07830084e-01 3.52636694e-01 3.39160360e-01 2.17996983e-01\n",
      "  3.35997183e-01 1.39113379e-02 7.24116790e-01 8.14954866e-01\n",
      "  2.41685362e-01 1.75034733e-01 2.96054094e-02 9.32524182e-01\n",
      "  4.65674102e-01 9.79171154e-01 8.04203008e-01 1.61069452e-01\n",
      "  3.98148297e-01 1.59934633e-01 4.77530019e-01 2.24084270e-02\n",
      "  7.50474060e-01 8.18731798e-01 8.06218265e-01 5.60816070e-01\n",
      "  9.13321849e-01 8.93211671e-01 2.04387244e-01 3.99975399e-01\n",
      "  6.82213288e-01 4.78422969e-01 2.04223906e-01 8.36656084e-01\n",
      "  7.14373604e-01 2.42750149e-01 6.69397454e-01 4.88102881e-01\n",
      "  9.52273326e-01 2.82860968e-01 9.38249092e-01 7.98684629e-01\n",
      "  8.46974429e-01 8.50873065e-01 2.86883013e-03 2.92007155e-01\n",
      "  7.72449931e-01 7.34961265e-01 4.99528503e-01 9.60726731e-01\n",
      "  2.69298958e-01 7.67438411e-02 8.13095340e-01 9.14852198e-01\n",
      "  8.52982387e-01 1.87464125e-01 9.12960058e-01 8.93471911e-01\n",
      "  1.53606125e-01 8.52539520e-01 4.12792955e-01 6.47763311e-01\n",
      "  4.37671107e-01 6.53467952e-01 8.43433871e-01 2.65409648e-01\n",
      "  2.97435645e-01 7.01278234e-01 6.26220942e-01 1.01229220e-01\n",
      "  2.28364794e-01 5.24534085e-01 6.52709073e-01 7.56429577e-01\n",
      "  6.24632816e-01 4.88484278e-01 7.96430892e-01 9.54923027e-01\n",
      "  3.26723701e-01 2.17759247e-01 8.58663689e-01 2.18627379e-01\n",
      "  9.66932388e-01 8.53119352e-01 3.38972156e-01 7.39867508e-01\n",
      "  9.03362956e-01 7.61743820e-02 9.34552275e-01 5.81327005e-02\n",
      "  5.65206254e-01 5.48722852e-01 2.58714188e-01 4.96655316e-01\n",
      "  9.55196517e-01 6.37777876e-01 4.51729514e-01 8.25724085e-01\n",
      "  6.62393618e-01 7.54714720e-01 8.38832067e-01 4.09215609e-01\n",
      "  6.80232350e-01 2.08248188e-01 7.07921282e-02 2.82840721e-01\n",
      "  3.36615107e-01 5.06705716e-01 2.40692460e-01 9.35621814e-01\n",
      "  3.39896385e-02 4.14598801e-01 6.14998031e-01 5.42620077e-01\n",
      "  7.55465679e-01 2.23763990e-01 4.93282726e-01 9.54502608e-01\n",
      "  8.64502190e-02 6.86300148e-01 7.55478732e-01 1.94841410e-01\n",
      "  2.51699798e-01 2.73440965e-01 4.60033545e-01 3.83432079e-01\n",
      "  5.09571285e-01 6.18547299e-01 8.51103030e-01 2.35313483e-01\n",
      "  4.01877647e-01 6.53873108e-01 6.34532270e-01]]\n",
      "Change in theta: 1.8601439515594545e-06\n",
      "---------------------------------------------\n",
      "Iteration number 23 finished, Running time 527.2001888751984, Roo2 iteration 23 = -3488282.019264486\n",
      "Iteration 1:\n",
      "Theta: [[6.82574365e-01 2.04522602e-01 3.44628185e-01 5.75118050e-01\n",
      "  1.67799781e-01 3.75649347e-01 1.16356729e-01 8.83588767e-01\n",
      "  5.22043327e-01 3.47342128e-01 8.63887003e-01 1.62103792e-01\n",
      "  1.26223278e-01 5.95407189e-01 9.41483967e-01 7.64166265e-01\n",
      "  5.77496302e-01 7.73455869e-01 1.63613615e-01 7.90843890e-01\n",
      "  9.35686677e-02 2.68798017e-01 3.84678809e-01 4.97088066e-01\n",
      "  4.94991034e-01 6.36576296e-01 3.65359151e-01 2.17184505e-01\n",
      "  4.22690724e-01 8.48221915e-01 8.36636294e-01 9.86257299e-01\n",
      "  4.69589766e-01 5.17769028e-01 1.62600890e-02 8.68939109e-01\n",
      "  6.89868698e-01 6.26075453e-01 4.87930203e-01 5.22124387e-01\n",
      "  4.13605543e-01 4.41756675e-01 7.98437726e-01 4.11090798e-01\n",
      "  7.66486021e-01 2.83450926e-01 6.01824107e-01 7.80446755e-02\n",
      "  9.99854048e-01 2.20901789e-01 7.66520198e-01 6.18848814e-01\n",
      "  6.52121668e-01 9.73280178e-01 3.28906330e-01 2.16792718e-02\n",
      "  3.20346675e-01 1.12522528e-01 5.44497730e-02 9.88055831e-01\n",
      "  6.05812195e-02 8.64991686e-01 4.53328556e-02 4.81477845e-01\n",
      "  7.91209818e-01 3.88684700e-01 4.32180584e-01 7.85720896e-02\n",
      "  1.92393032e-01 9.14166427e-01 7.07726923e-01 5.39093866e-02\n",
      "  8.76094611e-01 5.85051191e-01 5.29699735e-01 9.12786811e-01\n",
      "  7.89524156e-01 6.30189768e-01 9.55837815e-01 4.74090190e-01\n",
      "  4.29751971e-01 9.38228826e-01 6.08040253e-01 7.48823052e-01\n",
      "  5.09964150e-01 8.82829617e-01 9.07736087e-01 5.37810416e-01\n",
      "  5.87248525e-01 1.60273199e-01 8.63052187e-01 9.99101377e-02\n",
      "  9.15925141e-01 1.18104859e-01 5.79018107e-01 3.95718450e-01\n",
      "  2.48621028e-01 9.28312086e-01 2.19444590e-01 6.00843495e-02\n",
      "  9.26523129e-01 7.02471683e-01 4.80496611e-01 4.96210090e-01\n",
      "  6.64392027e-03 1.45517993e-01 4.15118962e-01 3.65159662e-02\n",
      "  7.29440909e-01 5.36882626e-01 3.39503592e-01 6.57800492e-01\n",
      "  1.25029535e-01 4.34491055e-01 8.53922724e-02 9.71365753e-01\n",
      "  2.27626810e-01 7.88512701e-01 8.18931759e-01 6.22108164e-01\n",
      "  9.20612385e-01 3.11789165e-01 1.56011880e-01 9.24313394e-01\n",
      "  5.82787912e-01 9.49797949e-01 3.92994411e-02 7.11921112e-01\n",
      "  5.12349626e-01 7.59409602e-01 8.43599553e-01 1.22674532e-01\n",
      "  4.76998699e-01 4.99927463e-01 3.46121190e-01 6.76129962e-01\n",
      "  1.99226834e-01 3.46425907e-01 7.67290156e-01 6.21683424e-02\n",
      "  4.65667818e-01 1.35889996e-01 2.75128401e-02 8.14448094e-01\n",
      "  5.12287766e-01 2.89725313e-01 2.26166296e-01 8.59433365e-01\n",
      "  8.31773462e-01 7.21033037e-01 2.36538602e-01 3.34414427e-01\n",
      "  5.19131918e-03 2.74527146e-01 9.88384003e-02 8.88445976e-01\n",
      "  6.15309267e-01 3.40357536e-01 8.85033019e-01 4.82986018e-01\n",
      "  6.52142324e-01 5.35472575e-01 8.15436123e-01 4.69733874e-01\n",
      "  9.84674411e-01 4.41136368e-02 8.85727618e-01 6.94571301e-01\n",
      "  8.33651158e-01 4.62206454e-01 6.11427695e-01 2.26863275e-01\n",
      "  7.93884768e-01 2.29917279e-01 4.08262323e-01 4.88009827e-01\n",
      "  3.36783732e-01 3.71374505e-01 6.59368897e-03 3.56353653e-01\n",
      "  7.18396227e-01 1.38385656e-01 9.48268589e-01 1.24279832e-01\n",
      "  8.18099054e-01 7.58732721e-01 5.16490979e-01 3.77495102e-01\n",
      "  1.32116701e-01 8.55643048e-01 9.48061845e-01 2.25140732e-01\n",
      "  5.16200578e-01 7.49110829e-01 6.40110857e-01 8.66497939e-01\n",
      "  1.47638347e-01 7.42608493e-01 6.45743322e-01 7.58024930e-01\n",
      "  8.00798262e-01 2.21041984e-01 6.04853326e-03 6.27651560e-01\n",
      "  2.51518428e-01 7.70842346e-01 1.97897402e-01 9.04973966e-01\n",
      "  8.38361697e-01 4.17781210e-01 7.56718018e-01 5.77382819e-01\n",
      "  9.48722113e-01 6.54359924e-01 5.91208753e-01 9.88765047e-01\n",
      "  5.36362483e-01 9.00715345e-01 1.28368583e-01 3.92142127e-01\n",
      "  9.80301967e-01 1.78506367e-01 5.28772553e-01 2.79125702e-01\n",
      "  8.17231943e-04 4.24811229e-01 3.87223192e-01 8.86488743e-01\n",
      "  3.16550539e-01 2.40263541e-01 1.78152940e-01 5.45051445e-01\n",
      "  9.52075059e-02 1.16802894e-01 4.73101654e-01 2.60926876e-02\n",
      "  9.36594466e-01 4.60459745e-01 9.32284479e-01 8.83191185e-01\n",
      "  6.57952957e-02 3.07765795e-01 1.89248332e-01 8.87249169e-01\n",
      "  6.49864936e-01 2.15843326e-01 1.20434381e-01 8.95819801e-01\n",
      "  9.55026564e-01 5.13470956e-01 2.84494743e-01 2.51497072e-01\n",
      "  6.40167117e-01 7.51371627e-01 3.00915020e-01 5.88150250e-01\n",
      "  4.87422206e-01 8.58464350e-01 1.42285419e-01 7.98697632e-01\n",
      "  2.73853286e-01 3.83841386e-01 8.75998475e-01 5.68031778e-01\n",
      "  9.93545815e-01 2.36377030e-01 1.41589209e-01 1.24075375e-01\n",
      "  5.27913862e-01 3.77270976e-01 1.40579369e-02 5.92229419e-01\n",
      "  2.26233864e-01 2.20738231e-01 9.22164480e-01 3.01009006e-01\n",
      "  8.47040374e-01 4.54765760e-01 7.00676371e-01 4.32779029e-01\n",
      "  2.82784855e-01 1.21300142e-02 9.12970109e-01 9.16898145e-02\n",
      "  4.80188367e-01 7.40668797e-01 1.07388708e-01 5.77284435e-01\n",
      "  2.73553043e-01 8.93725008e-01 8.18054278e-01 1.64506561e-01\n",
      "  3.99039933e-01 2.98322125e-01 1.90883024e-02 3.37457743e-01\n",
      "  8.73764974e-01 5.34797363e-01 1.41349736e-02 1.63438222e-01\n",
      "  7.28872949e-01 6.17559720e-01 5.02948874e-01 4.73831988e-01\n",
      "  9.99536021e-01 4.57523251e-02 1.05257989e-03 8.25583180e-01\n",
      "  3.13223980e-01 2.34291692e-02 5.74200216e-01 3.39243058e-01\n",
      "  2.72270685e-01 6.21530031e-01 7.02703368e-01 1.13287522e-01\n",
      "  4.60250923e-01 1.75229253e-01 8.24624765e-01 7.05595295e-02\n",
      "  6.04573882e-01 8.05313068e-01 9.78895594e-01 5.74007919e-01\n",
      "  5.04731825e-01 2.05056453e-01 2.04866969e-01 9.94132089e-01\n",
      "  4.67057054e-02 9.70119637e-01 9.73004540e-01 1.93460331e-01\n",
      "  9.06031488e-01 1.03417765e-01 2.73920120e-01 1.36032093e-01\n",
      "  2.00724349e-01 5.62376981e-01 6.69034963e-01 8.94449715e-01\n",
      "  1.83290771e-01 6.23319851e-01 3.17596899e-01 6.02593781e-01\n",
      "  1.42340769e-01 4.40905503e-01 1.35388221e-01 4.46420659e-01\n",
      "  8.82994468e-01 7.66579369e-01 8.33557026e-01 6.51920064e-01\n",
      "  4.96796120e-01 1.96714638e-01 4.71319990e-01 8.09278831e-01\n",
      "  9.68956756e-01 1.04469203e-01 9.82763460e-02 5.16562036e-02\n",
      "  5.56998355e-01 3.29733224e-01 4.34734155e-01 7.55832630e-01\n",
      "  8.99339012e-01 7.09527403e-01 2.51639601e-01 7.89585931e-01\n",
      "  1.51674646e-01 2.22349767e-01 2.34431887e-01 5.26803170e-01\n",
      "  7.36719688e-01 4.32955033e-01 2.72604734e-02 7.41959711e-01\n",
      "  5.81491299e-01 6.78601058e-01 3.52227874e-01 3.95200392e-01\n",
      "  2.58023098e-01 9.22854450e-01 5.25844796e-01 8.64687642e-01\n",
      "  8.70296438e-02 6.75852805e-01 1.13519250e-01 8.76542621e-01\n",
      "  8.34091464e-01 2.01396068e-01 1.08225920e-01 4.38360392e-01\n",
      "  2.80598715e-01 4.26079336e-01 7.02975131e-01 6.32363102e-01\n",
      "  3.66941536e-01 3.11667104e-01 3.82076530e-01 7.29478204e-01\n",
      "  9.69840461e-03 6.19875271e-02 3.53990461e-01 2.69965319e-01\n",
      "  9.66149921e-01 7.30328825e-01 9.43747519e-01 7.51398155e-01\n",
      "  5.49231660e-01 5.10253125e-01 6.98477034e-01 5.46679350e-01\n",
      "  4.81242646e-01 5.08644062e-01 1.50955745e-01 3.67523825e-01\n",
      "  8.24298893e-01 9.09533600e-01 9.44639622e-02 8.25108147e-01\n",
      "  2.40396655e-01 9.05178733e-01 3.45904093e-01 2.32407835e-01\n",
      "  2.50717341e-01 4.80705751e-02 9.59476689e-01 1.43134124e-01\n",
      "  9.60571117e-01 6.71631905e-01 3.57143266e-01 9.39047155e-01\n",
      "  3.73592044e-01 2.37526312e-01 1.69446285e-01 7.89374126e-01\n",
      "  1.27684273e-01 6.36394495e-01 2.12397431e-01 7.46967035e-01\n",
      "  6.92354104e-01 9.55249730e-01 1.11658567e-01 5.90113519e-01\n",
      "  7.51916985e-02 6.89097790e-01 1.23330867e-01 8.00806150e-01\n",
      "  4.69706678e-01 3.68753489e-01 5.13026506e-01 9.94911048e-01\n",
      "  9.26169079e-01 5.79564671e-01 5.40856793e-01 9.24876439e-01\n",
      "  4.96935262e-01 3.09739388e-01 9.76925300e-01 8.82781110e-01\n",
      "  8.42211598e-01 5.27676595e-02 5.77241500e-01 9.42874412e-01\n",
      "  9.52963041e-01 8.25970161e-02 9.22207631e-01 9.69205477e-02\n",
      "  6.48955971e-01 9.51239264e-01 1.90832509e-01 5.94355840e-01\n",
      "  8.19266598e-01 8.25587597e-01 9.12820378e-01 7.80937308e-01\n",
      "  1.40323851e-01 3.04266621e-01 1.01068325e-01 2.08815723e-01\n",
      "  8.76160793e-01 5.92044603e-01 6.43808638e-02 6.72092753e-02\n",
      "  8.63116232e-02 2.25004678e-01 6.52608720e-01 6.14434045e-01\n",
      "  8.17667732e-01 4.13473284e-01 5.53462523e-01 9.86159689e-01\n",
      "  9.98205602e-01 7.21735432e-01 6.89881372e-01 1.79587957e-01\n",
      "  3.67415040e-01 5.29402398e-01 9.16703057e-01 2.23359154e-01\n",
      "  1.03077844e-01 7.59485527e-01 5.64297560e-01 4.23058998e-01\n",
      "  6.15091577e-01 2.35913921e-01 8.00335739e-01 9.25106375e-01\n",
      "  8.50864620e-01 5.93769744e-01 1.46461386e-02 9.47515097e-01\n",
      "  1.23541745e-01 6.63006604e-01 5.01392042e-01 1.08922923e-01\n",
      "  1.02241382e-01 7.68051880e-01 4.78185408e-01 7.49528362e-02\n",
      "  6.96289254e-01 8.02053184e-02 4.29918666e-01 5.64906670e-01\n",
      "  3.91940520e-01 5.87260891e-01 4.26965131e-01 4.31610222e-01\n",
      "  6.36833551e-02 3.29836721e-01 1.59523583e-01 7.22780361e-01\n",
      "  1.45319647e-01 3.38390010e-01 8.64593344e-01 7.93632236e-02\n",
      "  8.82648143e-01 1.26560634e-01 4.54621456e-01 7.47073923e-01\n",
      "  8.38648900e-01 7.88240981e-01 4.87981687e-01 1.02034229e-01\n",
      "  7.53824883e-01 8.80503762e-01 5.26668073e-01 1.70832329e-01\n",
      "  4.59579305e-01 9.68491378e-01 3.58481659e-01 5.99295148e-01\n",
      "  9.17981118e-01 7.24717344e-01 1.48540490e-01 5.61828357e-01\n",
      "  8.79343564e-01 3.57899635e-01 9.23142309e-01 3.06276736e-01\n",
      "  1.99895633e-01 5.77202060e-01 5.64177007e-01 6.50676399e-01\n",
      "  8.99024107e-01 9.87216853e-01 1.40563133e-01 4.20669357e-01\n",
      "  8.65570758e-01 4.93327746e-01 8.36047961e-02 7.62234930e-01\n",
      "  8.09695171e-01 7.33151782e-01 8.53777581e-01 7.27790727e-01\n",
      "  7.21596521e-01 5.75475865e-01 1.85634797e-01 7.60941328e-01\n",
      "  8.68988585e-01 9.52526796e-01 1.98960191e-01 7.96138634e-01\n",
      "  7.70844118e-02 9.90882246e-01 1.97835927e-01 4.74451639e-02\n",
      "  4.47470529e-01 7.70386126e-01 3.76060529e-01 8.45132378e-01\n",
      "  3.03899962e-01 2.27218400e-01 7.00397252e-02 3.80355519e-01\n",
      "  7.33151794e-02 7.45899740e-01 9.68576792e-01 1.31821839e-01\n",
      "  1.21102681e-03 3.14204113e-01 4.05175405e-01 8.80299530e-01\n",
      "  6.47539655e-01 1.54626277e-01 1.79617713e-01 4.07199165e-01\n",
      "  6.54979787e-01 5.79013459e-01 2.89476829e-01 9.09077816e-01\n",
      "  9.27795695e-01 9.15114614e-01 1.25444442e-01 1.68089454e-01\n",
      "  5.24880468e-01 6.45077774e-01 5.74239091e-02 8.80103765e-01\n",
      "  4.66110676e-01 4.75795021e-01 9.24947817e-01 5.30051939e-01\n",
      "  3.95271189e-01 7.50718461e-01 2.51713075e-01 9.60988602e-01\n",
      "  6.68699237e-01 8.39521765e-01 9.94642057e-01 7.65509400e-01\n",
      "  5.10789438e-01 3.75964621e-01 4.98999675e-01 5.77282403e-02\n",
      "  7.28732993e-01 7.74328259e-01 7.94852510e-01 4.15577765e-01\n",
      "  4.42386021e-01 2.55212950e-01 6.40362905e-01 3.30229747e-01\n",
      "  1.65404151e-01 3.51086940e-01 6.43734389e-01 4.72091446e-01\n",
      "  2.44011508e-01 4.43201146e-01 5.10145054e-01 4.09463055e-01\n",
      "  2.07558744e-01 8.90966881e-01 9.23905273e-01 6.80915820e-01\n",
      "  1.56221041e-01 3.89121611e-01 7.27521611e-01 6.75598044e-01\n",
      "  8.96700293e-01 2.88810748e-01 5.00049425e-01 7.30119399e-01\n",
      "  1.90255473e-01 5.63508189e-01 2.45779519e-01 3.42289683e-01\n",
      "  4.26176206e-01 8.07079224e-01 4.93456982e-01 8.56638131e-02\n",
      "  7.99070632e-01 3.11807199e-01 7.64812987e-01 7.40975197e-01\n",
      "  4.86190601e-01 1.69102285e-01 4.74845912e-01 3.51403635e-01\n",
      "  9.34620794e-01 9.79416905e-01 3.23967457e-01 4.71716521e-01\n",
      "  6.93360971e-02 9.35393139e-01 5.14906949e-01 1.31701475e-01\n",
      "  2.70940347e-01 1.35228946e-01 9.95837066e-01 1.72704750e-01\n",
      "  4.46681322e-01 4.99668588e-01 8.38712683e-01 2.13996485e-01\n",
      "  9.03478082e-01 9.88385834e-01 8.00238608e-01 5.48060197e-01\n",
      "  5.24024420e-01 5.01593951e-01 1.10481398e-01 8.55223020e-01\n",
      "  8.68575345e-01 9.84733079e-01 7.90515289e-01 5.71469480e-01\n",
      "  2.88336572e-01 9.33100796e-01 8.21235088e-01 6.92288404e-01\n",
      "  2.08973272e-01 5.16271744e-01 7.86249099e-02 2.31583189e-01\n",
      "  9.92603382e-01 6.36647695e-01 2.47828524e-01 8.49987828e-01\n",
      "  3.60312373e-01 1.71886737e-01 1.25641181e-01 4.33522151e-01\n",
      "  3.68702680e-01 9.46485619e-01 7.36790296e-01 3.60603073e-01\n",
      "  8.07358300e-01 3.95647609e-01 4.06502760e-01 7.38389312e-01\n",
      "  8.54093607e-01 6.67345614e-01 7.23898785e-01 1.74530771e-01\n",
      "  9.23463323e-02 8.46753919e-01 3.91485927e-01 8.22162184e-01\n",
      "  2.63939368e-01 4.67599642e-01 2.50135698e-01 5.23066740e-01\n",
      "  2.45276395e-01 5.58068846e-01 1.28890911e-01 2.39349517e-02\n",
      "  3.29896814e-01 8.42650208e-01 4.12461634e-01 7.62081058e-01\n",
      "  1.83394135e-01 3.04328863e-01 5.46357162e-01 7.63157160e-01\n",
      "  8.09632390e-01 2.56458547e-01 9.98523715e-01 7.98311397e-02\n",
      "  7.79709161e-01 8.79005752e-01 6.58054082e-01 9.70739768e-01\n",
      "  1.45860779e-01 9.45154476e-01 5.79527569e-02 4.42631611e-01\n",
      "  7.67007620e-01 7.57388157e-02 1.64860720e-01 6.43935766e-01\n",
      "  5.57355198e-01 8.70987404e-01 2.54182921e-01 3.17573834e-02\n",
      "  3.30297822e-01 4.70755341e-02 8.03733200e-01 2.99279210e-01\n",
      "  7.07830391e-01 3.52637001e-01 3.39160667e-01 2.17997290e-01\n",
      "  3.35997490e-01 1.39116449e-02 7.24117097e-01 8.14955173e-01\n",
      "  2.41685669e-01 1.75035040e-01 2.96057164e-02 9.32524489e-01\n",
      "  4.65674409e-01 9.79171461e-01 8.04203315e-01 1.61069759e-01\n",
      "  3.98148604e-01 1.59934940e-01 4.77530326e-01 2.24087340e-02\n",
      "  7.50474367e-01 8.18732105e-01 8.06218572e-01 5.60816377e-01\n",
      "  9.13322156e-01 8.93211978e-01 2.04387551e-01 3.99975706e-01\n",
      "  6.82213595e-01 4.78423276e-01 2.04224213e-01 8.36656391e-01\n",
      "  7.14373911e-01 2.42750456e-01 6.69397761e-01 4.88103188e-01\n",
      "  9.52273633e-01 2.82861275e-01 9.38249399e-01 7.98684936e-01\n",
      "  8.46974736e-01 8.50873372e-01 2.86913710e-03 2.92007462e-01\n",
      "  7.72450238e-01 7.34961572e-01 4.99528810e-01 9.60727038e-01\n",
      "  2.69299265e-01 7.67441481e-02 8.13095647e-01 9.14852505e-01\n",
      "  8.52982694e-01 1.87464432e-01 9.12960365e-01 8.93472218e-01\n",
      "  1.53606432e-01 8.52539827e-01 4.12793262e-01 6.47763618e-01\n",
      "  4.37671414e-01 6.53468259e-01 8.43434178e-01 2.65409955e-01\n",
      "  2.97435952e-01 7.01278541e-01 6.26221249e-01 1.01229527e-01\n",
      "  2.28365101e-01 5.24534392e-01 6.52709380e-01 7.56429884e-01\n",
      "  6.24633123e-01 4.88484585e-01 7.96431199e-01 9.54923334e-01\n",
      "  3.26724008e-01 2.17759554e-01 8.58663996e-01 2.18627686e-01\n",
      "  9.66932695e-01 8.53119659e-01 3.38972463e-01 7.39867815e-01\n",
      "  9.03363263e-01 7.61746890e-02 9.34552582e-01 5.81330074e-02\n",
      "  5.65206561e-01 5.48723159e-01 2.58714495e-01 4.96655623e-01\n",
      "  9.55196824e-01 6.37778183e-01 4.51729821e-01 8.25724392e-01\n",
      "  6.62393925e-01 7.54715027e-01 8.38832374e-01 4.09215916e-01\n",
      "  6.80232657e-01 2.08248495e-01 7.07924352e-02 2.82841028e-01\n",
      "  3.36615414e-01 5.06706023e-01 2.40692767e-01 9.35622121e-01\n",
      "  3.39899455e-02 4.14599108e-01 6.14998338e-01 5.42620384e-01\n",
      "  7.55465986e-01 2.23764297e-01 4.93283033e-01 9.54502915e-01\n",
      "  8.64505260e-02 6.86300455e-01 7.55479039e-01 1.94841717e-01\n",
      "  2.51700105e-01 2.73441272e-01 4.60033852e-01 3.83432386e-01\n",
      "  5.09571592e-01 6.18547606e-01 8.51103337e-01 2.35313790e-01\n",
      "  4.01877954e-01 6.53873415e-01 6.34532577e-01]]\n",
      "Change in theta: 1.8855033301344997e-06\n",
      "---------------------------------------------\n",
      "Iteration number 24 finished, Running time 607.9889569282532, Roo2 iteration 24 = -4682214.278033669\n",
      "Iteration 1:\n",
      "Theta: [[6.82574605e-01 2.04522843e-01 3.44628426e-01 5.75118291e-01\n",
      "  1.67800021e-01 3.75649588e-01 1.16356970e-01 8.83589008e-01\n",
      "  5.22043568e-01 3.47342369e-01 8.63887243e-01 1.62104032e-01\n",
      "  1.26223518e-01 5.95407430e-01 9.41484207e-01 7.64166506e-01\n",
      "  5.77496542e-01 7.73456110e-01 1.63613856e-01 7.90844131e-01\n",
      "  9.35689084e-02 2.68798258e-01 3.84679050e-01 4.97088307e-01\n",
      "  4.94991274e-01 6.36576536e-01 3.65359392e-01 2.17184746e-01\n",
      "  4.22690964e-01 8.48222156e-01 8.36636535e-01 9.86257539e-01\n",
      "  4.69590007e-01 5.17769268e-01 1.62603297e-02 8.68939350e-01\n",
      "  6.89868939e-01 6.26075693e-01 4.87930443e-01 5.22124628e-01\n",
      "  4.13605784e-01 4.41756915e-01 7.98437967e-01 4.11091039e-01\n",
      "  7.66486262e-01 2.83451166e-01 6.01824348e-01 7.80449162e-02\n",
      "  9.99854288e-01 2.20902029e-01 7.66520439e-01 6.18849054e-01\n",
      "  6.52121909e-01 9.73280419e-01 3.28906571e-01 2.16795124e-02\n",
      "  3.20346915e-01 1.12522769e-01 5.44500137e-02 9.88056072e-01\n",
      "  6.05814602e-02 8.64991927e-01 4.53330963e-02 4.81478086e-01\n",
      "  7.91210059e-01 3.88684941e-01 4.32180824e-01 7.85723303e-02\n",
      "  1.92393273e-01 9.14166668e-01 7.07727164e-01 5.39096273e-02\n",
      "  8.76094852e-01 5.85051432e-01 5.29699975e-01 9.12787051e-01\n",
      "  7.89524396e-01 6.30190008e-01 9.55838056e-01 4.74090430e-01\n",
      "  4.29752212e-01 9.38229066e-01 6.08040493e-01 7.48823292e-01\n",
      "  5.09964390e-01 8.82829858e-01 9.07736328e-01 5.37810656e-01\n",
      "  5.87248765e-01 1.60273440e-01 8.63052427e-01 9.99103784e-02\n",
      "  9.15925382e-01 1.18105100e-01 5.79018347e-01 3.95718691e-01\n",
      "  2.48621269e-01 9.28312327e-01 2.19444831e-01 6.00845901e-02\n",
      "  9.26523370e-01 7.02471924e-01 4.80496852e-01 4.96210331e-01\n",
      "  6.64416095e-03 1.45518234e-01 4.15119203e-01 3.65162069e-02\n",
      "  7.29441150e-01 5.36882867e-01 3.39503832e-01 6.57800733e-01\n",
      "  1.25029776e-01 4.34491296e-01 8.53925131e-02 9.71365994e-01\n",
      "  2.27627051e-01 7.88512942e-01 8.18931999e-01 6.22108405e-01\n",
      "  9.20612625e-01 3.11789406e-01 1.56012121e-01 9.24313635e-01\n",
      "  5.82788153e-01 9.49798190e-01 3.92996818e-02 7.11921352e-01\n",
      "  5.12349867e-01 7.59409843e-01 8.43599793e-01 1.22674773e-01\n",
      "  4.76998940e-01 4.99927704e-01 3.46121431e-01 6.76130203e-01\n",
      "  1.99227075e-01 3.46426148e-01 7.67290397e-01 6.21685831e-02\n",
      "  4.65668059e-01 1.35890237e-01 2.75130808e-02 8.14448334e-01\n",
      "  5.12288006e-01 2.89725554e-01 2.26166537e-01 8.59433606e-01\n",
      "  8.31773702e-01 7.21033278e-01 2.36538843e-01 3.34414667e-01\n",
      "  5.19155986e-03 2.74527387e-01 9.88386409e-02 8.88446217e-01\n",
      "  6.15309508e-01 3.40357777e-01 8.85033259e-01 4.82986259e-01\n",
      "  6.52142564e-01 5.35472815e-01 8.15436363e-01 4.69734115e-01\n",
      "  9.84674652e-01 4.41138775e-02 8.85727858e-01 6.94571541e-01\n",
      "  8.33651399e-01 4.62206695e-01 6.11427935e-01 2.26863516e-01\n",
      "  7.93885008e-01 2.29917520e-01 4.08262563e-01 4.88010068e-01\n",
      "  3.36783972e-01 3.71374745e-01 6.59392966e-03 3.56353894e-01\n",
      "  7.18396468e-01 1.38385896e-01 9.48268829e-01 1.24280073e-01\n",
      "  8.18099295e-01 7.58732962e-01 5.16491220e-01 3.77495343e-01\n",
      "  1.32116941e-01 8.55643289e-01 9.48062086e-01 2.25140972e-01\n",
      "  5.16200819e-01 7.49111069e-01 6.40111097e-01 8.66498180e-01\n",
      "  1.47638588e-01 7.42608734e-01 6.45743563e-01 7.58025170e-01\n",
      "  8.00798503e-01 2.21042225e-01 6.04877394e-03 6.27651801e-01\n",
      "  2.51518669e-01 7.70842586e-01 1.97897643e-01 9.04974206e-01\n",
      "  8.38361938e-01 4.17781450e-01 7.56718259e-01 5.77383060e-01\n",
      "  9.48722354e-01 6.54360165e-01 5.91208993e-01 9.88765287e-01\n",
      "  5.36362723e-01 9.00715586e-01 1.28368823e-01 3.92142368e-01\n",
      "  9.80302207e-01 1.78506608e-01 5.28772793e-01 2.79125943e-01\n",
      "  8.17472625e-04 4.24811470e-01 3.87223432e-01 8.86488983e-01\n",
      "  3.16550780e-01 2.40263782e-01 1.78153181e-01 5.45051685e-01\n",
      "  9.52077466e-02 1.16803135e-01 4.73101894e-01 2.60929283e-02\n",
      "  9.36594707e-01 4.60459986e-01 9.32284720e-01 8.83191426e-01\n",
      "  6.57955364e-02 3.07766036e-01 1.89248573e-01 8.87249409e-01\n",
      "  6.49865176e-01 2.15843566e-01 1.20434622e-01 8.95820042e-01\n",
      "  9.55026805e-01 5.13471197e-01 2.84494984e-01 2.51497312e-01\n",
      "  6.40167357e-01 7.51371867e-01 3.00915261e-01 5.88150491e-01\n",
      "  4.87422447e-01 8.58464591e-01 1.42285659e-01 7.98697873e-01\n",
      "  2.73853527e-01 3.83841626e-01 8.75998716e-01 5.68032019e-01\n",
      "  9.93546056e-01 2.36377271e-01 1.41589450e-01 1.24075616e-01\n",
      "  5.27914103e-01 3.77271216e-01 1.40581776e-02 5.92229659e-01\n",
      "  2.26234105e-01 2.20738472e-01 9.22164721e-01 3.01009247e-01\n",
      "  8.47040615e-01 4.54766000e-01 7.00676611e-01 4.32779270e-01\n",
      "  2.82785096e-01 1.21302549e-02 9.12970350e-01 9.16900552e-02\n",
      "  4.80188608e-01 7.40669037e-01 1.07388948e-01 5.77284676e-01\n",
      "  2.73553283e-01 8.93725248e-01 8.18054519e-01 1.64506802e-01\n",
      "  3.99040173e-01 2.98322366e-01 1.90885431e-02 3.37457983e-01\n",
      "  8.73765215e-01 5.34797603e-01 1.41352143e-02 1.63438463e-01\n",
      "  7.28873190e-01 6.17559961e-01 5.02949115e-01 4.73832228e-01\n",
      "  9.99536261e-01 4.57525658e-02 1.05282057e-03 8.25583420e-01\n",
      "  3.13224221e-01 2.34294099e-02 5.74200457e-01 3.39243299e-01\n",
      "  2.72270926e-01 6.21530272e-01 7.02703609e-01 1.13287763e-01\n",
      "  4.60251164e-01 1.75229494e-01 8.24625006e-01 7.05597702e-02\n",
      "  6.04574123e-01 8.05313308e-01 9.78895835e-01 5.74008160e-01\n",
      "  5.04732066e-01 2.05056693e-01 2.04867210e-01 9.94132330e-01\n",
      "  4.67059460e-02 9.70119877e-01 9.73004780e-01 1.93460571e-01\n",
      "  9.06031728e-01 1.03418006e-01 2.73920361e-01 1.36032334e-01\n",
      "  2.00724590e-01 5.62377221e-01 6.69035204e-01 8.94449955e-01\n",
      "  1.83291012e-01 6.23320092e-01 3.17597140e-01 6.02594022e-01\n",
      "  1.42341010e-01 4.40905744e-01 1.35388462e-01 4.46420899e-01\n",
      "  8.82994708e-01 7.66579609e-01 8.33557266e-01 6.51920305e-01\n",
      "  4.96796361e-01 1.96714879e-01 4.71320231e-01 8.09279072e-01\n",
      "  9.68956997e-01 1.04469444e-01 9.82765867e-02 5.16564443e-02\n",
      "  5.56998596e-01 3.29733465e-01 4.34734395e-01 7.55832871e-01\n",
      "  8.99339253e-01 7.09527644e-01 2.51639841e-01 7.89586172e-01\n",
      "  1.51674887e-01 2.22350007e-01 2.34432128e-01 5.26803411e-01\n",
      "  7.36719928e-01 4.32955274e-01 2.72607141e-02 7.41959952e-01\n",
      "  5.81491540e-01 6.78601298e-01 3.52228115e-01 3.95200632e-01\n",
      "  2.58023338e-01 9.22854691e-01 5.25845036e-01 8.64687883e-01\n",
      "  8.70298845e-02 6.75853045e-01 1.13519490e-01 8.76542862e-01\n",
      "  8.34091704e-01 2.01396308e-01 1.08226161e-01 4.38360633e-01\n",
      "  2.80598956e-01 4.26079577e-01 7.02975371e-01 6.32363343e-01\n",
      "  3.66941776e-01 3.11667345e-01 3.82076771e-01 7.29478444e-01\n",
      "  9.69864530e-03 6.19877678e-02 3.53990702e-01 2.69965560e-01\n",
      "  9.66150162e-01 7.30329065e-01 9.43747760e-01 7.51398396e-01\n",
      "  5.49231901e-01 5.10253365e-01 6.98477274e-01 5.46679590e-01\n",
      "  4.81242887e-01 5.08644303e-01 1.50955986e-01 3.67524065e-01\n",
      "  8.24299134e-01 9.09533841e-01 9.44642029e-02 8.25108388e-01\n",
      "  2.40396896e-01 9.05178973e-01 3.45904334e-01 2.32408076e-01\n",
      "  2.50717582e-01 4.80708157e-02 9.59476930e-01 1.43134365e-01\n",
      "  9.60571358e-01 6.71632146e-01 3.57143507e-01 9.39047396e-01\n",
      "  3.73592284e-01 2.37526553e-01 1.69446525e-01 7.89374366e-01\n",
      "  1.27684513e-01 6.36394735e-01 2.12397672e-01 7.46967276e-01\n",
      "  6.92354345e-01 9.55249971e-01 1.11658808e-01 5.90113759e-01\n",
      "  7.51919392e-02 6.89098031e-01 1.23331108e-01 8.00806390e-01\n",
      "  4.69706918e-01 3.68753730e-01 5.13026747e-01 9.94911289e-01\n",
      "  9.26169320e-01 5.79564912e-01 5.40857034e-01 9.24876679e-01\n",
      "  4.96935502e-01 3.09739629e-01 9.76925541e-01 8.82781351e-01\n",
      "  8.42211839e-01 5.27679002e-02 5.77241740e-01 9.42874653e-01\n",
      "  9.52963282e-01 8.25972568e-02 9.22207871e-01 9.69207884e-02\n",
      "  6.48956212e-01 9.51239504e-01 1.90832750e-01 5.94356080e-01\n",
      "  8.19266838e-01 8.25587838e-01 9.12820619e-01 7.80937548e-01\n",
      "  1.40324092e-01 3.04266861e-01 1.01068565e-01 2.08815964e-01\n",
      "  8.76161034e-01 5.92044843e-01 6.43811045e-02 6.72095159e-02\n",
      "  8.63118638e-02 2.25004919e-01 6.52608961e-01 6.14434286e-01\n",
      "  8.17667973e-01 4.13473525e-01 5.53462763e-01 9.86159930e-01\n",
      "  9.98205843e-01 7.21735672e-01 6.89881613e-01 1.79588198e-01\n",
      "  3.67415281e-01 5.29402639e-01 9.16703297e-01 2.23359394e-01\n",
      "  1.03078085e-01 7.59485767e-01 5.64297801e-01 4.23059239e-01\n",
      "  6.15091817e-01 2.35914161e-01 8.00335980e-01 9.25106616e-01\n",
      "  8.50864861e-01 5.93769984e-01 1.46463793e-02 9.47515338e-01\n",
      "  1.23541985e-01 6.63006845e-01 5.01392283e-01 1.08923163e-01\n",
      "  1.02241623e-01 7.68052120e-01 4.78185649e-01 7.49530769e-02\n",
      "  6.96289495e-01 8.02055591e-02 4.29918907e-01 5.64906911e-01\n",
      "  3.91940761e-01 5.87261132e-01 4.26965372e-01 4.31610463e-01\n",
      "  6.36835958e-02 3.29836961e-01 1.59523823e-01 7.22780602e-01\n",
      "  1.45319888e-01 3.38390250e-01 8.64593584e-01 7.93634643e-02\n",
      "  8.82648383e-01 1.26560875e-01 4.54621697e-01 7.47074163e-01\n",
      "  8.38649141e-01 7.88241222e-01 4.87981927e-01 1.02034470e-01\n",
      "  7.53825124e-01 8.80504002e-01 5.26668314e-01 1.70832570e-01\n",
      "  4.59579546e-01 9.68491619e-01 3.58481900e-01 5.99295389e-01\n",
      "  9.17981359e-01 7.24717584e-01 1.48540730e-01 5.61828597e-01\n",
      "  8.79343805e-01 3.57899875e-01 9.23142550e-01 3.06276977e-01\n",
      "  1.99895874e-01 5.77202301e-01 5.64177247e-01 6.50676640e-01\n",
      "  8.99024348e-01 9.87217094e-01 1.40563374e-01 4.20669598e-01\n",
      "  8.65570999e-01 4.93327986e-01 8.36050368e-02 7.62235171e-01\n",
      "  8.09695412e-01 7.33152023e-01 8.53777822e-01 7.27790968e-01\n",
      "  7.21596762e-01 5.75476106e-01 1.85635038e-01 7.60941568e-01\n",
      "  8.68988826e-01 9.52527037e-01 1.98960431e-01 7.96138875e-01\n",
      "  7.70846525e-02 9.90882487e-01 1.97836167e-01 4.74454045e-02\n",
      "  4.47470769e-01 7.70386366e-01 3.76060770e-01 8.45132619e-01\n",
      "  3.03900202e-01 2.27218641e-01 7.00399659e-02 3.80355760e-01\n",
      "  7.33154201e-02 7.45899981e-01 9.68577033e-01 1.31822079e-01\n",
      "  1.21126749e-03 3.14204354e-01 4.05175645e-01 8.80299771e-01\n",
      "  6.47539896e-01 1.54626518e-01 1.79617953e-01 4.07199406e-01\n",
      "  6.54980028e-01 5.79013700e-01 2.89477070e-01 9.09078057e-01\n",
      "  9.27795935e-01 9.15114855e-01 1.25444683e-01 1.68089694e-01\n",
      "  5.24880708e-01 6.45078015e-01 5.74241498e-02 8.80104006e-01\n",
      "  4.66110917e-01 4.75795261e-01 9.24948058e-01 5.30052179e-01\n",
      "  3.95271430e-01 7.50718702e-01 2.51713316e-01 9.60988843e-01\n",
      "  6.68699478e-01 8.39522006e-01 9.94642298e-01 7.65509641e-01\n",
      "  5.10789679e-01 3.75964861e-01 4.98999916e-01 5.77284810e-02\n",
      "  7.28733233e-01 7.74328500e-01 7.94852751e-01 4.15578006e-01\n",
      "  4.42386261e-01 2.55213191e-01 6.40363146e-01 3.30229988e-01\n",
      "  1.65404392e-01 3.51087181e-01 6.43734630e-01 4.72091687e-01\n",
      "  2.44011749e-01 4.43201387e-01 5.10145295e-01 4.09463296e-01\n",
      "  2.07558984e-01 8.90967122e-01 9.23905514e-01 6.80916061e-01\n",
      "  1.56221281e-01 3.89121852e-01 7.27521852e-01 6.75598285e-01\n",
      "  8.96700534e-01 2.88810989e-01 5.00049666e-01 7.30119640e-01\n",
      "  1.90255714e-01 5.63508429e-01 2.45779760e-01 3.42289924e-01\n",
      "  4.26176447e-01 8.07079464e-01 4.93457223e-01 8.56640538e-02\n",
      "  7.99070873e-01 3.11807439e-01 7.64813228e-01 7.40975437e-01\n",
      "  4.86190842e-01 1.69102526e-01 4.74846153e-01 3.51403876e-01\n",
      "  9.34621034e-01 9.79417146e-01 3.23967697e-01 4.71716762e-01\n",
      "  6.93363378e-02 9.35393380e-01 5.14907190e-01 1.31701716e-01\n",
      "  2.70940588e-01 1.35229186e-01 9.95837307e-01 1.72704990e-01\n",
      "  4.46681563e-01 4.99668829e-01 8.38712924e-01 2.13996725e-01\n",
      "  9.03478323e-01 9.88386075e-01 8.00238848e-01 5.48060438e-01\n",
      "  5.24024661e-01 5.01594192e-01 1.10481639e-01 8.55223260e-01\n",
      "  8.68575585e-01 9.84733320e-01 7.90515530e-01 5.71469721e-01\n",
      "  2.88336813e-01 9.33101036e-01 8.21235329e-01 6.92288644e-01\n",
      "  2.08973513e-01 5.16271985e-01 7.86251506e-02 2.31583430e-01\n",
      "  9.92603623e-01 6.36647936e-01 2.47828764e-01 8.49988069e-01\n",
      "  3.60312614e-01 1.71886977e-01 1.25641421e-01 4.33522392e-01\n",
      "  3.68702920e-01 9.46485860e-01 7.36790537e-01 3.60603313e-01\n",
      "  8.07358540e-01 3.95647850e-01 4.06503001e-01 7.38389553e-01\n",
      "  8.54093848e-01 6.67345855e-01 7.23899026e-01 1.74531012e-01\n",
      "  9.23465730e-02 8.46754160e-01 3.91486168e-01 8.22162425e-01\n",
      "  2.63939609e-01 4.67599883e-01 2.50135939e-01 5.23066980e-01\n",
      "  2.45276636e-01 5.58069087e-01 1.28891151e-01 2.39351924e-02\n",
      "  3.29897055e-01 8.42650448e-01 4.12461875e-01 7.62081299e-01\n",
      "  1.83394376e-01 3.04329103e-01 5.46357403e-01 7.63157400e-01\n",
      "  8.09632631e-01 2.56458788e-01 9.98523956e-01 7.98313804e-02\n",
      "  7.79709402e-01 8.79005993e-01 6.58054322e-01 9.70740009e-01\n",
      "  1.45861020e-01 9.45154716e-01 5.79529976e-02 4.42631851e-01\n",
      "  7.67007861e-01 7.57390564e-02 1.64860960e-01 6.43936007e-01\n",
      "  5.57355438e-01 8.70987645e-01 2.54183162e-01 3.17576241e-02\n",
      "  3.30298062e-01 4.70757748e-02 8.03733440e-01 2.99279451e-01\n",
      "  7.07830632e-01 3.52637242e-01 3.39160907e-01 2.17997530e-01\n",
      "  3.35997731e-01 1.39118856e-02 7.24117338e-01 8.14955413e-01\n",
      "  2.41685909e-01 1.75035281e-01 2.96059571e-02 9.32524729e-01\n",
      "  4.65674650e-01 9.79171702e-01 8.04203556e-01 1.61069999e-01\n",
      "  3.98148844e-01 1.59935181e-01 4.77530566e-01 2.24089746e-02\n",
      "  7.50474607e-01 8.18732345e-01 8.06218813e-01 5.60816617e-01\n",
      "  9.13322396e-01 8.93212218e-01 2.04387792e-01 3.99975947e-01\n",
      "  6.82213836e-01 4.78423517e-01 2.04224453e-01 8.36656632e-01\n",
      "  7.14374151e-01 2.42750696e-01 6.69398002e-01 4.88103429e-01\n",
      "  9.52273874e-01 2.82861515e-01 9.38249640e-01 7.98685177e-01\n",
      "  8.46974977e-01 8.50873613e-01 2.86937778e-03 2.92007703e-01\n",
      "  7.72450479e-01 7.34961813e-01 4.99529051e-01 9.60727279e-01\n",
      "  2.69299505e-01 7.67443888e-02 8.13095887e-01 9.14852745e-01\n",
      "  8.52982934e-01 1.87464673e-01 9.12960606e-01 8.93472458e-01\n",
      "  1.53606673e-01 8.52540068e-01 4.12793502e-01 6.47763859e-01\n",
      "  4.37671654e-01 6.53468499e-01 8.43434418e-01 2.65410196e-01\n",
      "  2.97436193e-01 7.01278782e-01 6.26221489e-01 1.01229768e-01\n",
      "  2.28365342e-01 5.24534633e-01 6.52709620e-01 7.56430124e-01\n",
      "  6.24633364e-01 4.88484826e-01 7.96431440e-01 9.54923575e-01\n",
      "  3.26724248e-01 2.17759794e-01 8.58664236e-01 2.18627926e-01\n",
      "  9.66932936e-01 8.53119900e-01 3.38972703e-01 7.39868056e-01\n",
      "  9.03363504e-01 7.61749297e-02 9.34552822e-01 5.81332481e-02\n",
      "  5.65206802e-01 5.48723400e-01 2.58714736e-01 4.96655864e-01\n",
      "  9.55197064e-01 6.37778424e-01 4.51730062e-01 8.25724633e-01\n",
      "  6.62394165e-01 7.54715267e-01 8.38832615e-01 4.09216156e-01\n",
      "  6.80232898e-01 2.08248736e-01 7.07926759e-02 2.82841269e-01\n",
      "  3.36615655e-01 5.06706263e-01 2.40693008e-01 9.35622362e-01\n",
      "  3.39901861e-02 4.14599348e-01 6.14998579e-01 5.42620625e-01\n",
      "  7.55466226e-01 2.23764538e-01 4.93283273e-01 9.54503155e-01\n",
      "  8.64507667e-02 6.86300696e-01 7.55479279e-01 1.94841958e-01\n",
      "  2.51700346e-01 2.73441513e-01 4.60034093e-01 3.83432627e-01\n",
      "  5.09571833e-01 6.18547847e-01 8.51103578e-01 2.35314031e-01\n",
      "  4.01878195e-01 6.53873655e-01 6.34532818e-01]]\n",
      "Change in theta: 1.905506296743062e-06\n",
      "---------------------------------------------\n",
      "Iteration number 25 finished, Running time 611.0557260513306, Roo2 iteration 25 = -6800709.034243452\n",
      "Iteration 1:\n",
      "Theta: [[6.82574799e-01 2.04523036e-01 3.44628619e-01 5.75118484e-01\n",
      "  1.67800215e-01 3.75649781e-01 1.16357163e-01 8.83589201e-01\n",
      "  5.22043761e-01 3.47342562e-01 8.63887436e-01 1.62104226e-01\n",
      "  1.26223712e-01 5.95407623e-01 9.41484401e-01 7.64166699e-01\n",
      "  5.77496736e-01 7.73456303e-01 1.63614049e-01 7.90844324e-01\n",
      "  9.35691016e-02 2.68798451e-01 3.84679243e-01 4.97088500e-01\n",
      "  4.94991468e-01 6.36576730e-01 3.65359585e-01 2.17184939e-01\n",
      "  4.22691158e-01 8.48222349e-01 8.36636728e-01 9.86257733e-01\n",
      "  4.69590200e-01 5.17769462e-01 1.62605229e-02 8.68939543e-01\n",
      "  6.89869132e-01 6.26075886e-01 4.87930637e-01 5.22124821e-01\n",
      "  4.13605977e-01 4.41757109e-01 7.98438160e-01 4.11091232e-01\n",
      "  7.66486455e-01 2.83451359e-01 6.01824541e-01 7.80451095e-02\n",
      "  9.99854482e-01 2.20902223e-01 7.66520632e-01 6.18849248e-01\n",
      "  6.52122102e-01 9.73280612e-01 3.28906764e-01 2.16797057e-02\n",
      "  3.20347108e-01 1.12522962e-01 5.44502069e-02 9.88056265e-01\n",
      "  6.05816534e-02 8.64992120e-01 4.53332895e-02 4.81478279e-01\n",
      "  7.91210252e-01 3.88685134e-01 4.32181018e-01 7.85725236e-02\n",
      "  1.92393466e-01 9.14166861e-01 7.07727357e-01 5.39098205e-02\n",
      "  8.76095045e-01 5.85051625e-01 5.29700169e-01 9.12787245e-01\n",
      "  7.89524590e-01 6.30190202e-01 9.55838249e-01 4.74090624e-01\n",
      "  4.29752405e-01 9.38229259e-01 6.08040687e-01 7.48823486e-01\n",
      "  5.09964584e-01 8.82830051e-01 9.07736521e-01 5.37810850e-01\n",
      "  5.87248959e-01 1.60273633e-01 8.63052621e-01 9.99105716e-02\n",
      "  9.15925575e-01 1.18105293e-01 5.79018541e-01 3.95718884e-01\n",
      "  2.48621462e-01 9.28312520e-01 2.19445024e-01 6.00847834e-02\n",
      "  9.26523563e-01 7.02472117e-01 4.80497045e-01 4.96210524e-01\n",
      "  6.64435420e-03 1.45518427e-01 4.15119396e-01 3.65164001e-02\n",
      "  7.29441343e-01 5.36883060e-01 3.39504025e-01 6.57800926e-01\n",
      "  1.25029969e-01 4.34491489e-01 8.53927063e-02 9.71366187e-01\n",
      "  2.27627244e-01 7.88513135e-01 8.18932193e-01 6.22108598e-01\n",
      "  9.20612819e-01 3.11789599e-01 1.56012314e-01 9.24313828e-01\n",
      "  5.82788346e-01 9.49798383e-01 3.92998750e-02 7.11921546e-01\n",
      "  5.12350060e-01 7.59410036e-01 8.43599987e-01 1.22674966e-01\n",
      "  4.76999133e-01 4.99927897e-01 3.46121624e-01 6.76130396e-01\n",
      "  1.99227268e-01 3.46426341e-01 7.67290590e-01 6.21687763e-02\n",
      "  4.65668252e-01 1.35890430e-01 2.75132740e-02 8.14448528e-01\n",
      "  5.12288200e-01 2.89725747e-01 2.26166730e-01 8.59433799e-01\n",
      "  8.31773896e-01 7.21033471e-01 2.36539036e-01 3.34414861e-01\n",
      "  5.19175311e-03 2.74527580e-01 9.88388342e-02 8.88446410e-01\n",
      "  6.15309701e-01 3.40357970e-01 8.85033453e-01 4.82986452e-01\n",
      "  6.52142757e-01 5.35473009e-01 8.15436556e-01 4.69734308e-01\n",
      "  9.84674845e-01 4.41140707e-02 8.85728051e-01 6.94571735e-01\n",
      "  8.33651592e-01 4.62206888e-01 6.11428129e-01 2.26863709e-01\n",
      "  7.93885202e-01 2.29917713e-01 4.08262756e-01 4.88010261e-01\n",
      "  3.36784166e-01 3.71374938e-01 6.59412290e-03 3.56354087e-01\n",
      "  7.18396661e-01 1.38386090e-01 9.48269022e-01 1.24280266e-01\n",
      "  8.18099488e-01 7.58733155e-01 5.16491413e-01 3.77495536e-01\n",
      "  1.32117135e-01 8.55643482e-01 9.48062279e-01 2.25141166e-01\n",
      "  5.16201012e-01 7.49111263e-01 6.40111291e-01 8.66498373e-01\n",
      "  1.47638781e-01 7.42608927e-01 6.45743756e-01 7.58025364e-01\n",
      "  8.00798696e-01 2.21042418e-01 6.04896719e-03 6.27651994e-01\n",
      "  2.51518862e-01 7.70842780e-01 1.97897836e-01 9.04974400e-01\n",
      "  8.38362131e-01 4.17781644e-01 7.56718452e-01 5.77383253e-01\n",
      "  9.48722547e-01 6.54360358e-01 5.91209187e-01 9.88765480e-01\n",
      "  5.36362917e-01 9.00715779e-01 1.28369017e-01 3.92142561e-01\n",
      "  9.80302401e-01 1.78506801e-01 5.28772987e-01 2.79126136e-01\n",
      "  8.17665872e-04 4.24811663e-01 3.87223626e-01 8.86489177e-01\n",
      "  3.16550973e-01 2.40263975e-01 1.78153374e-01 5.45051879e-01\n",
      "  9.52079399e-02 1.16803328e-01 4.73102088e-01 2.60931215e-02\n",
      "  9.36594900e-01 4.60460179e-01 9.32284913e-01 8.83191619e-01\n",
      "  6.57957297e-02 3.07766229e-01 1.89248766e-01 8.87249602e-01\n",
      "  6.49865369e-01 2.15843759e-01 1.20434815e-01 8.95820235e-01\n",
      "  9.55026998e-01 5.13471390e-01 2.84495177e-01 2.51497506e-01\n",
      "  6.40167551e-01 7.51372060e-01 3.00915454e-01 5.88150684e-01\n",
      "  4.87422640e-01 8.58464784e-01 1.42285853e-01 7.98698066e-01\n",
      "  2.73853720e-01 3.83841820e-01 8.75998909e-01 5.68032212e-01\n",
      "  9.93546249e-01 2.36377464e-01 1.41589643e-01 1.24075809e-01\n",
      "  5.27914296e-01 3.77271410e-01 1.40583708e-02 5.92229852e-01\n",
      "  2.26234298e-01 2.20738665e-01 9.22164914e-01 3.01009440e-01\n",
      "  8.47040808e-01 4.54766194e-01 7.00676805e-01 4.32779463e-01\n",
      "  2.82785289e-01 1.21304481e-02 9.12970543e-01 9.16902484e-02\n",
      "  4.80188801e-01 7.40669230e-01 1.07389142e-01 5.77284869e-01\n",
      "  2.73553476e-01 8.93725441e-01 8.18054712e-01 1.64506995e-01\n",
      "  3.99040367e-01 2.98322559e-01 1.90887364e-02 3.37458176e-01\n",
      "  8.73765408e-01 5.34797797e-01 1.41354075e-02 1.63438656e-01\n",
      "  7.28873383e-01 6.17560154e-01 5.02949308e-01 4.73832422e-01\n",
      "  9.99536455e-01 4.57527590e-02 1.05301382e-03 8.25583614e-01\n",
      "  3.13224414e-01 2.34296031e-02 5.74200650e-01 3.39243492e-01\n",
      "  2.72271119e-01 6.21530465e-01 7.02703802e-01 1.13287956e-01\n",
      "  4.60251357e-01 1.75229687e-01 8.24625199e-01 7.05599635e-02\n",
      "  6.04574316e-01 8.05313501e-01 9.78896028e-01 5.74008353e-01\n",
      "  5.04732259e-01 2.05056887e-01 2.04867403e-01 9.94132523e-01\n",
      "  4.67061393e-02 9.70120071e-01 9.73004973e-01 1.93460765e-01\n",
      "  9.06031922e-01 1.03418199e-01 2.73920554e-01 1.36032527e-01\n",
      "  2.00724783e-01 5.62377415e-01 6.69035397e-01 8.94450149e-01\n",
      "  1.83291205e-01 6.23320285e-01 3.17597333e-01 6.02594215e-01\n",
      "  1.42341203e-01 4.40905937e-01 1.35388655e-01 4.46421092e-01\n",
      "  8.82994902e-01 7.66579803e-01 8.33557460e-01 6.51920498e-01\n",
      "  4.96796554e-01 1.96715072e-01 4.71320424e-01 8.09279265e-01\n",
      "  9.68957190e-01 1.04469637e-01 9.82767799e-02 5.16566376e-02\n",
      "  5.56998789e-01 3.29733658e-01 4.34734588e-01 7.55833064e-01\n",
      "  8.99339446e-01 7.09527837e-01 2.51640034e-01 7.89586365e-01\n",
      "  1.51675080e-01 2.22350200e-01 2.34432321e-01 5.26803604e-01\n",
      "  7.36720122e-01 4.32955467e-01 2.72609073e-02 7.41960145e-01\n",
      "  5.81491733e-01 6.78601492e-01 3.52228308e-01 3.95200826e-01\n",
      "  2.58023532e-01 9.22854884e-01 5.25845230e-01 8.64688076e-01\n",
      "  8.70300778e-02 6.75853239e-01 1.13519683e-01 8.76543055e-01\n",
      "  8.34091898e-01 2.01396502e-01 1.08226354e-01 4.38360826e-01\n",
      "  2.80599149e-01 4.26079770e-01 7.02975565e-01 6.32363536e-01\n",
      "  3.66941970e-01 3.11667538e-01 3.82076964e-01 7.29478638e-01\n",
      "  9.69883854e-03 6.19879611e-02 3.53990895e-01 2.69965753e-01\n",
      "  9.66150355e-01 7.30329258e-01 9.43747953e-01 7.51398589e-01\n",
      "  5.49232094e-01 5.10253559e-01 6.98477468e-01 5.46679784e-01\n",
      "  4.81243080e-01 5.08644496e-01 1.50956179e-01 3.67524259e-01\n",
      "  8.24299327e-01 9.09534034e-01 9.44643962e-02 8.25108581e-01\n",
      "  2.40397089e-01 9.05179167e-01 3.45904527e-01 2.32408269e-01\n",
      "  2.50717775e-01 4.80710090e-02 9.59477123e-01 1.43134558e-01\n",
      "  9.60571551e-01 6.71632339e-01 3.57143700e-01 9.39047589e-01\n",
      "  3.73592478e-01 2.37526746e-01 1.69446719e-01 7.89374559e-01\n",
      "  1.27684706e-01 6.36394929e-01 2.12397865e-01 7.46967469e-01\n",
      "  6.92354538e-01 9.55250164e-01 1.11659001e-01 5.90113953e-01\n",
      "  7.51921324e-02 6.89098224e-01 1.23331301e-01 8.00806584e-01\n",
      "  4.69707112e-01 3.68753923e-01 5.13026940e-01 9.94911482e-01\n",
      "  9.26169513e-01 5.79565105e-01 5.40857227e-01 9.24876873e-01\n",
      "  4.96935696e-01 3.09739822e-01 9.76925734e-01 8.82781544e-01\n",
      "  8.42212032e-01 5.27680935e-02 5.77241934e-01 9.42874846e-01\n",
      "  9.52963475e-01 8.25974500e-02 9.22208064e-01 9.69209817e-02\n",
      "  6.48956405e-01 9.51239698e-01 1.90832943e-01 5.94356273e-01\n",
      "  8.19267032e-01 8.25588031e-01 9.12820812e-01 7.80937742e-01\n",
      "  1.40324285e-01 3.04267054e-01 1.01068759e-01 2.08816157e-01\n",
      "  8.76161227e-01 5.92045037e-01 6.43812977e-02 6.72097092e-02\n",
      "  8.63120571e-02 2.25005112e-01 6.52609154e-01 6.14434479e-01\n",
      "  8.17668166e-01 4.13473718e-01 5.53462957e-01 9.86160123e-01\n",
      "  9.98206036e-01 7.21735866e-01 6.89881806e-01 1.79588391e-01\n",
      "  3.67415474e-01 5.29402832e-01 9.16703490e-01 2.23359587e-01\n",
      "  1.03078278e-01 7.59485960e-01 5.64297994e-01 4.23059432e-01\n",
      "  6.15092011e-01 2.35914354e-01 8.00336173e-01 9.25106809e-01\n",
      "  8.50865054e-01 5.93770178e-01 1.46465725e-02 9.47515531e-01\n",
      "  1.23542179e-01 6.63007038e-01 5.01392476e-01 1.08923357e-01\n",
      "  1.02241816e-01 7.68052314e-01 4.78185842e-01 7.49532701e-02\n",
      "  6.96289688e-01 8.02057523e-02 4.29919100e-01 5.64907104e-01\n",
      "  3.91940954e-01 5.87261325e-01 4.26965565e-01 4.31610656e-01\n",
      "  6.36837891e-02 3.29837154e-01 1.59524016e-01 7.22780795e-01\n",
      "  1.45320081e-01 3.38390444e-01 8.64593778e-01 7.93636576e-02\n",
      "  8.82648577e-01 1.26561068e-01 4.54621890e-01 7.47074357e-01\n",
      "  8.38649334e-01 7.88241415e-01 4.87982121e-01 1.02034663e-01\n",
      "  7.53825317e-01 8.80504196e-01 5.26668507e-01 1.70832763e-01\n",
      "  4.59579739e-01 9.68491812e-01 3.58482093e-01 5.99295582e-01\n",
      "  9.17981552e-01 7.24717778e-01 1.48540924e-01 5.61828791e-01\n",
      "  8.79343998e-01 3.57900069e-01 9.23142743e-01 3.06277170e-01\n",
      "  1.99896067e-01 5.77202494e-01 5.64177441e-01 6.50676833e-01\n",
      "  8.99024541e-01 9.87217287e-01 1.40563567e-01 4.20669791e-01\n",
      "  8.65571192e-01 4.93328180e-01 8.36052300e-02 7.62235364e-01\n",
      "  8.09695605e-01 7.33152216e-01 8.53778015e-01 7.27791161e-01\n",
      "  7.21596955e-01 5.75476299e-01 1.85635231e-01 7.60941762e-01\n",
      "  8.68989019e-01 9.52527230e-01 1.98960625e-01 7.96139068e-01\n",
      "  7.70848457e-02 9.90882680e-01 1.97836361e-01 4.74455978e-02\n",
      "  4.47470962e-01 7.70386560e-01 3.76060963e-01 8.45132812e-01\n",
      "  3.03900396e-01 2.27218834e-01 7.00401592e-02 3.80355953e-01\n",
      "  7.33156133e-02 7.45900174e-01 9.68577226e-01 1.31822273e-01\n",
      "  1.21146074e-03 3.14204547e-01 4.05175838e-01 8.80299964e-01\n",
      "  6.47540089e-01 1.54626711e-01 1.79618147e-01 4.07199599e-01\n",
      "  6.54980221e-01 5.79013893e-01 2.89477263e-01 9.09078250e-01\n",
      "  9.27796129e-01 9.15115048e-01 1.25444876e-01 1.68089888e-01\n",
      "  5.24880902e-01 6.45078208e-01 5.74243430e-02 8.80104199e-01\n",
      "  4.66111110e-01 4.75795455e-01 9.24948251e-01 5.30052373e-01\n",
      "  3.95271623e-01 7.50718895e-01 2.51713509e-01 9.60989036e-01\n",
      "  6.68699671e-01 8.39522199e-01 9.94642491e-01 7.65509834e-01\n",
      "  5.10789872e-01 3.75965054e-01 4.99000109e-01 5.77286742e-02\n",
      "  7.28733426e-01 7.74328693e-01 7.94852944e-01 4.15578199e-01\n",
      "  4.42386455e-01 2.55213384e-01 6.40363339e-01 3.30230181e-01\n",
      "  1.65404585e-01 3.51087374e-01 6.43734823e-01 4.72091880e-01\n",
      "  2.44011942e-01 4.43201580e-01 5.10145488e-01 4.09463489e-01\n",
      "  2.07559178e-01 8.90967315e-01 9.23905707e-01 6.80916254e-01\n",
      "  1.56221475e-01 3.89122045e-01 7.27522045e-01 6.75598478e-01\n",
      "  8.96700727e-01 2.88811182e-01 5.00049859e-01 7.30119833e-01\n",
      "  1.90255907e-01 5.63508623e-01 2.45779953e-01 3.42290117e-01\n",
      "  4.26176640e-01 8.07079658e-01 4.93457416e-01 8.56642470e-02\n",
      "  7.99071066e-01 3.11807633e-01 7.64813421e-01 7.40975630e-01\n",
      "  4.86191035e-01 1.69102719e-01 4.74846346e-01 3.51404069e-01\n",
      "  9.34621227e-01 9.79417339e-01 3.23967891e-01 4.71716955e-01\n",
      "  6.93365310e-02 9.35393573e-01 5.14907383e-01 1.31701909e-01\n",
      "  2.70940781e-01 1.35229379e-01 9.95837500e-01 1.72705184e-01\n",
      "  4.46681756e-01 4.99669022e-01 8.38713117e-01 2.13996919e-01\n",
      "  9.03478516e-01 9.88386268e-01 8.00239042e-01 5.48060631e-01\n",
      "  5.24024854e-01 5.01594385e-01 1.10481832e-01 8.55223454e-01\n",
      "  8.68575779e-01 9.84733513e-01 7.90515723e-01 5.71469914e-01\n",
      "  2.88337006e-01 9.33101229e-01 8.21235522e-01 6.92288838e-01\n",
      "  2.08973706e-01 5.16272178e-01 7.86253438e-02 2.31583623e-01\n",
      "  9.92603816e-01 6.36648129e-01 2.47828958e-01 8.49988262e-01\n",
      "  3.60312807e-01 1.71887171e-01 1.25641614e-01 4.33522585e-01\n",
      "  3.68703114e-01 9.46486053e-01 7.36790730e-01 3.60603507e-01\n",
      "  8.07358734e-01 3.95648043e-01 4.06503194e-01 7.38389746e-01\n",
      "  8.54094041e-01 6.67346048e-01 7.23899219e-01 1.74531205e-01\n",
      "  9.23467663e-02 8.46754353e-01 3.91486361e-01 8.22162618e-01\n",
      "  2.63939802e-01 4.67600076e-01 2.50136132e-01 5.23067173e-01\n",
      "  2.45276829e-01 5.58069280e-01 1.28891345e-01 2.39353856e-02\n",
      "  3.29897248e-01 8.42650641e-01 4.12462068e-01 7.62081492e-01\n",
      "  1.83394569e-01 3.04329297e-01 5.46357596e-01 7.63157593e-01\n",
      "  8.09632824e-01 2.56458981e-01 9.98524149e-01 7.98315736e-02\n",
      "  7.79709595e-01 8.79006186e-01 6.58054516e-01 9.70740202e-01\n",
      "  1.45861213e-01 9.45154909e-01 5.79531909e-02 4.42632045e-01\n",
      "  7.67008054e-01 7.57392497e-02 1.64861153e-01 6.43936200e-01\n",
      "  5.57355632e-01 8.70987838e-01 2.54183355e-01 3.17578173e-02\n",
      "  3.30298256e-01 4.70759681e-02 8.03733634e-01 2.99279644e-01\n",
      "  7.07830825e-01 3.52637435e-01 3.39161101e-01 2.17997724e-01\n",
      "  3.35997924e-01 1.39120788e-02 7.24117531e-01 8.14955606e-01\n",
      "  2.41686102e-01 1.75035474e-01 2.96061503e-02 9.32524923e-01\n",
      "  4.65674843e-01 9.79171895e-01 8.04203749e-01 1.61070193e-01\n",
      "  3.98149038e-01 1.59935374e-01 4.77530760e-01 2.24091679e-02\n",
      "  7.50474801e-01 8.18732539e-01 8.06219006e-01 5.60816811e-01\n",
      "  9.13322590e-01 8.93212412e-01 2.04387985e-01 3.99976140e-01\n",
      "  6.82214029e-01 4.78423710e-01 2.04224646e-01 8.36656825e-01\n",
      "  7.14374345e-01 2.42750890e-01 6.69398195e-01 4.88103622e-01\n",
      "  9.52274067e-01 2.82861709e-01 9.38249833e-01 7.98685370e-01\n",
      "  8.46975170e-01 8.50873806e-01 2.86957103e-03 2.92007896e-01\n",
      "  7.72450672e-01 7.34962006e-01 4.99529244e-01 9.60727472e-01\n",
      "  2.69299699e-01 7.67445820e-02 8.13096081e-01 9.14852938e-01\n",
      "  8.52983128e-01 1.87464866e-01 9.12960799e-01 8.93472652e-01\n",
      "  1.53606866e-01 8.52540261e-01 4.12793696e-01 6.47764052e-01\n",
      "  4.37671848e-01 6.53468693e-01 8.43434612e-01 2.65410389e-01\n",
      "  2.97436386e-01 7.01278975e-01 6.26221683e-01 1.01229961e-01\n",
      "  2.28365535e-01 5.24534826e-01 6.52709814e-01 7.56430318e-01\n",
      "  6.24633557e-01 4.88485019e-01 7.96431633e-01 9.54923768e-01\n",
      "  3.26724442e-01 2.17759988e-01 8.58664430e-01 2.18628120e-01\n",
      "  9.66933129e-01 8.53120093e-01 3.38972896e-01 7.39868249e-01\n",
      "  9.03363697e-01 7.61751229e-02 9.34553016e-01 5.81334414e-02\n",
      "  5.65206995e-01 5.48723593e-01 2.58714929e-01 4.96656057e-01\n",
      "  9.55197258e-01 6.37778617e-01 4.51730255e-01 8.25724826e-01\n",
      "  6.62394359e-01 7.54715461e-01 8.38832808e-01 4.09216350e-01\n",
      "  6.80233091e-01 2.08248929e-01 7.07928691e-02 2.82841462e-01\n",
      "  3.36615848e-01 5.06706457e-01 2.40693201e-01 9.35622555e-01\n",
      "  3.39903794e-02 4.14599542e-01 6.14998772e-01 5.42620818e-01\n",
      "  7.55466419e-01 2.23764731e-01 4.93283466e-01 9.54503349e-01\n",
      "  8.64509599e-02 6.86300889e-01 7.55479473e-01 1.94842151e-01\n",
      "  2.51700539e-01 2.73441706e-01 4.60034286e-01 3.83432820e-01\n",
      "  5.09572026e-01 6.18548040e-01 8.51103771e-01 2.35314224e-01\n",
      "  4.01878388e-01 6.53873848e-01 6.34533011e-01]]\n",
      "Change in theta: 1.9216433104576834e-06\n",
      "---------------------------------------------\n",
      "Iteration number 26 finished, Running time 855.5039350986481, Roo2 iteration 26 = -15494361.409415636\n",
      "Iteration 1:\n",
      "Theta: [[6.82574946e-01 2.04523184e-01 3.44628767e-01 5.75118631e-01\n",
      "  1.67800362e-01 3.75649928e-01 1.16357310e-01 8.83589349e-01\n",
      "  5.22043909e-01 3.47342709e-01 8.63887584e-01 1.62104373e-01\n",
      "  1.26223859e-01 5.95407770e-01 9.41484548e-01 7.64166847e-01\n",
      "  5.77496883e-01 7.73456451e-01 1.63614197e-01 7.90844471e-01\n",
      "  9.35692492e-02 2.68798598e-01 3.84679391e-01 4.97088647e-01\n",
      "  4.94991615e-01 6.36576877e-01 3.65359733e-01 2.17185086e-01\n",
      "  4.22691305e-01 8.48222497e-01 8.36636876e-01 9.86257880e-01\n",
      "  4.69590348e-01 5.17769609e-01 1.62606705e-02 8.68939691e-01\n",
      "  6.89869280e-01 6.26076034e-01 4.87930784e-01 5.22124969e-01\n",
      "  4.13606125e-01 4.41757256e-01 7.98438308e-01 4.11091380e-01\n",
      "  7.66486603e-01 2.83451507e-01 6.01824689e-01 7.80452570e-02\n",
      "  9.99854629e-01 2.20902370e-01 7.66520780e-01 6.18849395e-01\n",
      "  6.52122250e-01 9.73280759e-01 3.28906912e-01 2.16798532e-02\n",
      "  3.20347256e-01 1.12523109e-01 5.44503544e-02 9.88056413e-01\n",
      "  6.05818009e-02 8.64992267e-01 4.53334371e-02 4.81478427e-01\n",
      "  7.91210400e-01 3.88685281e-01 4.32181165e-01 7.85726711e-02\n",
      "  1.92393614e-01 9.14167009e-01 7.07727504e-01 5.39099681e-02\n",
      "  8.76095193e-01 5.85051772e-01 5.29700316e-01 9.12787392e-01\n",
      "  7.89524737e-01 6.30190349e-01 9.55838397e-01 4.74090771e-01\n",
      "  4.29752553e-01 9.38229407e-01 6.08040834e-01 7.48823633e-01\n",
      "  5.09964731e-01 8.82830199e-01 9.07736668e-01 5.37810997e-01\n",
      "  5.87249106e-01 1.60273781e-01 8.63052768e-01 9.99107191e-02\n",
      "  9.15925723e-01 1.18105441e-01 5.79018688e-01 3.95719031e-01\n",
      "  2.48621610e-01 9.28312668e-01 2.19445172e-01 6.00849309e-02\n",
      "  9.26523711e-01 7.02472265e-01 4.80497192e-01 4.96210672e-01\n",
      "  6.64450173e-03 1.45518574e-01 4.15119544e-01 3.65165477e-02\n",
      "  7.29441491e-01 5.36883208e-01 3.39504173e-01 6.57801074e-01\n",
      "  1.25030117e-01 4.34491637e-01 8.53928539e-02 9.71366335e-01\n",
      "  2.27627392e-01 7.88513282e-01 8.18932340e-01 6.22108746e-01\n",
      "  9.20612966e-01 3.11789747e-01 1.56012462e-01 9.24313975e-01\n",
      "  5.82788494e-01 9.49798531e-01 3.93000225e-02 7.11921693e-01\n",
      "  5.12350208e-01 7.59410184e-01 8.43600134e-01 1.22675113e-01\n",
      "  4.76999281e-01 4.99928044e-01 3.46121772e-01 6.76130544e-01\n",
      "  1.99227415e-01 3.46426489e-01 7.67290738e-01 6.21689239e-02\n",
      "  4.65668399e-01 1.35890578e-01 2.75134216e-02 8.14448675e-01\n",
      "  5.12288347e-01 2.89725895e-01 2.26166878e-01 8.59433946e-01\n",
      "  8.31774043e-01 7.21033619e-01 2.36539184e-01 3.34415008e-01\n",
      "  5.19190064e-03 2.74527727e-01 9.88389817e-02 8.88446558e-01\n",
      "  6.15309849e-01 3.40358118e-01 8.85033600e-01 4.82986600e-01\n",
      "  6.52142905e-01 5.35473156e-01 8.15436704e-01 4.69734455e-01\n",
      "  9.84674993e-01 4.41142183e-02 8.85728199e-01 6.94571882e-01\n",
      "  8.33651739e-01 4.62207035e-01 6.11428276e-01 2.26863857e-01\n",
      "  7.93885349e-01 2.29917861e-01 4.08262904e-01 4.88010409e-01\n",
      "  3.36784313e-01 3.71375086e-01 6.59427044e-03 3.56354235e-01\n",
      "  7.18396809e-01 1.38386237e-01 9.48269170e-01 1.24280414e-01\n",
      "  8.18099635e-01 7.58733303e-01 5.16491560e-01 3.77495684e-01\n",
      "  1.32117282e-01 8.55643630e-01 9.48062427e-01 2.25141313e-01\n",
      "  5.16201160e-01 7.49111410e-01 6.40111438e-01 8.66498520e-01\n",
      "  1.47638928e-01 7.42609074e-01 6.45743904e-01 7.58025511e-01\n",
      "  8.00798843e-01 2.21042566e-01 6.04911472e-03 6.27652141e-01\n",
      "  2.51519010e-01 7.70842927e-01 1.97897983e-01 9.04974547e-01\n",
      "  8.38362279e-01 4.17781791e-01 7.56718600e-01 5.77383400e-01\n",
      "  9.48722694e-01 6.54360505e-01 5.91209334e-01 9.88765628e-01\n",
      "  5.36363064e-01 9.00715927e-01 1.28369164e-01 3.92142709e-01\n",
      "  9.80302548e-01 1.78506949e-01 5.28773134e-01 2.79126284e-01\n",
      "  8.17813409e-04 4.24811811e-01 3.87223773e-01 8.86489324e-01\n",
      "  3.16551121e-01 2.40264123e-01 1.78153521e-01 5.45052026e-01\n",
      "  9.52080874e-02 1.16803476e-01 4.73102235e-01 2.60932691e-02\n",
      "  9.36595048e-01 4.60460327e-01 9.32285061e-01 8.83191767e-01\n",
      "  6.57958772e-02 3.07766376e-01 1.89248914e-01 8.87249750e-01\n",
      "  6.49865517e-01 2.15843907e-01 1.20434963e-01 8.95820383e-01\n",
      "  9.55027145e-01 5.13471538e-01 2.84495325e-01 2.51497653e-01\n",
      "  6.40167698e-01 7.51372208e-01 3.00915601e-01 5.88150832e-01\n",
      "  4.87422788e-01 8.58464932e-01 1.42286000e-01 7.98698214e-01\n",
      "  2.73853868e-01 3.83841967e-01 8.75999057e-01 5.68032360e-01\n",
      "  9.93546397e-01 2.36377612e-01 1.41589790e-01 1.24075956e-01\n",
      "  5.27914444e-01 3.77271557e-01 1.40585184e-02 5.92230000e-01\n",
      "  2.26234446e-01 2.20738812e-01 9.22165061e-01 3.01009587e-01\n",
      "  8.47040956e-01 4.54766341e-01 7.00676952e-01 4.32779611e-01\n",
      "  2.82785437e-01 1.21305957e-02 9.12970690e-01 9.16903959e-02\n",
      "  4.80188949e-01 7.40669378e-01 1.07389289e-01 5.77285017e-01\n",
      "  2.73553624e-01 8.93725589e-01 8.18054860e-01 1.64507143e-01\n",
      "  3.99040514e-01 2.98322707e-01 1.90888839e-02 3.37458324e-01\n",
      "  8.73765556e-01 5.34797944e-01 1.41355551e-02 1.63438804e-01\n",
      "  7.28873531e-01 6.17560301e-01 5.02949456e-01 4.73832569e-01\n",
      "  9.99536602e-01 4.57529066e-02 1.05316135e-03 8.25583761e-01\n",
      "  3.13224562e-01 2.34297506e-02 5.74200798e-01 3.39243640e-01\n",
      "  2.72271267e-01 6.21530612e-01 7.02703949e-01 1.13288104e-01\n",
      "  4.60251505e-01 1.75229834e-01 8.24625346e-01 7.05601110e-02\n",
      "  6.04574464e-01 8.05313649e-01 9.78896176e-01 5.74008501e-01\n",
      "  5.04732407e-01 2.05057034e-01 2.04867551e-01 9.94132671e-01\n",
      "  4.67062868e-02 9.70120218e-01 9.73005121e-01 1.93460912e-01\n",
      "  9.06032069e-01 1.03418347e-01 2.73920702e-01 1.36032675e-01\n",
      "  2.00724931e-01 5.62377562e-01 6.69035544e-01 8.94450296e-01\n",
      "  1.83291353e-01 6.23320433e-01 3.17597480e-01 6.02594362e-01\n",
      "  1.42341351e-01 4.40906085e-01 1.35388802e-01 4.46421240e-01\n",
      "  8.82995049e-01 7.66579950e-01 8.33557607e-01 6.51920645e-01\n",
      "  4.96796702e-01 1.96715219e-01 4.71320572e-01 8.09279412e-01\n",
      "  9.68957338e-01 1.04469785e-01 9.82769275e-02 5.16567851e-02\n",
      "  5.56998937e-01 3.29733806e-01 4.34734736e-01 7.55833212e-01\n",
      "  8.99339594e-01 7.09527985e-01 2.51640182e-01 7.89586513e-01\n",
      "  1.51675228e-01 2.22350348e-01 2.34432469e-01 5.26803752e-01\n",
      "  7.36720269e-01 4.32955615e-01 2.72610549e-02 7.41960292e-01\n",
      "  5.81491881e-01 6.78601639e-01 3.52228456e-01 3.95200973e-01\n",
      "  2.58023679e-01 9.22855032e-01 5.25845377e-01 8.64688224e-01\n",
      "  8.70302253e-02 6.75853386e-01 1.13519831e-01 8.76543203e-01\n",
      "  8.34092045e-01 2.01396649e-01 1.08226501e-01 4.38360973e-01\n",
      "  2.80599296e-01 4.26079917e-01 7.02975712e-01 6.32363684e-01\n",
      "  3.66942117e-01 3.11667686e-01 3.82077111e-01 7.29478785e-01\n",
      "  9.69898608e-03 6.19881086e-02 3.53991043e-01 2.69965901e-01\n",
      "  9.66150503e-01 7.30329406e-01 9.43748100e-01 7.51398736e-01\n",
      "  5.49232242e-01 5.10253706e-01 6.98477615e-01 5.46679931e-01\n",
      "  4.81243228e-01 5.08644644e-01 1.50956326e-01 3.67524406e-01\n",
      "  8.24299475e-01 9.09534181e-01 9.44645437e-02 8.25108729e-01\n",
      "  2.40397237e-01 9.05179314e-01 3.45904675e-01 2.32408417e-01\n",
      "  2.50717922e-01 4.80711565e-02 9.59477270e-01 1.43134706e-01\n",
      "  9.60571699e-01 6.71632487e-01 3.57143848e-01 9.39047737e-01\n",
      "  3.73592625e-01 2.37526894e-01 1.69446866e-01 7.89374707e-01\n",
      "  1.27684854e-01 6.36395076e-01 2.12398013e-01 7.46967617e-01\n",
      "  6.92354686e-01 9.55250311e-01 1.11659149e-01 5.90114100e-01\n",
      "  7.51922800e-02 6.89098372e-01 1.23331449e-01 8.00806731e-01\n",
      "  4.69707259e-01 3.68754071e-01 5.13027088e-01 9.94911630e-01\n",
      "  9.26169660e-01 5.79565253e-01 5.40857374e-01 9.24877020e-01\n",
      "  4.96935843e-01 3.09739969e-01 9.76925881e-01 8.82781692e-01\n",
      "  8.42212180e-01 5.27682410e-02 5.77242081e-01 9.42874994e-01\n",
      "  9.52963623e-01 8.25975976e-02 9.22208212e-01 9.69211292e-02\n",
      "  6.48956553e-01 9.51239845e-01 1.90833090e-01 5.94356421e-01\n",
      "  8.19267179e-01 8.25588179e-01 9.12820960e-01 7.80937889e-01\n",
      "  1.40324432e-01 3.04267202e-01 1.01068906e-01 2.08816305e-01\n",
      "  8.76161374e-01 5.92045184e-01 6.43814453e-02 6.72098567e-02\n",
      "  8.63122046e-02 2.25005260e-01 6.52609302e-01 6.14434627e-01\n",
      "  8.17668313e-01 4.13473866e-01 5.53463104e-01 9.86160271e-01\n",
      "  9.98206184e-01 7.21736013e-01 6.89881954e-01 1.79588539e-01\n",
      "  3.67415622e-01 5.29402980e-01 9.16703638e-01 2.23359735e-01\n",
      "  1.03078425e-01 7.59486108e-01 5.64298142e-01 4.23059580e-01\n",
      "  6.15092158e-01 2.35914502e-01 8.00336321e-01 9.25106956e-01\n",
      "  8.50865202e-01 5.93770325e-01 1.46467201e-02 9.47515679e-01\n",
      "  1.23542326e-01 6.63007186e-01 5.01392623e-01 1.08923504e-01\n",
      "  1.02241964e-01 7.68052461e-01 4.78185990e-01 7.49534177e-02\n",
      "  6.96289836e-01 8.02058999e-02 4.29919248e-01 5.64907251e-01\n",
      "  3.91941102e-01 5.87261473e-01 4.26965712e-01 4.31610803e-01\n",
      "  6.36839366e-02 3.29837302e-01 1.59524164e-01 7.22780943e-01\n",
      "  1.45320229e-01 3.38390591e-01 8.64593925e-01 7.93638051e-02\n",
      "  8.82648724e-01 1.26561216e-01 4.54622038e-01 7.47074504e-01\n",
      "  8.38649482e-01 7.88241563e-01 4.87982268e-01 1.02034811e-01\n",
      "  7.53825464e-01 8.80504343e-01 5.26668655e-01 1.70832911e-01\n",
      "  4.59579887e-01 9.68491959e-01 3.58482241e-01 5.99295730e-01\n",
      "  9.17981699e-01 7.24717925e-01 1.48541071e-01 5.61828938e-01\n",
      "  8.79344145e-01 3.57900216e-01 9.23142891e-01 3.06277317e-01\n",
      "  1.99896214e-01 5.77202642e-01 5.64177588e-01 6.50676981e-01\n",
      "  8.99024689e-01 9.87217435e-01 1.40563715e-01 4.20669939e-01\n",
      "  8.65571340e-01 4.93328327e-01 8.36053776e-02 7.62235511e-01\n",
      "  8.09695753e-01 7.33152363e-01 8.53778162e-01 7.27791309e-01\n",
      "  7.21597102e-01 5.75476447e-01 1.85635379e-01 7.60941909e-01\n",
      "  8.68989166e-01 9.52527378e-01 1.98960772e-01 7.96139215e-01\n",
      "  7.70849933e-02 9.90882827e-01 1.97836508e-01 4.74457453e-02\n",
      "  4.47471110e-01 7.70386707e-01 3.76061111e-01 8.45132960e-01\n",
      "  3.03900543e-01 2.27218981e-01 7.00403067e-02 3.80356101e-01\n",
      "  7.33157609e-02 7.45900322e-01 9.68577374e-01 1.31822420e-01\n",
      "  1.21160827e-03 3.14204694e-01 4.05175986e-01 8.80300112e-01\n",
      "  6.47540237e-01 1.54626858e-01 1.79618294e-01 4.07199747e-01\n",
      "  6.54980368e-01 5.79014041e-01 2.89477411e-01 9.09078398e-01\n",
      "  9.27796276e-01 9.15115196e-01 1.25445024e-01 1.68090035e-01\n",
      "  5.24881049e-01 6.45078356e-01 5.74244906e-02 8.80104347e-01\n",
      "  4.66111258e-01 4.75795602e-01 9.24948399e-01 5.30052520e-01\n",
      "  3.95271770e-01 7.50719043e-01 2.51713657e-01 9.60989184e-01\n",
      "  6.68699819e-01 8.39522347e-01 9.94642639e-01 7.65509981e-01\n",
      "  5.10790020e-01 3.75965202e-01 4.99000257e-01 5.77288218e-02\n",
      "  7.28733574e-01 7.74328841e-01 7.94853092e-01 4.15578347e-01\n",
      "  4.42386602e-01 2.55213531e-01 6.40363487e-01 3.30230328e-01\n",
      "  1.65404733e-01 3.51087521e-01 6.43734970e-01 4.72092028e-01\n",
      "  2.44012090e-01 4.43201727e-01 5.10145636e-01 4.09463637e-01\n",
      "  2.07559325e-01 8.90967463e-01 9.23905855e-01 6.80916401e-01\n",
      "  1.56221622e-01 3.89122193e-01 7.27522193e-01 6.75598625e-01\n",
      "  8.96700874e-01 2.88811330e-01 5.00050006e-01 7.30119980e-01\n",
      "  1.90256055e-01 5.63508770e-01 2.45780101e-01 3.42290265e-01\n",
      "  4.26176788e-01 8.07079805e-01 4.93457564e-01 8.56643946e-02\n",
      "  7.99071213e-01 3.11807780e-01 7.64813569e-01 7.40975778e-01\n",
      "  4.86191182e-01 1.69102866e-01 4.74846494e-01 3.51404216e-01\n",
      "  9.34621375e-01 9.79417487e-01 3.23968038e-01 4.71717102e-01\n",
      "  6.93366785e-02 9.35393721e-01 5.14907531e-01 1.31702057e-01\n",
      "  2.70940929e-01 1.35229527e-01 9.95837648e-01 1.72705331e-01\n",
      "  4.46681904e-01 4.99669170e-01 8.38713264e-01 2.13997066e-01\n",
      "  9.03478664e-01 9.88386415e-01 8.00239189e-01 5.48060779e-01\n",
      "  5.24025002e-01 5.01594532e-01 1.10481979e-01 8.55223601e-01\n",
      "  8.68575926e-01 9.84733661e-01 7.90515870e-01 5.71470061e-01\n",
      "  2.88337153e-01 9.33101377e-01 8.21235670e-01 6.92288985e-01\n",
      "  2.08973853e-01 5.16272326e-01 7.86254913e-02 2.31583771e-01\n",
      "  9.92603964e-01 6.36648277e-01 2.47829105e-01 8.49988410e-01\n",
      "  3.60312954e-01 1.71887318e-01 1.25641762e-01 4.33522732e-01\n",
      "  3.68703261e-01 9.46486200e-01 7.36790878e-01 3.60603654e-01\n",
      "  8.07358881e-01 3.95648191e-01 4.06503342e-01 7.38389894e-01\n",
      "  8.54094188e-01 6.67346196e-01 7.23899367e-01 1.74531353e-01\n",
      "  9.23469138e-02 8.46754501e-01 3.91486509e-01 8.22162766e-01\n",
      "  2.63939950e-01 4.67600223e-01 2.50136280e-01 5.23067321e-01\n",
      "  2.45276977e-01 5.58069427e-01 1.28891492e-01 2.39355332e-02\n",
      "  3.29897396e-01 8.42650789e-01 4.12462216e-01 7.62081640e-01\n",
      "  1.83394717e-01 3.04329444e-01 5.46357744e-01 7.63157741e-01\n",
      "  8.09632971e-01 2.56459129e-01 9.98524296e-01 7.98317211e-02\n",
      "  7.79709743e-01 8.79006334e-01 6.58054663e-01 9.70740350e-01\n",
      "  1.45861360e-01 9.45155057e-01 5.79533384e-02 4.42632192e-01\n",
      "  7.67008202e-01 7.57393972e-02 1.64861301e-01 6.43936348e-01\n",
      "  5.57355779e-01 8.70987986e-01 2.54183502e-01 3.17579649e-02\n",
      "  3.30298403e-01 4.70761156e-02 8.03733781e-01 2.99279792e-01\n",
      "  7.07830973e-01 3.52637582e-01 3.39161248e-01 2.17997871e-01\n",
      "  3.35998072e-01 1.39122264e-02 7.24117679e-01 8.14955754e-01\n",
      "  2.41686250e-01 1.75035622e-01 2.96062978e-02 9.32525070e-01\n",
      "  4.65674991e-01 9.79172043e-01 8.04203897e-01 1.61070340e-01\n",
      "  3.98149185e-01 1.59935521e-01 4.77530907e-01 2.24093154e-02\n",
      "  7.50474948e-01 8.18732686e-01 8.06219154e-01 5.60816958e-01\n",
      "  9.13322737e-01 8.93212559e-01 2.04388132e-01 3.99976287e-01\n",
      "  6.82214177e-01 4.78423858e-01 2.04224794e-01 8.36656973e-01\n",
      "  7.14374492e-01 2.42751037e-01 6.69398342e-01 4.88103769e-01\n",
      "  9.52274215e-01 2.82861856e-01 9.38249981e-01 7.98685517e-01\n",
      "  8.46975318e-01 8.50873954e-01 2.86971857e-03 2.92008044e-01\n",
      "  7.72450820e-01 7.34962154e-01 4.99529392e-01 9.60727620e-01\n",
      "  2.69299846e-01 7.67447296e-02 8.13096228e-01 9.14853086e-01\n",
      "  8.52983275e-01 1.87465013e-01 9.12960947e-01 8.93472799e-01\n",
      "  1.53607013e-01 8.52540408e-01 4.12793843e-01 6.47764200e-01\n",
      "  4.37671995e-01 6.53468840e-01 8.43434759e-01 2.65410536e-01\n",
      "  2.97436534e-01 7.01279122e-01 6.26221830e-01 1.01230109e-01\n",
      "  2.28365683e-01 5.24534973e-01 6.52709961e-01 7.56430465e-01\n",
      "  6.24633704e-01 4.88485167e-01 7.96431781e-01 9.54923916e-01\n",
      "  3.26724589e-01 2.17760135e-01 8.58664577e-01 2.18628267e-01\n",
      "  9.66933276e-01 8.53120240e-01 3.38973044e-01 7.39868396e-01\n",
      "  9.03363844e-01 7.61752705e-02 9.34553163e-01 5.81335889e-02\n",
      "  5.65207142e-01 5.48723741e-01 2.58715077e-01 4.96656205e-01\n",
      "  9.55197405e-01 6.37778765e-01 4.51730403e-01 8.25724973e-01\n",
      "  6.62394506e-01 7.54715608e-01 8.38832956e-01 4.09216497e-01\n",
      "  6.80233239e-01 2.08249076e-01 7.07930167e-02 2.82841610e-01\n",
      "  3.36615995e-01 5.06706604e-01 2.40693349e-01 9.35622702e-01\n",
      "  3.39905269e-02 4.14599689e-01 6.14998920e-01 5.42620966e-01\n",
      "  7.55466567e-01 2.23764879e-01 4.93283614e-01 9.54503496e-01\n",
      "  8.64511074e-02 6.86301037e-01 7.55479620e-01 1.94842298e-01\n",
      "  2.51700686e-01 2.73441854e-01 4.60034433e-01 3.83432967e-01\n",
      "  5.09572173e-01 6.18548188e-01 8.51103919e-01 2.35314372e-01\n",
      "  4.01878536e-01 6.53873996e-01 6.34533158e-01]]\n",
      "Change in theta: 1.934009192741e-06\n",
      "---------------------------------------------\n",
      "Iteration number 27 finished, Running time 908.3472199440002, Roo2 iteration 27 = -14203748.423519686\n",
      "Iteration 1:\n",
      "Theta: [[6.82575179e-01 2.04523416e-01 3.44628999e-01 5.75118864e-01\n",
      "  1.67800595e-01 3.75650161e-01 1.16357543e-01 8.83589582e-01\n",
      "  5.22044141e-01 3.47342942e-01 8.63887817e-01 1.62104606e-01\n",
      "  1.26224092e-01 5.95408003e-01 9.41484781e-01 7.64167079e-01\n",
      "  5.77497116e-01 7.73456684e-01 1.63614430e-01 7.90844704e-01\n",
      "  9.35694818e-02 2.68798831e-01 3.84679624e-01 4.97088880e-01\n",
      "  4.94991848e-01 6.36577110e-01 3.65359965e-01 2.17185319e-01\n",
      "  4.22691538e-01 8.48222729e-01 8.36637109e-01 9.86258113e-01\n",
      "  4.69590580e-01 5.17769842e-01 1.62609031e-02 8.68939923e-01\n",
      "  6.89869512e-01 6.26076267e-01 4.87931017e-01 5.22125201e-01\n",
      "  4.13606357e-01 4.41757489e-01 7.98438540e-01 4.11091612e-01\n",
      "  7.66486835e-01 2.83451740e-01 6.01824921e-01 7.80454896e-02\n",
      "  9.99854862e-01 2.20902603e-01 7.66521012e-01 6.18849628e-01\n",
      "  6.52122482e-01 9.73280992e-01 3.28907144e-01 2.16800858e-02\n",
      "  3.20347489e-01 1.12523342e-01 5.44505871e-02 9.88056645e-01\n",
      "  6.05820336e-02 8.64992500e-01 4.53336697e-02 4.81478659e-01\n",
      "  7.91210632e-01 3.88685514e-01 4.32181398e-01 7.85729037e-02\n",
      "  1.92393846e-01 9.14167241e-01 7.07727737e-01 5.39102007e-02\n",
      "  8.76095425e-01 5.85052005e-01 5.29700549e-01 9.12787625e-01\n",
      "  7.89524970e-01 6.30190582e-01 9.55838629e-01 4.74091004e-01\n",
      "  4.29752785e-01 9.38229640e-01 6.08041067e-01 7.48823866e-01\n",
      "  5.09964964e-01 8.82830431e-01 9.07736901e-01 5.37811230e-01\n",
      "  5.87249339e-01 1.60274013e-01 8.63053001e-01 9.99109518e-02\n",
      "  9.15925955e-01 1.18105673e-01 5.79018921e-01 3.95719264e-01\n",
      "  2.48621842e-01 9.28312900e-01 2.19445404e-01 6.00851636e-02\n",
      "  9.26523943e-01 7.02472497e-01 4.80497425e-01 4.96210904e-01\n",
      "  6.64473436e-03 1.45518807e-01 4.15119776e-01 3.65167803e-02\n",
      "  7.29441723e-01 5.36883440e-01 3.39504406e-01 6.57801306e-01\n",
      "  1.25030349e-01 4.34491869e-01 8.53930865e-02 9.71366567e-01\n",
      "  2.27627624e-01 7.88513515e-01 8.18932573e-01 6.22108978e-01\n",
      "  9.20613199e-01 3.11789979e-01 1.56012694e-01 9.24314208e-01\n",
      "  5.82788726e-01 9.49798763e-01 3.93002552e-02 7.11921926e-01\n",
      "  5.12350440e-01 7.59410416e-01 8.43600367e-01 1.22675346e-01\n",
      "  4.76999513e-01 4.99928277e-01 3.46122004e-01 6.76130776e-01\n",
      "  1.99227648e-01 3.46426721e-01 7.67290970e-01 6.21691565e-02\n",
      "  4.65668632e-01 1.35890810e-01 2.75136542e-02 8.14448908e-01\n",
      "  5.12288580e-01 2.89726127e-01 2.26167110e-01 8.59434179e-01\n",
      "  8.31774276e-01 7.21033852e-01 2.36539416e-01 3.34415241e-01\n",
      "  5.19213327e-03 2.74527960e-01 9.88392144e-02 8.88446790e-01\n",
      "  6.15310081e-01 3.40358350e-01 8.85033833e-01 4.82986832e-01\n",
      "  6.52143138e-01 5.35473389e-01 8.15436937e-01 4.69734688e-01\n",
      "  9.84675225e-01 4.41144509e-02 8.85728432e-01 6.94572115e-01\n",
      "  8.33651972e-01 4.62207268e-01 6.11428509e-01 2.26864089e-01\n",
      "  7.93885582e-01 2.29918093e-01 4.08263137e-01 4.88010641e-01\n",
      "  3.36784546e-01 3.71375319e-01 6.59450307e-03 3.56354467e-01\n",
      "  7.18397041e-01 1.38386470e-01 9.48269403e-01 1.24280646e-01\n",
      "  8.18099868e-01 7.58733535e-01 5.16491793e-01 3.77495916e-01\n",
      "  1.32117515e-01 8.55643862e-01 9.48062659e-01 2.25141546e-01\n",
      "  5.16201392e-01 7.49111643e-01 6.40111671e-01 8.66498753e-01\n",
      "  1.47639161e-01 7.42609307e-01 6.45744136e-01 7.58025744e-01\n",
      "  8.00799076e-01 2.21042798e-01 6.04934735e-03 6.27652374e-01\n",
      "  2.51519242e-01 7.70843160e-01 1.97898216e-01 9.04974780e-01\n",
      "  8.38362511e-01 4.17782024e-01 7.56718833e-01 5.77383633e-01\n",
      "  9.48722927e-01 6.54360738e-01 5.91209567e-01 9.88765861e-01\n",
      "  5.36363297e-01 9.00716159e-01 1.28369397e-01 3.92142941e-01\n",
      "  9.80302781e-01 1.78507181e-01 5.28773367e-01 2.79126516e-01\n",
      "  8.18046036e-04 4.24812043e-01 3.87224006e-01 8.86489557e-01\n",
      "  3.16551353e-01 2.40264355e-01 1.78153754e-01 5.45052259e-01\n",
      "  9.52083200e-02 1.16803708e-01 4.73102468e-01 2.60935017e-02\n",
      "  9.36595280e-01 4.60460560e-01 9.32285294e-01 8.83191999e-01\n",
      "  6.57961098e-02 3.07766609e-01 1.89249146e-01 8.87249983e-01\n",
      "  6.49865750e-01 2.15844140e-01 1.20435195e-01 8.95820616e-01\n",
      "  9.55027378e-01 5.13471770e-01 2.84495558e-01 2.51497886e-01\n",
      "  6.40167931e-01 7.51372441e-01 3.00915834e-01 5.88151064e-01\n",
      "  4.87423020e-01 8.58465164e-01 1.42286233e-01 7.98698446e-01\n",
      "  2.73854101e-01 3.83842200e-01 8.75999290e-01 5.68032593e-01\n",
      "  9.93546629e-01 2.36377845e-01 1.41590023e-01 1.24076189e-01\n",
      "  5.27914676e-01 3.77271790e-01 1.40587510e-02 5.92230233e-01\n",
      "  2.26234678e-01 2.20739045e-01 9.22165294e-01 3.01009820e-01\n",
      "  8.47041188e-01 4.54766574e-01 7.00677185e-01 4.32779843e-01\n",
      "  2.82785670e-01 1.21308283e-02 9.12970923e-01 9.16906286e-02\n",
      "  4.80189181e-01 7.40669611e-01 1.07389522e-01 5.77285249e-01\n",
      "  2.73553857e-01 8.93725822e-01 8.18055092e-01 1.64507375e-01\n",
      "  3.99040747e-01 2.98322939e-01 1.90891165e-02 3.37458557e-01\n",
      "  8.73765788e-01 5.34798177e-01 1.41357877e-02 1.63439037e-01\n",
      "  7.28873763e-01 6.17560534e-01 5.02949688e-01 4.73832802e-01\n",
      "  9.99536835e-01 4.57531392e-02 1.05339398e-03 8.25583994e-01\n",
      "  3.13224794e-01 2.34299833e-02 5.74201030e-01 3.39243872e-01\n",
      "  2.72271499e-01 6.21530845e-01 7.02704182e-01 1.13288336e-01\n",
      "  4.60251737e-01 1.75230067e-01 8.24625579e-01 7.05603436e-02\n",
      "  6.04574696e-01 8.05313882e-01 9.78896408e-01 5.74008733e-01\n",
      "  5.04732639e-01 2.05057267e-01 2.04867783e-01 9.94132903e-01\n",
      "  4.67065194e-02 9.70120451e-01 9.73005354e-01 1.93461145e-01\n",
      "  9.06032302e-01 1.03418579e-01 2.73920935e-01 1.36032907e-01\n",
      "  2.00725163e-01 5.62377795e-01 6.69035777e-01 8.94450529e-01\n",
      "  1.83291586e-01 6.23320665e-01 3.17597713e-01 6.02594595e-01\n",
      "  1.42341584e-01 4.40906317e-01 1.35389035e-01 4.46421473e-01\n",
      "  8.82995282e-01 7.66580183e-01 8.33557840e-01 6.51920878e-01\n",
      "  4.96796935e-01 1.96715452e-01 4.71320805e-01 8.09279645e-01\n",
      "  9.68957571e-01 1.04470017e-01 9.82771601e-02 5.16570177e-02\n",
      "  5.56999170e-01 3.29734038e-01 4.34734969e-01 7.55833444e-01\n",
      "  8.99339826e-01 7.09528217e-01 2.51640415e-01 7.89586745e-01\n",
      "  1.51675460e-01 2.22350581e-01 2.34432701e-01 5.26803984e-01\n",
      "  7.36720502e-01 4.32955847e-01 2.72612875e-02 7.41960525e-01\n",
      "  5.81492113e-01 6.78601872e-01 3.52228688e-01 3.95201206e-01\n",
      "  2.58023912e-01 9.22855264e-01 5.25845610e-01 8.64688456e-01\n",
      "  8.70304579e-02 6.75853619e-01 1.13520064e-01 8.76543435e-01\n",
      "  8.34092278e-01 2.01396882e-01 1.08226734e-01 4.38361206e-01\n",
      "  2.80599529e-01 4.26080150e-01 7.02975945e-01 6.32363917e-01\n",
      "  3.66942350e-01 3.11667919e-01 3.82077344e-01 7.29479018e-01\n",
      "  9.69921871e-03 6.19883412e-02 3.53991276e-01 2.69966133e-01\n",
      "  9.66150735e-01 7.30329639e-01 9.43748333e-01 7.51398969e-01\n",
      "  5.49232474e-01 5.10253939e-01 6.98477848e-01 5.46680164e-01\n",
      "  4.81243460e-01 5.08644877e-01 1.50956559e-01 3.67524639e-01\n",
      "  8.24299708e-01 9.09534414e-01 9.44647763e-02 8.25108961e-01\n",
      "  2.40397469e-01 9.05179547e-01 3.45904907e-01 2.32408649e-01\n",
      "  2.50718155e-01 4.80713892e-02 9.59477503e-01 1.43134938e-01\n",
      "  9.60571931e-01 6.71632719e-01 3.57144080e-01 9.39047969e-01\n",
      "  3.73592858e-01 2.37527127e-01 1.69447099e-01 7.89374940e-01\n",
      "  1.27685087e-01 6.36395309e-01 2.12398245e-01 7.46967849e-01\n",
      "  6.92354918e-01 9.55250544e-01 1.11659382e-01 5.90114333e-01\n",
      "  7.51925126e-02 6.89098604e-01 1.23331681e-01 8.00806964e-01\n",
      "  4.69707492e-01 3.68754303e-01 5.13027320e-01 9.94911862e-01\n",
      "  9.26169893e-01 5.79565486e-01 5.40857607e-01 9.24877253e-01\n",
      "  4.96936076e-01 3.09740202e-01 9.76926114e-01 8.82781924e-01\n",
      "  8.42212412e-01 5.27684736e-02 5.77242314e-01 9.42875226e-01\n",
      "  9.52963856e-01 8.25978302e-02 9.22208445e-01 9.69213618e-02\n",
      "  6.48956785e-01 9.51240078e-01 1.90833323e-01 5.94356654e-01\n",
      "  8.19267412e-01 8.25588411e-01 9.12821192e-01 7.80938122e-01\n",
      "  1.40324665e-01 3.04267435e-01 1.01069139e-01 2.08816537e-01\n",
      "  8.76161607e-01 5.92045417e-01 6.43816779e-02 6.72100894e-02\n",
      "  8.63124373e-02 2.25005492e-01 6.52609534e-01 6.14434859e-01\n",
      "  8.17668546e-01 4.13474098e-01 5.53463337e-01 9.86160503e-01\n",
      "  9.98206416e-01 7.21736246e-01 6.89882187e-01 1.79588772e-01\n",
      "  3.67415854e-01 5.29403212e-01 9.16703871e-01 2.23359968e-01\n",
      "  1.03078658e-01 7.59486341e-01 5.64298374e-01 4.23059812e-01\n",
      "  6.15092391e-01 2.35914735e-01 8.00336553e-01 9.25107189e-01\n",
      "  8.50865434e-01 5.93770558e-01 1.46469527e-02 9.47515911e-01\n",
      "  1.23542559e-01 6.63007418e-01 5.01392856e-01 1.08923737e-01\n",
      "  1.02242196e-01 7.68052694e-01 4.78186222e-01 7.49536503e-02\n",
      "  6.96290069e-01 8.02061325e-02 4.29919480e-01 5.64907484e-01\n",
      "  3.91941334e-01 5.87261705e-01 4.26965945e-01 4.31611036e-01\n",
      "  6.36841692e-02 3.29837535e-01 1.59524397e-01 7.22781175e-01\n",
      "  1.45320461e-01 3.38390824e-01 8.64594158e-01 7.93640377e-02\n",
      "  8.82648957e-01 1.26561448e-01 4.54622271e-01 7.47074737e-01\n",
      "  8.38649715e-01 7.88241795e-01 4.87982501e-01 1.02035043e-01\n",
      "  7.53825697e-01 8.80504576e-01 5.26668887e-01 1.70833143e-01\n",
      "  4.59580120e-01 9.68492192e-01 3.58482474e-01 5.99295962e-01\n",
      "  9.17981932e-01 7.24718158e-01 1.48541304e-01 5.61829171e-01\n",
      "  8.79344378e-01 3.57900449e-01 9.23143123e-01 3.06277550e-01\n",
      "  1.99896447e-01 5.77202874e-01 5.64177821e-01 6.50677214e-01\n",
      "  8.99024921e-01 9.87217668e-01 1.40563947e-01 4.20670172e-01\n",
      "  8.65571572e-01 4.93328560e-01 8.36056102e-02 7.62235744e-01\n",
      "  8.09695986e-01 7.33152596e-01 8.53778395e-01 7.27791542e-01\n",
      "  7.21597335e-01 5.75476679e-01 1.85635612e-01 7.60942142e-01\n",
      "  8.68989399e-01 9.52527610e-01 1.98961005e-01 7.96139448e-01\n",
      "  7.70852259e-02 9.90883060e-01 1.97836741e-01 4.74459780e-02\n",
      "  4.47471343e-01 7.70386940e-01 3.76061343e-01 8.45133192e-01\n",
      "  3.03900776e-01 2.27219214e-01 7.00405393e-02 3.80356333e-01\n",
      "  7.33159935e-02 7.45900554e-01 9.68577606e-01 1.31822653e-01\n",
      "  1.21184090e-03 3.14204927e-01 4.05176219e-01 8.80300344e-01\n",
      "  6.47540470e-01 1.54627091e-01 1.79618527e-01 4.07199979e-01\n",
      "  6.54980601e-01 5.79014273e-01 2.89477643e-01 9.09078630e-01\n",
      "  9.27796509e-01 9.15115428e-01 1.25445256e-01 1.68090268e-01\n",
      "  5.24881282e-01 6.45078588e-01 5.74247232e-02 8.80104579e-01\n",
      "  4.66111490e-01 4.75795835e-01 9.24948631e-01 5.30052753e-01\n",
      "  3.95272003e-01 7.50719275e-01 2.51713889e-01 9.60989416e-01\n",
      "  6.68700052e-01 8.39522579e-01 9.94642871e-01 7.65510214e-01\n",
      "  5.10790253e-01 3.75965435e-01 4.99000489e-01 5.77290544e-02\n",
      "  7.28733807e-01 7.74329073e-01 7.94853324e-01 4.15578580e-01\n",
      "  4.42386835e-01 2.55213764e-01 6.40363719e-01 3.30230561e-01\n",
      "  1.65404966e-01 3.51087754e-01 6.43735203e-01 4.72092260e-01\n",
      "  2.44012322e-01 4.43201960e-01 5.10145868e-01 4.09463869e-01\n",
      "  2.07559558e-01 8.90967695e-01 9.23906087e-01 6.80916634e-01\n",
      "  1.56221855e-01 3.89122425e-01 7.27522425e-01 6.75598858e-01\n",
      "  8.96701107e-01 2.88811562e-01 5.00050239e-01 7.30120213e-01\n",
      "  1.90256287e-01 5.63509003e-01 2.45780333e-01 3.42290497e-01\n",
      "  4.26177020e-01 8.07080038e-01 4.93457796e-01 8.56646272e-02\n",
      "  7.99071446e-01 3.11808013e-01 7.64813801e-01 7.40976011e-01\n",
      "  4.86191415e-01 1.69103099e-01 4.74846726e-01 3.51404449e-01\n",
      "  9.34621608e-01 9.79417719e-01 3.23968271e-01 4.71717335e-01\n",
      "  6.93369112e-02 9.35393953e-01 5.14907764e-01 1.31702289e-01\n",
      "  2.70941161e-01 1.35229760e-01 9.95837881e-01 1.72705564e-01\n",
      "  4.46682136e-01 4.99669402e-01 8.38713497e-01 2.13997299e-01\n",
      "  9.03478896e-01 9.88386648e-01 8.00239422e-01 5.48061012e-01\n",
      "  5.24025234e-01 5.01594765e-01 1.10482212e-01 8.55223834e-01\n",
      "  8.68576159e-01 9.84733894e-01 7.90516103e-01 5.71470294e-01\n",
      "  2.88337386e-01 9.33101610e-01 8.21235902e-01 6.92289218e-01\n",
      "  2.08974086e-01 5.16272558e-01 7.86257240e-02 2.31584003e-01\n",
      "  9.92604197e-01 6.36648509e-01 2.47829338e-01 8.49988642e-01\n",
      "  3.60313187e-01 1.71887551e-01 1.25641995e-01 4.33522965e-01\n",
      "  3.68703494e-01 9.46486433e-01 7.36791110e-01 3.60603887e-01\n",
      "  8.07359114e-01 3.95648423e-01 4.06503574e-01 7.38390126e-01\n",
      "  8.54094421e-01 6.67346428e-01 7.23899599e-01 1.74531585e-01\n",
      "  9.23471464e-02 8.46754734e-01 3.91486742e-01 8.22162999e-01\n",
      "  2.63940182e-01 4.67600456e-01 2.50136513e-01 5.23067554e-01\n",
      "  2.45277209e-01 5.58069660e-01 1.28891725e-01 2.39357658e-02\n",
      "  3.29897628e-01 8.42651022e-01 4.12462448e-01 7.62081872e-01\n",
      "  1.83394949e-01 3.04329677e-01 5.46357976e-01 7.63157974e-01\n",
      "  8.09633204e-01 2.56459361e-01 9.98524529e-01 7.98319538e-02\n",
      "  7.79709975e-01 8.79006566e-01 6.58054896e-01 9.70740582e-01\n",
      "  1.45861593e-01 9.45155290e-01 5.79535710e-02 4.42632425e-01\n",
      "  7.67008434e-01 7.57396298e-02 1.64861534e-01 6.43936580e-01\n",
      "  5.57356012e-01 8.70988219e-01 2.54183735e-01 3.17581975e-02\n",
      "  3.30298636e-01 4.70763482e-02 8.03734014e-01 2.99280024e-01\n",
      "  7.07831205e-01 3.52637815e-01 3.39161481e-01 2.17998104e-01\n",
      "  3.35998304e-01 1.39124590e-02 7.24117911e-01 8.14955987e-01\n",
      "  2.41686483e-01 1.75035855e-01 2.96065305e-02 9.32525303e-01\n",
      "  4.65675223e-01 9.79172276e-01 8.04204129e-01 1.61070573e-01\n",
      "  3.98149418e-01 1.59935754e-01 4.77531140e-01 2.24095481e-02\n",
      "  7.50475181e-01 8.18732919e-01 8.06219386e-01 5.60817191e-01\n",
      "  9.13322970e-01 8.93212792e-01 2.04388365e-01 3.99976520e-01\n",
      "  6.82214409e-01 4.78424090e-01 2.04225027e-01 8.36657205e-01\n",
      "  7.14374725e-01 2.42751270e-01 6.69398575e-01 4.88104002e-01\n",
      "  9.52274447e-01 2.82862089e-01 9.38250213e-01 7.98685750e-01\n",
      "  8.46975551e-01 8.50874187e-01 2.86995119e-03 2.92008276e-01\n",
      "  7.72451052e-01 7.34962386e-01 4.99529624e-01 9.60727852e-01\n",
      "  2.69300079e-01 7.67449622e-02 8.13096461e-01 9.14853319e-01\n",
      "  8.52983508e-01 1.87465246e-01 9.12961179e-01 8.93473032e-01\n",
      "  1.53607246e-01 8.52540641e-01 4.12794076e-01 6.47764432e-01\n",
      "  4.37672228e-01 6.53469073e-01 8.43434992e-01 2.65410769e-01\n",
      "  2.97436766e-01 7.01279355e-01 6.26222063e-01 1.01230341e-01\n",
      "  2.28365915e-01 5.24535206e-01 6.52710194e-01 7.56430698e-01\n",
      "  6.24633937e-01 4.88485399e-01 7.96432014e-01 9.54924148e-01\n",
      "  3.26724822e-01 2.17760368e-01 8.58664810e-01 2.18628500e-01\n",
      "  9.66933509e-01 8.53120473e-01 3.38973277e-01 7.39868629e-01\n",
      "  9.03364077e-01 7.61755031e-02 9.34553396e-01 5.81338215e-02\n",
      "  5.65207375e-01 5.48723973e-01 2.58715310e-01 4.96656437e-01\n",
      "  9.55197638e-01 6.37778997e-01 4.51730635e-01 8.25725206e-01\n",
      "  6.62394739e-01 7.54715841e-01 8.38833188e-01 4.09216730e-01\n",
      "  6.80233471e-01 2.08249309e-01 7.07932493e-02 2.82841842e-01\n",
      "  3.36616228e-01 5.06706837e-01 2.40693581e-01 9.35622935e-01\n",
      "  3.39907596e-02 4.14599922e-01 6.14999152e-01 5.42621198e-01\n",
      "  7.55466800e-01 2.23765111e-01 4.93283847e-01 9.54503729e-01\n",
      "  8.64513401e-02 6.86301269e-01 7.55479853e-01 1.94842531e-01\n",
      "  2.51700919e-01 2.73442086e-01 4.60034666e-01 3.83433200e-01\n",
      "  5.09572406e-01 6.18548420e-01 8.51104151e-01 2.35314604e-01\n",
      "  4.01878768e-01 6.53874229e-01 6.34533391e-01]]\n",
      "Change in theta: 1.9535874143742273e-06\n",
      "---------------------------------------------\n",
      "Iteration number 28 finished, Running time 1098.0744681358337, Roo2 iteration 28 = -23252439.649881113\n",
      "Iteration 1:\n",
      "Theta: [[6.82575331e-01 2.04523569e-01 3.44629152e-01 5.75119016e-01\n",
      "  1.67800747e-01 3.75650313e-01 1.16357695e-01 8.83589734e-01\n",
      "  5.22044294e-01 3.47343094e-01 8.63887969e-01 1.62104758e-01\n",
      "  1.26224244e-01 5.95408155e-01 9.41484933e-01 7.64167232e-01\n",
      "  5.77497268e-01 7.73456836e-01 1.63614582e-01 7.90844856e-01\n",
      "  9.35696340e-02 2.68798983e-01 3.84679776e-01 4.97089032e-01\n",
      "  4.94992000e-01 6.36577262e-01 3.65360118e-01 2.17185471e-01\n",
      "  4.22691690e-01 8.48222882e-01 8.36637261e-01 9.86258265e-01\n",
      "  4.69590732e-01 5.17769994e-01 1.62610554e-02 8.68940076e-01\n",
      "  6.89869665e-01 6.26076419e-01 4.87931169e-01 5.22125354e-01\n",
      "  4.13606509e-01 4.41757641e-01 7.98438693e-01 4.11091765e-01\n",
      "  7.66486988e-01 2.83451892e-01 6.01825073e-01 7.80456419e-02\n",
      "  9.99855014e-01 2.20902755e-01 7.66521164e-01 6.18849780e-01\n",
      "  6.52122634e-01 9.73281144e-01 3.28907296e-01 2.16802381e-02\n",
      "  3.20347641e-01 1.12523494e-01 5.44507393e-02 9.88056798e-01\n",
      "  6.05821858e-02 8.64992652e-01 4.53338220e-02 4.81478811e-01\n",
      "  7.91210785e-01 3.88685666e-01 4.32181550e-01 7.85730560e-02\n",
      "  1.92393998e-01 9.14167394e-01 7.07727889e-01 5.39103530e-02\n",
      "  8.76095578e-01 5.85052157e-01 5.29700701e-01 9.12787777e-01\n",
      "  7.89525122e-01 6.30190734e-01 9.55838781e-01 4.74091156e-01\n",
      "  4.29752938e-01 9.38229792e-01 6.08041219e-01 7.48824018e-01\n",
      "  5.09965116e-01 8.82830583e-01 9.07737053e-01 5.37811382e-01\n",
      "  5.87249491e-01 1.60274166e-01 8.63053153e-01 9.99111040e-02\n",
      "  9.15926108e-01 1.18105825e-01 5.79019073e-01 3.95719416e-01\n",
      "  2.48621994e-01 9.28313053e-01 2.19445557e-01 6.00853158e-02\n",
      "  9.26524096e-01 7.02472650e-01 4.80497577e-01 4.96211057e-01\n",
      "  6.64488662e-03 1.45518959e-01 4.15119929e-01 3.65169326e-02\n",
      "  7.29441876e-01 5.36883593e-01 3.39504558e-01 6.57801458e-01\n",
      "  1.25030501e-01 4.34492021e-01 8.53932388e-02 9.71366720e-01\n",
      "  2.27627777e-01 7.88513667e-01 8.18932725e-01 6.22109131e-01\n",
      "  9.20613351e-01 3.11790131e-01 1.56012847e-01 9.24314360e-01\n",
      "  5.82788879e-01 9.49798916e-01 3.93004074e-02 7.11922078e-01\n",
      "  5.12350592e-01 7.59410568e-01 8.43600519e-01 1.22675498e-01\n",
      "  4.76999666e-01 4.99928429e-01 3.46122157e-01 6.76130929e-01\n",
      "  1.99227800e-01 3.46426874e-01 7.67291122e-01 6.21693087e-02\n",
      "  4.65668784e-01 1.35890963e-01 2.75138065e-02 8.14449060e-01\n",
      "  5.12288732e-01 2.89726280e-01 2.26167262e-01 8.59434331e-01\n",
      "  8.31774428e-01 7.21034004e-01 2.36539569e-01 3.34415393e-01\n",
      "  5.19228553e-03 2.74528112e-01 9.88393666e-02 8.88446943e-01\n",
      "  6.15310234e-01 3.40358503e-01 8.85033985e-01 4.82986985e-01\n",
      "  6.52143290e-01 5.35473541e-01 8.15437089e-01 4.69734840e-01\n",
      "  9.84675378e-01 4.41146032e-02 8.85728584e-01 6.94572267e-01\n",
      "  8.33652124e-01 4.62207420e-01 6.11428661e-01 2.26864242e-01\n",
      "  7.93885734e-01 2.29918245e-01 4.08263289e-01 4.88010794e-01\n",
      "  3.36784698e-01 3.71375471e-01 6.59465533e-03 3.56354620e-01\n",
      "  7.18397194e-01 1.38386622e-01 9.48269555e-01 1.24280799e-01\n",
      "  8.18100020e-01 7.58733688e-01 5.16491945e-01 3.77496069e-01\n",
      "  1.32117667e-01 8.55644015e-01 9.48062811e-01 2.25141698e-01\n",
      "  5.16201545e-01 7.49111795e-01 6.40111823e-01 8.66498905e-01\n",
      "  1.47639313e-01 7.42609459e-01 6.45744289e-01 7.58025896e-01\n",
      "  8.00799228e-01 2.21042951e-01 6.04949961e-03 6.27652526e-01\n",
      "  2.51519395e-01 7.70843312e-01 1.97898368e-01 9.04974932e-01\n",
      "  8.38362664e-01 4.17782176e-01 7.56718985e-01 5.77383785e-01\n",
      "  9.48723079e-01 6.54360890e-01 5.91209719e-01 9.88766013e-01\n",
      "  5.36363449e-01 9.00716312e-01 1.28369549e-01 3.92143093e-01\n",
      "  9.80302933e-01 1.78507334e-01 5.28773519e-01 2.79126669e-01\n",
      "  8.18198300e-04 4.24812195e-01 3.87224158e-01 8.86489709e-01\n",
      "  3.16551506e-01 2.40264508e-01 1.78153906e-01 5.45052411e-01\n",
      "  9.52084723e-02 1.16803860e-01 4.73102620e-01 2.60936539e-02\n",
      "  9.36595433e-01 4.60460712e-01 9.32285446e-01 8.83192152e-01\n",
      "  6.57962621e-02 3.07766761e-01 1.89249299e-01 8.87250135e-01\n",
      "  6.49865902e-01 2.15844292e-01 1.20435348e-01 8.95820768e-01\n",
      "  9.55027530e-01 5.13471923e-01 2.84495710e-01 2.51498038e-01\n",
      "  6.40168083e-01 7.51372593e-01 3.00915986e-01 5.88151216e-01\n",
      "  4.87423173e-01 8.58465317e-01 1.42286385e-01 7.98698599e-01\n",
      "  2.73854253e-01 3.83842352e-01 8.75999442e-01 5.68032745e-01\n",
      "  9.93546782e-01 2.36377997e-01 1.41590175e-01 1.24076341e-01\n",
      "  5.27914828e-01 3.77271942e-01 1.40589032e-02 5.92230385e-01\n",
      "  2.26234831e-01 2.20739197e-01 9.22165446e-01 3.01009972e-01\n",
      "  8.47041341e-01 4.54766726e-01 7.00677337e-01 4.32779996e-01\n",
      "  2.82785822e-01 1.21309806e-02 9.12971075e-01 9.16907808e-02\n",
      "  4.80189334e-01 7.40669763e-01 1.07389674e-01 5.77285402e-01\n",
      "  2.73554009e-01 8.93725974e-01 8.18055245e-01 1.64507527e-01\n",
      "  3.99040899e-01 2.98323091e-01 1.90892688e-02 3.37458709e-01\n",
      "  8.73765941e-01 5.34798329e-01 1.41359400e-02 1.63439189e-01\n",
      "  7.28873915e-01 6.17560686e-01 5.02949841e-01 4.73832954e-01\n",
      "  9.99536987e-01 4.57532915e-02 1.05354624e-03 8.25584146e-01\n",
      "  3.13224947e-01 2.34301355e-02 5.74201183e-01 3.39244025e-01\n",
      "  2.72271652e-01 6.21530997e-01 7.02704334e-01 1.13288488e-01\n",
      "  4.60251889e-01 1.75230219e-01 8.24625731e-01 7.05604959e-02\n",
      "  6.04574849e-01 8.05314034e-01 9.78896560e-01 5.74008885e-01\n",
      "  5.04732792e-01 2.05057419e-01 2.04867936e-01 9.94133056e-01\n",
      "  4.67066717e-02 9.70120603e-01 9.73005506e-01 1.93461297e-01\n",
      "  9.06032454e-01 1.03418732e-01 2.73921087e-01 1.36033059e-01\n",
      "  2.00725316e-01 5.62377947e-01 6.69035929e-01 8.94450681e-01\n",
      "  1.83291738e-01 6.23320818e-01 3.17597865e-01 6.02594747e-01\n",
      "  1.42341736e-01 4.40906469e-01 1.35389187e-01 4.46421625e-01\n",
      "  8.82995434e-01 7.66580335e-01 8.33557992e-01 6.51921030e-01\n",
      "  4.96797087e-01 1.96715604e-01 4.71320957e-01 8.09279797e-01\n",
      "  9.68957723e-01 1.04470170e-01 9.82773124e-02 5.16571700e-02\n",
      "  5.56999322e-01 3.29734191e-01 4.34735121e-01 7.55833597e-01\n",
      "  8.99339978e-01 7.09528370e-01 2.51640567e-01 7.89586898e-01\n",
      "  1.51675613e-01 2.22350733e-01 2.34432853e-01 5.26804137e-01\n",
      "  7.36720654e-01 4.32956000e-01 2.72614398e-02 7.41960677e-01\n",
      "  5.81492266e-01 6.78602024e-01 3.52228841e-01 3.95201358e-01\n",
      "  2.58024064e-01 9.22855416e-01 5.25845762e-01 8.64688609e-01\n",
      "  8.70306102e-02 6.75853771e-01 1.13520216e-01 8.76543588e-01\n",
      "  8.34092430e-01 2.01397034e-01 1.08226886e-01 4.38361358e-01\n",
      "  2.80599681e-01 4.26080302e-01 7.02976097e-01 6.32364069e-01\n",
      "  3.66942502e-01 3.11668071e-01 3.82077496e-01 7.29479170e-01\n",
      "  9.69937097e-03 6.19884935e-02 3.53991428e-01 2.69966286e-01\n",
      "  9.66150888e-01 7.30329791e-01 9.43748485e-01 7.51399121e-01\n",
      "  5.49232627e-01 5.10254091e-01 6.98478000e-01 5.46680316e-01\n",
      "  4.81243613e-01 5.08645029e-01 1.50956711e-01 3.67524791e-01\n",
      "  8.24299860e-01 9.09534566e-01 9.44649286e-02 8.25109114e-01\n",
      "  2.40397622e-01 9.05179699e-01 3.45905060e-01 2.32408802e-01\n",
      "  2.50718307e-01 4.80715414e-02 9.59477655e-01 1.43135091e-01\n",
      "  9.60572084e-01 6.71632871e-01 3.57144233e-01 9.39048122e-01\n",
      "  3.73593010e-01 2.37527279e-01 1.69447251e-01 7.89375092e-01\n",
      "  1.27685239e-01 6.36395461e-01 2.12398398e-01 7.46968002e-01\n",
      "  6.92355070e-01 9.55250696e-01 1.11659534e-01 5.90114485e-01\n",
      "  7.51926648e-02 6.89098756e-01 1.23331833e-01 8.00807116e-01\n",
      "  4.69707644e-01 3.68754455e-01 5.13027473e-01 9.94912014e-01\n",
      "  9.26170045e-01 5.79565638e-01 5.40857759e-01 9.24877405e-01\n",
      "  4.96936228e-01 3.09740354e-01 9.76926266e-01 8.82782076e-01\n",
      "  8.42212565e-01 5.27686259e-02 5.77242466e-01 9.42875378e-01\n",
      "  9.52964008e-01 8.25979825e-02 9.22208597e-01 9.69215141e-02\n",
      "  6.48956938e-01 9.51240230e-01 1.90833475e-01 5.94356806e-01\n",
      "  8.19267564e-01 8.25588563e-01 9.12821345e-01 7.80938274e-01\n",
      "  1.40324817e-01 3.04267587e-01 1.01069291e-01 2.08816690e-01\n",
      "  8.76161759e-01 5.92045569e-01 6.43818302e-02 6.72102416e-02\n",
      "  8.63125895e-02 2.25005645e-01 6.52609687e-01 6.14435011e-01\n",
      "  8.17668698e-01 4.13474250e-01 5.53463489e-01 9.86160656e-01\n",
      "  9.98206569e-01 7.21736398e-01 6.89882339e-01 1.79588924e-01\n",
      "  3.67416007e-01 5.29403365e-01 9.16704023e-01 2.23360120e-01\n",
      "  1.03078810e-01 7.59486493e-01 5.64298526e-01 4.23059965e-01\n",
      "  6.15092543e-01 2.35914887e-01 8.00336706e-01 9.25107341e-01\n",
      "  8.50865586e-01 5.93770710e-01 1.46471050e-02 9.47516064e-01\n",
      "  1.23542711e-01 6.63007571e-01 5.01393008e-01 1.08923889e-01\n",
      "  1.02242349e-01 7.68052846e-01 4.78186375e-01 7.49538026e-02\n",
      "  6.96290221e-01 8.02062847e-02 4.29919633e-01 5.64907636e-01\n",
      "  3.91941487e-01 5.87261858e-01 4.26966097e-01 4.31611188e-01\n",
      "  6.36843215e-02 3.29837687e-01 1.59524549e-01 7.22781328e-01\n",
      "  1.45320614e-01 3.38390976e-01 8.64594310e-01 7.93641900e-02\n",
      "  8.82649109e-01 1.26561601e-01 4.54622423e-01 7.47074889e-01\n",
      "  8.38649867e-01 7.88241948e-01 4.87982653e-01 1.02035196e-01\n",
      "  7.53825849e-01 8.80504728e-01 5.26669040e-01 1.70833295e-01\n",
      "  4.59580272e-01 9.68492344e-01 3.58482626e-01 5.99296114e-01\n",
      "  9.17982084e-01 7.24718310e-01 1.48541456e-01 5.61829323e-01\n",
      "  8.79344530e-01 3.57900601e-01 9.23143276e-01 3.06277702e-01\n",
      "  1.99896599e-01 5.77203026e-01 5.64177973e-01 6.50677366e-01\n",
      "  8.99025074e-01 9.87217820e-01 1.40564100e-01 4.20670324e-01\n",
      "  8.65571725e-01 4.93328712e-01 8.36057625e-02 7.62235896e-01\n",
      "  8.09696138e-01 7.33152748e-01 8.53778547e-01 7.27791694e-01\n",
      "  7.21597487e-01 5.75476832e-01 1.85635764e-01 7.60942294e-01\n",
      "  8.68989551e-01 9.52527763e-01 1.98961157e-01 7.96139600e-01\n",
      "  7.70853782e-02 9.90883212e-01 1.97836893e-01 4.74461302e-02\n",
      "  4.47471495e-01 7.70387092e-01 3.76061495e-01 8.45133344e-01\n",
      "  3.03900928e-01 2.27219366e-01 7.00406916e-02 3.80356486e-01\n",
      "  7.33161458e-02 7.45900707e-01 9.68577759e-01 1.31822805e-01\n",
      "  1.21199317e-03 3.14205079e-01 4.05176371e-01 8.80300497e-01\n",
      "  6.47540622e-01 1.54627243e-01 1.79618679e-01 4.07200132e-01\n",
      "  6.54980753e-01 5.79014426e-01 2.89477796e-01 9.09078783e-01\n",
      "  9.27796661e-01 9.15115581e-01 1.25445409e-01 1.68090420e-01\n",
      "  5.24881434e-01 6.45078740e-01 5.74248755e-02 8.80104731e-01\n",
      "  4.66111643e-01 4.75795987e-01 9.24948784e-01 5.30052905e-01\n",
      "  3.95272155e-01 7.50719427e-01 2.51714042e-01 9.60989569e-01\n",
      "  6.68700204e-01 8.39522731e-01 9.94643024e-01 7.65510366e-01\n",
      "  5.10790405e-01 3.75965587e-01 4.99000642e-01 5.77292067e-02\n",
      "  7.28733959e-01 7.74329226e-01 7.94853476e-01 4.15578732e-01\n",
      "  4.42386987e-01 2.55213916e-01 6.40363872e-01 3.30230713e-01\n",
      "  1.65405118e-01 3.51087906e-01 6.43735355e-01 4.72092413e-01\n",
      "  2.44012475e-01 4.43202112e-01 5.10146020e-01 4.09464021e-01\n",
      "  2.07559710e-01 8.90967848e-01 9.23906240e-01 6.80916786e-01\n",
      "  1.56222007e-01 3.89122578e-01 7.27522577e-01 6.75599010e-01\n",
      "  8.96701259e-01 2.88811715e-01 5.00050391e-01 7.30120365e-01\n",
      "  1.90256440e-01 5.63509155e-01 2.45780485e-01 3.42290650e-01\n",
      "  4.26177172e-01 8.07080190e-01 4.93457949e-01 8.56647795e-02\n",
      "  7.99071598e-01 3.11808165e-01 7.64813953e-01 7.40976163e-01\n",
      "  4.86191567e-01 1.69103251e-01 4.74846878e-01 3.51404601e-01\n",
      "  9.34621760e-01 9.79417871e-01 3.23968423e-01 4.71717487e-01\n",
      "  6.93370634e-02 9.35394106e-01 5.14907916e-01 1.31702441e-01\n",
      "  2.70941313e-01 1.35229912e-01 9.95838033e-01 1.72705716e-01\n",
      "  4.46682288e-01 4.99669554e-01 8.38713649e-01 2.13997451e-01\n",
      "  9.03479049e-01 9.88386800e-01 8.00239574e-01 5.48061164e-01\n",
      "  5.24025387e-01 5.01594917e-01 1.10482364e-01 8.55223986e-01\n",
      "  8.68576311e-01 9.84734046e-01 7.90516255e-01 5.71470446e-01\n",
      "  2.88337538e-01 9.33101762e-01 8.21236054e-01 6.92289370e-01\n",
      "  2.08974238e-01 5.16272711e-01 7.86258762e-02 2.31584155e-01\n",
      "  9.92604349e-01 6.36648662e-01 2.47829490e-01 8.49988795e-01\n",
      "  3.60313339e-01 1.71887703e-01 1.25642147e-01 4.33523117e-01\n",
      "  3.68703646e-01 9.46486585e-01 7.36791263e-01 3.60604039e-01\n",
      "  8.07359266e-01 3.95648575e-01 4.06503727e-01 7.38390278e-01\n",
      "  8.54094573e-01 6.67346580e-01 7.23899752e-01 1.74531738e-01\n",
      "  9.23472987e-02 8.46754886e-01 3.91486894e-01 8.22163151e-01\n",
      "  2.63940335e-01 4.67600608e-01 2.50136665e-01 5.23067706e-01\n",
      "  2.45277361e-01 5.58069812e-01 1.28891877e-01 2.39359181e-02\n",
      "  3.29897781e-01 8.42651174e-01 4.12462600e-01 7.62082025e-01\n",
      "  1.83395102e-01 3.04329829e-01 5.46358128e-01 7.63158126e-01\n",
      "  8.09633356e-01 2.56459514e-01 9.98524681e-01 7.98321060e-02\n",
      "  7.79710128e-01 8.79006718e-01 6.58055048e-01 9.70740735e-01\n",
      "  1.45861745e-01 9.45155442e-01 5.79537233e-02 4.42632577e-01\n",
      "  7.67008587e-01 7.57397821e-02 1.64861686e-01 6.43936732e-01\n",
      "  5.57356164e-01 8.70988371e-01 2.54183887e-01 3.17583498e-02\n",
      "  3.30298788e-01 4.70765005e-02 8.03734166e-01 2.99280176e-01\n",
      "  7.07831358e-01 3.52637967e-01 3.39161633e-01 2.17998256e-01\n",
      "  3.35998457e-01 1.39126113e-02 7.24118064e-01 8.14956139e-01\n",
      "  2.41686635e-01 1.75036007e-01 2.96066827e-02 9.32525455e-01\n",
      "  4.65675376e-01 9.79172428e-01 8.04204282e-01 1.61070725e-01\n",
      "  3.98149570e-01 1.59935906e-01 4.77531292e-01 2.24097003e-02\n",
      "  7.50475333e-01 8.18733071e-01 8.06219539e-01 5.60817343e-01\n",
      "  9.13323122e-01 8.93212944e-01 2.04388517e-01 3.99976672e-01\n",
      "  6.82214562e-01 4.78424243e-01 2.04225179e-01 8.36657358e-01\n",
      "  7.14374877e-01 2.42751422e-01 6.69398727e-01 4.88104154e-01\n",
      "  9.52274600e-01 2.82862241e-01 9.38250366e-01 7.98685902e-01\n",
      "  8.46975703e-01 8.50874339e-01 2.87010346e-03 2.92008429e-01\n",
      "  7.72451205e-01 7.34962539e-01 4.99529777e-01 9.60728005e-01\n",
      "  2.69300231e-01 7.67451145e-02 8.13096613e-01 9.14853471e-01\n",
      "  8.52983660e-01 1.87465398e-01 9.12961332e-01 8.93473184e-01\n",
      "  1.53607398e-01 8.52540793e-01 4.12794228e-01 6.47764585e-01\n",
      "  4.37672380e-01 6.53469225e-01 8.43435144e-01 2.65410921e-01\n",
      "  2.97436918e-01 7.01279507e-01 6.26222215e-01 1.01230493e-01\n",
      "  2.28366068e-01 5.24535358e-01 6.52710346e-01 7.56430850e-01\n",
      "  6.24634089e-01 4.88485551e-01 7.96432166e-01 9.54924301e-01\n",
      "  3.26724974e-01 2.17760520e-01 8.58664962e-01 2.18628652e-01\n",
      "  9.66933661e-01 8.53120625e-01 3.38973429e-01 7.39868781e-01\n",
      "  9.03364229e-01 7.61756554e-02 9.34553548e-01 5.81339738e-02\n",
      "  5.65207527e-01 5.48724126e-01 2.58715462e-01 4.96656590e-01\n",
      "  9.55197790e-01 6.37779150e-01 4.51730787e-01 8.25725358e-01\n",
      "  6.62394891e-01 7.54715993e-01 8.38833340e-01 4.09216882e-01\n",
      "  6.80233624e-01 2.08249461e-01 7.07934016e-02 2.82841995e-01\n",
      "  3.36616380e-01 5.06706989e-01 2.40693734e-01 9.35623087e-01\n",
      "  3.39909118e-02 4.14600074e-01 6.14999304e-01 5.42621351e-01\n",
      "  7.55466952e-01 2.23765264e-01 4.93283999e-01 9.54503881e-01\n",
      "  8.64514923e-02 6.86301422e-01 7.55480005e-01 1.94842683e-01\n",
      "  2.51701071e-01 2.73442239e-01 4.60034818e-01 3.83433352e-01\n",
      "  5.09572558e-01 6.18548573e-01 8.51104304e-01 2.35314757e-01\n",
      "  4.01878921e-01 6.53874381e-01 6.34533543e-01]]\n",
      "Change in theta: 1.966455541198446e-06\n",
      "---------------------------------------------\n",
      "Iteration number 29 finished, Running time 873.330169916153, Roo2 iteration 29 = -10896833.095162382\n",
      "Iteration 1:\n",
      "Theta: [[6.82575459e-01 2.04523696e-01 3.44629279e-01 5.75119144e-01\n",
      "  1.67800875e-01 3.75650441e-01 1.16357823e-01 8.83589861e-01\n",
      "  5.22044421e-01 3.47343222e-01 8.63888097e-01 1.62104886e-01\n",
      "  1.26224372e-01 5.95408283e-01 9.41485061e-01 7.64167359e-01\n",
      "  5.77497396e-01 7.73456963e-01 1.63614709e-01 7.90844984e-01\n",
      "  9.35697617e-02 2.68799111e-01 3.84679903e-01 4.97089160e-01\n",
      "  4.94992128e-01 6.36577390e-01 3.65360245e-01 2.17185599e-01\n",
      "  4.22691818e-01 8.48223009e-01 8.36637388e-01 9.86258393e-01\n",
      "  4.69590860e-01 5.17770122e-01 1.62611830e-02 8.68940203e-01\n",
      "  6.89869792e-01 6.26076547e-01 4.87931297e-01 5.22125481e-01\n",
      "  4.13606637e-01 4.41757769e-01 7.98438820e-01 4.11091892e-01\n",
      "  7.66487115e-01 2.83452020e-01 6.01825201e-01 7.80457695e-02\n",
      "  9.99855142e-01 2.20902883e-01 7.66521292e-01 6.18849908e-01\n",
      "  6.52122762e-01 9.73281272e-01 3.28907424e-01 2.16803657e-02\n",
      "  3.20347769e-01 1.12523622e-01 5.44508670e-02 9.88056925e-01\n",
      "  6.05823135e-02 8.64992780e-01 4.53339496e-02 4.81478939e-01\n",
      "  7.91210912e-01 3.88685794e-01 4.32181678e-01 7.85731836e-02\n",
      "  1.92394126e-01 9.14167521e-01 7.07728017e-01 5.39104806e-02\n",
      "  8.76095705e-01 5.85052285e-01 5.29700829e-01 9.12787905e-01\n",
      "  7.89525250e-01 6.30190862e-01 9.55838909e-01 4.74091284e-01\n",
      "  4.29753065e-01 9.38229920e-01 6.08041347e-01 7.48824146e-01\n",
      "  5.09965244e-01 8.82830711e-01 9.07737181e-01 5.37811510e-01\n",
      "  5.87249619e-01 1.60274293e-01 8.63053281e-01 9.99112317e-02\n",
      "  9.15926235e-01 1.18105953e-01 5.79019201e-01 3.95719544e-01\n",
      "  2.48622122e-01 9.28313180e-01 2.19445684e-01 6.00854435e-02\n",
      "  9.26524223e-01 7.02472777e-01 4.80497705e-01 4.96211184e-01\n",
      "  6.64501426e-03 1.45519087e-01 4.15120056e-01 3.65170602e-02\n",
      "  7.29442003e-01 5.36883720e-01 3.39504686e-01 6.57801586e-01\n",
      "  1.25030629e-01 4.34492149e-01 8.53933664e-02 9.71366847e-01\n",
      "  2.27627904e-01 7.88513795e-01 8.18932853e-01 6.22109258e-01\n",
      "  9.20613479e-01 3.11790259e-01 1.56012974e-01 9.24314488e-01\n",
      "  5.82789006e-01 9.49799043e-01 3.93005351e-02 7.11922206e-01\n",
      "  5.12350720e-01 7.59410696e-01 8.43600647e-01 1.22675626e-01\n",
      "  4.76999793e-01 4.99928557e-01 3.46122284e-01 6.76131056e-01\n",
      "  1.99227928e-01 3.46427001e-01 7.67291250e-01 6.21694364e-02\n",
      "  4.65668912e-01 1.35891090e-01 2.75139341e-02 8.14449188e-01\n",
      "  5.12288860e-01 2.89726407e-01 2.26167390e-01 8.59434459e-01\n",
      "  8.31774556e-01 7.21034131e-01 2.36539696e-01 3.34415521e-01\n",
      "  5.19241317e-03 2.74528240e-01 9.88394943e-02 8.88447070e-01\n",
      "  6.15310361e-01 3.40358630e-01 8.85034113e-01 4.82987112e-01\n",
      "  6.52143418e-01 5.35473669e-01 8.15437217e-01 4.69734968e-01\n",
      "  9.84675505e-01 4.41147308e-02 8.85728712e-01 6.94572395e-01\n",
      "  8.33652252e-01 4.62207548e-01 6.11428789e-01 2.26864369e-01\n",
      "  7.93885862e-01 2.29918373e-01 4.08263417e-01 4.88010921e-01\n",
      "  3.36784826e-01 3.71375599e-01 6.59478297e-03 3.56354747e-01\n",
      "  7.18397321e-01 1.38386750e-01 9.48269683e-01 1.24280926e-01\n",
      "  8.18100148e-01 7.58733815e-01 5.16492073e-01 3.77496196e-01\n",
      "  1.32117795e-01 8.55644142e-01 9.48062939e-01 2.25141826e-01\n",
      "  5.16201672e-01 7.49111923e-01 6.40111951e-01 8.66499033e-01\n",
      "  1.47639441e-01 7.42609587e-01 6.45744416e-01 7.58026024e-01\n",
      "  8.00799356e-01 2.21043078e-01 6.04962725e-03 6.27652654e-01\n",
      "  2.51519522e-01 7.70843440e-01 1.97898496e-01 9.04975060e-01\n",
      "  8.38362791e-01 4.17782304e-01 7.56719112e-01 5.77383913e-01\n",
      "  9.48723207e-01 6.54361018e-01 5.91209847e-01 9.88766141e-01\n",
      "  5.36363577e-01 9.00716439e-01 1.28369677e-01 3.92143221e-01\n",
      "  9.80303061e-01 1.78507461e-01 5.28773647e-01 2.79126796e-01\n",
      "  8.18325939e-04 4.24812323e-01 3.87224286e-01 8.86489837e-01\n",
      "  3.16551633e-01 2.40264635e-01 1.78154034e-01 5.45052539e-01\n",
      "  9.52085999e-02 1.16803988e-01 4.73102748e-01 2.60937816e-02\n",
      "  9.36595560e-01 4.60460839e-01 9.32285573e-01 8.83192279e-01\n",
      "  6.57963897e-02 3.07766889e-01 1.89249426e-01 8.87250263e-01\n",
      "  6.49866030e-01 2.15844420e-01 1.20435475e-01 8.95820895e-01\n",
      "  9.55027658e-01 5.13472050e-01 2.84495837e-01 2.51498166e-01\n",
      "  6.40168211e-01 7.51372721e-01 3.00916114e-01 5.88151344e-01\n",
      "  4.87423300e-01 8.58465444e-01 1.42286513e-01 7.98698726e-01\n",
      "  2.73854380e-01 3.83842480e-01 8.75999569e-01 5.68032872e-01\n",
      "  9.93546909e-01 2.36378124e-01 1.41590303e-01 1.24076469e-01\n",
      "  5.27914956e-01 3.77272070e-01 1.40590309e-02 5.92230513e-01\n",
      "  2.26234958e-01 2.20739325e-01 9.22165574e-01 3.01010100e-01\n",
      "  8.47041468e-01 4.54766854e-01 7.00677465e-01 4.32780123e-01\n",
      "  2.82785949e-01 1.21311082e-02 9.12971203e-01 9.16909085e-02\n",
      "  4.80189461e-01 7.40669891e-01 1.07389802e-01 5.77285529e-01\n",
      "  2.73554137e-01 8.93726102e-01 8.18055372e-01 1.64507655e-01\n",
      "  3.99041027e-01 2.98323219e-01 1.90893964e-02 3.37458837e-01\n",
      "  8.73766068e-01 5.34798457e-01 1.41360676e-02 1.63439316e-01\n",
      "  7.28874043e-01 6.17560814e-01 5.02949968e-01 4.73833082e-01\n",
      "  9.99537115e-01 4.57534191e-02 1.05367388e-03 8.25584274e-01\n",
      "  3.13225074e-01 2.34302632e-02 5.74201310e-01 3.39244152e-01\n",
      "  2.72271779e-01 6.21531125e-01 7.02704462e-01 1.13288616e-01\n",
      "  4.60252017e-01 1.75230347e-01 8.24625859e-01 7.05606235e-02\n",
      "  6.04574976e-01 8.05314162e-01 9.78896688e-01 5.74009013e-01\n",
      "  5.04732919e-01 2.05057547e-01 2.04868063e-01 9.94133183e-01\n",
      "  4.67067993e-02 9.70120731e-01 9.73005634e-01 1.93461425e-01\n",
      "  9.06032582e-01 1.03418859e-01 2.73921214e-01 1.36033187e-01\n",
      "  2.00725443e-01 5.62378075e-01 6.69036057e-01 8.94450809e-01\n",
      "  1.83291865e-01 6.23320945e-01 3.17597993e-01 6.02594875e-01\n",
      "  1.42341863e-01 4.40906597e-01 1.35389315e-01 4.46421753e-01\n",
      "  8.82995562e-01 7.66580463e-01 8.33558120e-01 6.51921158e-01\n",
      "  4.96797214e-01 1.96715732e-01 4.71321084e-01 8.09279925e-01\n",
      "  9.68957850e-01 1.04470297e-01 9.82774400e-02 5.16572976e-02\n",
      "  5.56999449e-01 3.29734318e-01 4.34735249e-01 7.55833724e-01\n",
      "  8.99340106e-01 7.09528497e-01 2.51640695e-01 7.89587025e-01\n",
      "  1.51675740e-01 2.22350861e-01 2.34432981e-01 5.26804264e-01\n",
      "  7.36720782e-01 4.32956127e-01 2.72615674e-02 7.41960805e-01\n",
      "  5.81492393e-01 6.78602152e-01 3.52228968e-01 3.95201486e-01\n",
      "  2.58024192e-01 9.22855544e-01 5.25845890e-01 8.64688736e-01\n",
      "  8.70307378e-02 6.75853899e-01 1.13520344e-01 8.76543715e-01\n",
      "  8.34092558e-01 2.01397162e-01 1.08227014e-01 4.38361486e-01\n",
      "  2.80599809e-01 4.26080430e-01 7.02976225e-01 6.32364196e-01\n",
      "  3.66942630e-01 3.11668198e-01 3.82077624e-01 7.29479298e-01\n",
      "  9.69949861e-03 6.19886211e-02 3.53991555e-01 2.69966413e-01\n",
      "  9.66151015e-01 7.30329919e-01 9.43748613e-01 7.51399249e-01\n",
      "  5.49232754e-01 5.10254219e-01 6.98478128e-01 5.46680444e-01\n",
      "  4.81243740e-01 5.08645156e-01 1.50956839e-01 3.67524919e-01\n",
      "  8.24299987e-01 9.09534694e-01 9.44650562e-02 8.25109241e-01\n",
      "  2.40397749e-01 9.05179827e-01 3.45905187e-01 2.32408929e-01\n",
      "  2.50718435e-01 4.80716691e-02 9.59477783e-01 1.43135218e-01\n",
      "  9.60572211e-01 6.71632999e-01 3.57144360e-01 9.39048249e-01\n",
      "  3.73593138e-01 2.37527406e-01 1.69447379e-01 7.89375220e-01\n",
      "  1.27685367e-01 6.36395589e-01 2.12398525e-01 7.46968129e-01\n",
      "  6.92355198e-01 9.55250824e-01 1.11659661e-01 5.90114613e-01\n",
      "  7.51927925e-02 6.89098884e-01 1.23331961e-01 8.00807244e-01\n",
      "  4.69707772e-01 3.68754583e-01 5.13027600e-01 9.94912142e-01\n",
      "  9.26170173e-01 5.79565765e-01 5.40857887e-01 9.24877533e-01\n",
      "  4.96936356e-01 3.09740482e-01 9.76926394e-01 8.82782204e-01\n",
      "  8.42212692e-01 5.27687535e-02 5.77242594e-01 9.42875506e-01\n",
      "  9.52964135e-01 8.25981101e-02 9.22208725e-01 9.69216417e-02\n",
      "  6.48957065e-01 9.51240358e-01 1.90833603e-01 5.94356934e-01\n",
      "  8.19267692e-01 8.25588691e-01 9.12821472e-01 7.80938402e-01\n",
      "  1.40324945e-01 3.04267715e-01 1.01069419e-01 2.08816817e-01\n",
      "  8.76161887e-01 5.92045697e-01 6.43819578e-02 6.72103693e-02\n",
      "  8.63127172e-02 2.25005772e-01 6.52609814e-01 6.14435139e-01\n",
      "  8.17668826e-01 4.13474378e-01 5.53463617e-01 9.86160783e-01\n",
      "  9.98206696e-01 7.21736526e-01 6.89882466e-01 1.79589051e-01\n",
      "  3.67416134e-01 5.29403492e-01 9.16704151e-01 2.23360248e-01\n",
      "  1.03078938e-01 7.59486621e-01 5.64298654e-01 4.23060092e-01\n",
      "  6.15092671e-01 2.35915015e-01 8.00336833e-01 9.25107469e-01\n",
      "  8.50865714e-01 5.93770838e-01 1.46472326e-02 9.47516191e-01\n",
      "  1.23542839e-01 6.63007698e-01 5.01393136e-01 1.08924017e-01\n",
      "  1.02242476e-01 7.68052974e-01 4.78186502e-01 7.49539302e-02\n",
      "  6.96290348e-01 8.02064124e-02 4.29919760e-01 5.64907764e-01\n",
      "  3.91941614e-01 5.87261985e-01 4.26966225e-01 4.31611316e-01\n",
      "  6.36844491e-02 3.29837815e-01 1.59524677e-01 7.22781455e-01\n",
      "  1.45320741e-01 3.38391104e-01 8.64594438e-01 7.93643176e-02\n",
      "  8.82649237e-01 1.26561728e-01 4.54622550e-01 7.47075017e-01\n",
      "  8.38649994e-01 7.88242075e-01 4.87982781e-01 1.02035323e-01\n",
      "  7.53825977e-01 8.80504856e-01 5.26669167e-01 1.70833423e-01\n",
      "  4.59580399e-01 9.68492472e-01 3.58482753e-01 5.99296242e-01\n",
      "  9.17982212e-01 7.24718438e-01 1.48541584e-01 5.61829451e-01\n",
      "  8.79344658e-01 3.57900729e-01 9.23143403e-01 3.06277830e-01\n",
      "  1.99896727e-01 5.77203154e-01 5.64178101e-01 6.50677493e-01\n",
      "  8.99025201e-01 9.87217947e-01 1.40564227e-01 4.20670451e-01\n",
      "  8.65571852e-01 4.93328840e-01 8.36058901e-02 7.62236024e-01\n",
      "  8.09696265e-01 7.33152876e-01 8.53778675e-01 7.27791821e-01\n",
      "  7.21597615e-01 5.75476959e-01 1.85635891e-01 7.60942422e-01\n",
      "  8.68989679e-01 9.52527890e-01 1.98961285e-01 7.96139728e-01\n",
      "  7.70855058e-02 9.90883340e-01 1.97837021e-01 4.74462579e-02\n",
      "  4.47471623e-01 7.70387220e-01 3.76061623e-01 8.45133472e-01\n",
      "  3.03901056e-01 2.27219494e-01 7.00408192e-02 3.80356613e-01\n",
      "  7.33162734e-02 7.45900834e-01 9.68577886e-01 1.31822933e-01\n",
      "  1.21212080e-03 3.14205207e-01 4.05176499e-01 8.80300624e-01\n",
      "  6.47540749e-01 1.54627371e-01 1.79618807e-01 4.07200259e-01\n",
      "  6.54980881e-01 5.79014553e-01 2.89477923e-01 9.09078910e-01\n",
      "  9.27796789e-01 9.15115708e-01 1.25445536e-01 1.68090548e-01\n",
      "  5.24881562e-01 6.45078868e-01 5.74250031e-02 8.80104859e-01\n",
      "  4.66111770e-01 4.75796115e-01 9.24948911e-01 5.30053033e-01\n",
      "  3.95272283e-01 7.50719555e-01 2.51714169e-01 9.60989696e-01\n",
      "  6.68700331e-01 8.39522859e-01 9.94643151e-01 7.65510494e-01\n",
      "  5.10790532e-01 3.75965715e-01 4.99000769e-01 5.77293343e-02\n",
      "  7.28734087e-01 7.74329353e-01 7.94853604e-01 4.15578859e-01\n",
      "  4.42387115e-01 2.55214044e-01 6.40363999e-01 3.30230841e-01\n",
      "  1.65405245e-01 3.51088034e-01 6.43735483e-01 4.72092540e-01\n",
      "  2.44012602e-01 4.43202240e-01 5.10146148e-01 4.09464149e-01\n",
      "  2.07559838e-01 8.90967975e-01 9.23906367e-01 6.80916914e-01\n",
      "  1.56222135e-01 3.89122705e-01 7.27522705e-01 6.75599138e-01\n",
      "  8.96701387e-01 2.88811842e-01 5.00050519e-01 7.30120493e-01\n",
      "  1.90256567e-01 5.63509283e-01 2.45780613e-01 3.42290777e-01\n",
      "  4.26177300e-01 8.07080318e-01 4.93458076e-01 8.56649071e-02\n",
      "  7.99071726e-01 3.11808293e-01 7.64814081e-01 7.40976291e-01\n",
      "  4.86191695e-01 1.69103379e-01 4.74847006e-01 3.51404729e-01\n",
      "  9.34621888e-01 9.79417999e-01 3.23968551e-01 4.71717615e-01\n",
      "  6.93371911e-02 9.35394233e-01 5.14908043e-01 1.31702569e-01\n",
      "  2.70941441e-01 1.35230040e-01 9.95838160e-01 1.72705844e-01\n",
      "  4.46682416e-01 4.99669682e-01 8.38713777e-01 2.13997579e-01\n",
      "  9.03479176e-01 9.88386928e-01 8.00239702e-01 5.48061291e-01\n",
      "  5.24025514e-01 5.01595045e-01 1.10482492e-01 8.55224114e-01\n",
      "  8.68576439e-01 9.84734173e-01 7.90516383e-01 5.71470574e-01\n",
      "  2.88337666e-01 9.33101890e-01 8.21236182e-01 6.92289498e-01\n",
      "  2.08974366e-01 5.16272838e-01 7.86260039e-02 2.31584283e-01\n",
      "  9.92604476e-01 6.36648789e-01 2.47829618e-01 8.49988922e-01\n",
      "  3.60313467e-01 1.71887831e-01 1.25642275e-01 4.33523245e-01\n",
      "  3.68703774e-01 9.46486713e-01 7.36791390e-01 3.60604167e-01\n",
      "  8.07359394e-01 3.95648703e-01 4.06503854e-01 7.38390406e-01\n",
      "  8.54094701e-01 6.67346708e-01 7.23899879e-01 1.74531865e-01\n",
      "  9.23474263e-02 8.46755013e-01 3.91487021e-01 8.22163278e-01\n",
      "  2.63940462e-01 4.67600736e-01 2.50136792e-01 5.23067833e-01\n",
      "  2.45277489e-01 5.58069940e-01 1.28892005e-01 2.39360457e-02\n",
      "  3.29897908e-01 8.42651302e-01 4.12462728e-01 7.62082152e-01\n",
      "  1.83395229e-01 3.04329957e-01 5.46358256e-01 7.63158254e-01\n",
      "  8.09633484e-01 2.56459641e-01 9.98524809e-01 7.98322337e-02\n",
      "  7.79710255e-01 8.79006846e-01 6.58055176e-01 9.70740862e-01\n",
      "  1.45861873e-01 9.45155570e-01 5.79538509e-02 4.42632705e-01\n",
      "  7.67008714e-01 7.57399097e-02 1.64861814e-01 6.43936860e-01\n",
      "  5.57356292e-01 8.70988498e-01 2.54184015e-01 3.17584774e-02\n",
      "  3.30298916e-01 4.70766281e-02 8.03734294e-01 2.99280304e-01\n",
      "  7.07831485e-01 3.52638095e-01 3.39161761e-01 2.17998384e-01\n",
      "  3.35998584e-01 1.39127389e-02 7.24118191e-01 8.14956267e-01\n",
      "  2.41686763e-01 1.75036134e-01 2.96068104e-02 9.32525583e-01\n",
      "  4.65675503e-01 9.79172555e-01 8.04204409e-01 1.61070853e-01\n",
      "  3.98149698e-01 1.59936034e-01 4.77531420e-01 2.24098280e-02\n",
      "  7.50475461e-01 8.18733199e-01 8.06219666e-01 5.60817471e-01\n",
      "  9.13323250e-01 8.93213072e-01 2.04388645e-01 3.99976800e-01\n",
      "  6.82214689e-01 4.78424370e-01 2.04225307e-01 8.36657485e-01\n",
      "  7.14375005e-01 2.42751550e-01 6.69398855e-01 4.88104282e-01\n",
      "  9.52274727e-01 2.82862369e-01 9.38250493e-01 7.98686030e-01\n",
      "  8.46975830e-01 8.50874466e-01 2.87023110e-03 2.92008556e-01\n",
      "  7.72451332e-01 7.34962666e-01 4.99529904e-01 9.60728132e-01\n",
      "  2.69300359e-01 7.67452421e-02 8.13096741e-01 9.14853599e-01\n",
      "  8.52983788e-01 1.87465526e-01 9.12961459e-01 8.93473312e-01\n",
      "  1.53607526e-01 8.52540921e-01 4.12794356e-01 6.47764712e-01\n",
      "  4.37672508e-01 6.53469353e-01 8.43435272e-01 2.65411049e-01\n",
      "  2.97437046e-01 7.01279635e-01 6.26222343e-01 1.01230621e-01\n",
      "  2.28366195e-01 5.24535486e-01 6.52710474e-01 7.56430978e-01\n",
      "  6.24634217e-01 4.88485679e-01 7.96432293e-01 9.54924428e-01\n",
      "  3.26725102e-01 2.17760648e-01 8.58665090e-01 2.18628780e-01\n",
      "  9.66933789e-01 8.53120753e-01 3.38973557e-01 7.39868909e-01\n",
      "  9.03364357e-01 7.61757830e-02 9.34553676e-01 5.81341014e-02\n",
      "  5.65207655e-01 5.48724253e-01 2.58715589e-01 4.96656717e-01\n",
      "  9.55197918e-01 6.37779277e-01 4.51730915e-01 8.25725486e-01\n",
      "  6.62395019e-01 7.54716121e-01 8.38833468e-01 4.09217010e-01\n",
      "  6.80233751e-01 2.08249589e-01 7.07935292e-02 2.82842122e-01\n",
      "  3.36616508e-01 5.06707117e-01 2.40693861e-01 9.35623215e-01\n",
      "  3.39910395e-02 4.14600202e-01 6.14999432e-01 5.42621478e-01\n",
      "  7.55467080e-01 2.23765391e-01 4.93284127e-01 9.54504009e-01\n",
      "  8.64516200e-02 6.86301549e-01 7.55480133e-01 1.94842811e-01\n",
      "  2.51701199e-01 2.73442366e-01 4.60034946e-01 3.83433480e-01\n",
      "  5.09572686e-01 6.18548700e-01 8.51104431e-01 2.35314884e-01\n",
      "  4.01879048e-01 6.53874509e-01 6.34533671e-01]]\n",
      "Change in theta: 1.977275121360013e-06\n",
      "---------------------------------------------\n",
      "Iteration number 30 finished, Running time 1207.933516740799, Roo2 iteration 30 = -19331953.856826793\n",
      "TOTAL RUNNING TIME = 13681.916965007782.\n",
      "----------------------------------------------RESULTS----------------------------------------------\n",
      "OverallModel Roo2: -4805491.540162884 || Average of Single Iterations Roo2: -9228308.244891252\n"
     ]
    }
   ],
   "source": [
    "# Results Elastic Net with Huber loss function\n",
    "ENet_Huber_Roos, ENet_Huber_iteravg_Roos = ENet_with_huber(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso + H (not adjusted to newest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Lasso_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Lasso Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [1], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        LASH_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = LASH_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Lasso_with_huber' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Results Lasso Regression with Huber loss function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m Lasso_with_huber_scores \u001b[38;5;241m=\u001b[39m Lasso_with_huber(Dependent\u001b[38;5;241m=\u001b[39my, Predictors\u001b[38;5;241m=\u001b[39mX, stock_weights\u001b[38;5;241m=\u001b[39mweights)\n\u001b[1;32m      3\u001b[0m Lasso_with_huber_scores\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Lasso_with_huber' is not defined"
     ]
    }
   ],
   "source": [
    "# Results Lasso Regression with Huber loss function\n",
    "Lasso_with_huber_scores = Lasso_with_huber(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "Lasso_with_huber_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge + H (not adjusted to newest code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        RIDH_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = RIDH_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.218e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.218e+02, tolerance: 8.441e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.538e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.538e+02, tolerance: 1.108e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+02, tolerance: 1.230e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.472e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.472e+02, tolerance: 1.895e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.169e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.169e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24203593905499088"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Ridge Regression with Huber loss function\n",
    "Ridge_with_huber_scores = Ridge_with_huber(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "Ridge_with_huber_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - GLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLM(y, X,stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = X.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    tuning_par = {\n",
    "    #'knots': [3],\n",
    "    'group_reg':[1e-4,1e-1],\n",
    "    'l1_reg': [1e-4,0],\n",
    "    'groups': [],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "\n",
    "    #GETTING THE SPLINES\n",
    "    spline_data = pd.DataFrame(np.ones((X.shape[0],1)),index=X.index,columns=['const'])\n",
    "    for i in X.columns:\n",
    "        i_dat = X.loc[:,i]\n",
    "        i_sqr = i_dat**2\n",
    "        i_cut, bins = pd.cut(i_dat, 3, right=True, ordered=True, retbins=True)\n",
    "        i_dum = pd.get_dummies(i_cut)\n",
    "        for j in np.arange(3):\n",
    "            i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
    "        i_dum.columns = [f\"{i}_{k}\" for k in np.arange(1,3+1)]\n",
    "        spline_data = pd.concat((spline_data,i_dat,i_dum),axis=1)\n",
    "\n",
    "\n",
    "    def run_iteration(start_year):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = spline_data[(X.index.year < end_train_year)]\n",
    "        X_test = spline_data[(X.index.year >= end_validation_year) & (X.index.year < end_test_year)]\n",
    "        X_val = spline_data[(X.index.year >= end_train_year) & (X.index.year < end_validation_year)]\n",
    "        y_train = y[(y.index.year < end_train_year)].values.ravel()\n",
    "        y_test = y[(y.index.year >= end_validation_year) & (y.index.year < end_test_year)].values.ravel()\n",
    "        y_val = y[(y.index.year >= end_train_year) & (y.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        groups = [0]+flatten([list(np.repeat(i,3+1))[:] for i in np.arange(1,X.shape[1]+1)])\n",
    "        tuning_par['groups'] = groups\n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(GroupLasso, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        GL=GroupLasso(groups=best_par['groups'], group_reg=best_par['group_reg'], l1_reg=best_par['l1_reg'], fit_intercept=False, random_state=best_par['random_state'],supress_warning=True).fit(X_train,y_train)\n",
    "        #GL=GroupLasso(groups=best_par.groups,group_reg=best_par.lmd,l1_reg=best_par.l1_reg,fit_intercept=False,random_state=best_par.random_state)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = GL.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "\n",
    "        return iter_difference_list, iter_actual_list\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(run_iteration)(yrs[i]) for i in range(len(yrs) - initial_train_years - validation_years)\n",
    "    )\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.315774\n",
      "1957-01-01    0.344997\n",
      "1957-01-01    0.328586\n",
      "1957-01-01    0.361710\n",
      "1957-01-01    0.328800\n",
      "                ...   \n",
      "2016-12-01    0.329053\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.318052\n",
      "2016-12-01    0.350582\n",
      "2016-12-01    0.363564\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "                ...   \n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.002201\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "             ... \n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "                ...   \n",
      "2016-12-01    0.000047\n",
      "2016-12-01    0.008226\n",
      "2016-12-01    0.050468\n",
      "2016-12-01    0.041507\n",
      "2016-12-01    0.000449\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "                ...   \n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "             ... \n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2548427126697421"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results GLM\n",
    "GLM_Roos, GLM_iteravg_Roos = GLM(y=y, X=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mom1m</th>\n",
       "      <th>dy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.440062</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.414635</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.428776</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.400577</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.428589</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.428368</td>\n",
       "      <td>-0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.286416</td>\n",
       "      <td>-0.911305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.438039</td>\n",
       "      <td>-0.777349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.409901</td>\n",
       "      <td>-0.798266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.399037</td>\n",
       "      <td>-0.980820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359357 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               mom1m        dy\n",
       "Date                          \n",
       "1957-01-01 -0.440062  0.000000\n",
       "1957-01-01 -0.414635  0.000000\n",
       "1957-01-01 -0.428776  0.000000\n",
       "1957-01-01 -0.400577  0.000000\n",
       "1957-01-01 -0.428589  0.000000\n",
       "...              ...       ...\n",
       "2016-12-01 -0.428368 -0.995178\n",
       "2016-12-01 -0.286416 -0.911305\n",
       "2016-12-01 -0.438039 -0.777349\n",
       "2016-12-01 -0.409901 -0.798266\n",
       "2016-12-01 -0.399037 -0.980820\n",
       "\n",
       "[359357 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred = ['mom1m', 'dy']\n",
    "X_red_rf = X[rf_pred]\n",
    "X_red_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_F(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Random Forest\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3,6],\n",
    "    'max_features': [30,50,100],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(RF, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        RF_SP500 = RF(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], max_features=best_par['max_features'], \n",
    "               random_state=best_par['random_state']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = RF_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Random Forest\n",
    "RF_Roos, RF_iteravg_Roos = Random_F(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Gradient Boosted Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBRT(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Gradient Boosted Regression Tree\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Predictors.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': range(1, 150),\n",
    "    'max_depth': range(1,2),\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(GradientBoostingRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        GBRT_SP500 = GradientBoostingRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], learning_rate=best_par['learning_rate']).fit(X_train, y_train)\n",
    "\n",
    "        r_stock_pred = GBRT_SP500.predict(X_test)\n",
    "   \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function and get results\n",
    "GBRT_Roos, GBRT_iteravg_Roos = GBRT(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "light version os GBRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_huber_obj(y_true, y_pred):\n",
    "    xi = 0.999\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    grad = np.zeros(N)\n",
    "    try:\n",
    "        grad[ind_m] = (-2*(y_true-y_pred))[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_u] = np.repeat(2*xi,N)[ind_u]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        grad[ind_l] = np.repeat(-2*xi,N)[ind_l]\n",
    "    except:\n",
    "        pass\n",
    "    return grad/N\n",
    "\n",
    "# hessian of huber loss with respect to y_pred\n",
    "def hess_huber_obj(y_true, y_pred):\n",
    "    xi = 0.999\n",
    "    y_true, y_pred = np.array(y_true).flatten(), np.array(y_pred).flatten()\n",
    "    N = len(y_true)\n",
    "    resid = y_true - y_pred\n",
    "    ind_m = np.where(np.abs(resid)<=xi)\n",
    "    ind_u = np.where(resid>xi)\n",
    "    ind_l = np.where(resid< -xi)\n",
    "    hess = np.zeros(N)\n",
    "    try:\n",
    "        hess[ind_m] = np.repeat(2,N)[ind_m]\n",
    "    except:\n",
    "        pass\n",
    "    return hess/N\n",
    "\n",
    "# huber loss for lgbm\n",
    "def huber_obj(y_true, y_pred):\n",
    "    grad = grad_huber_obj(y_true, y_pred)\n",
    "    hess = hess_huber_obj(y_true, y_pred)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/saracerqueira/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: '/usr/local/opt/libomp/lib/libomp.dylib'\n  Referenced from: '/Users/saracerqueira/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so'\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/__init__.py:8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"LightGBM, Light Gradient Boosting Machine.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/microsoft/LightGBM/graphs/contributors.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopException, early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/basic.py:247\u001b[0m\n\u001b[1;32m    245\u001b[0m     _LIB \u001b[38;5;241m=\u001b[39m Mock(ctypes\u001b[38;5;241m.\u001b[39mCDLL)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     _LIB \u001b[38;5;241m=\u001b[39m _load_lib()\n\u001b[1;32m    250\u001b[0m _NUMERIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    251\u001b[0m _ArrayLike \u001b[38;5;241m=\u001b[39m Union[List, np\u001b[38;5;241m.\u001b[39mndarray, pd_Series]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/basic.py:232\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load LightGBM library.\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m lib_path \u001b[38;5;241m=\u001b[39m find_lib_path()\n\u001b[0;32m--> 232\u001b[0m lib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mcdll\u001b[38;5;241m.\u001b[39mLoadLibrary(lib_path[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    233\u001b[0m lib\u001b[38;5;241m.\u001b[39mLGBM_GetLastError\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[1;32m    234\u001b[0m callback \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCFUNCTYPE(\u001b[38;5;28;01mNone\u001b[39;00m, ctypes\u001b[38;5;241m.\u001b[39mc_char_p)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ctypes/__init__.py:454\u001b[0m, in \u001b[0;36mLibraryLoader.LoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dlltype(name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/saracerqueira/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: '/usr/local/opt/libomp/lib/libomp.dylib'\n  Referenced from: '/Users/saracerqueira/anaconda3/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so'\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LIGHT_GBRT(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Gradient Boosted Regression Tree\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Predictors.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    tuning_par =  {\n",
    "    'objective':[None, huber_obj],\n",
    "    'max_depth':[1,2],\n",
    "    'n_estimators':[10,50,100,200,500,1000],\n",
    "    'random_state':[12308],\n",
    "    'learning_rate':[.01,.1]\n",
    "}\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(LGBMRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        LGBM_SP500 = LGBMRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], learning_rate=best_par['learning_rate']).fit(X_train, y_train)\n",
    "\n",
    "        r_stock_pred = LGBM_SP500.predict(X_test)\n",
    "   \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LGBMRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run function and get results\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m LGBM_Roos, LGBM_iteravg_Roos \u001b[38;5;241m=\u001b[39m LIGHT_GBRT(Dependent\u001b[38;5;241m=\u001b[39my, Predictors\u001b[38;5;241m=\u001b[39mX, stock_weights\u001b[38;5;241m=\u001b[39mweights)\n",
      "Cell \u001b[0;32mIn[37], line 47\u001b[0m, in \u001b[0;36mLIGHT_GBRT\u001b[0;34m(Dependent, Predictors, stock_weights, initial_train_years, validation_years, test_years)\u001b[0m\n\u001b[1;32m     44\u001b[0m y_val \u001b[38;5;241m=\u001b[39m Dependent[(Dependent\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_train_year) \u001b[38;5;241m&\u001b[39m (Dependent\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m<\u001b[39m end_validation_year)]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# This part runs the tuning to find the best combination of the tuning parameters for every split\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m best_par \u001b[38;5;241m=\u001b[39m val_fun(LGBMRegressor, params\u001b[38;5;241m=\u001b[39mtuning_par, X_trn\u001b[38;5;241m=\u001b[39mX_train, y_trn\u001b[38;5;241m=\u001b[39my_train, X_vld\u001b[38;5;241m=\u001b[39mX_val, y_vld\u001b[38;5;241m=\u001b[39my_val)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Now we test the model\u001b[39;00m\n\u001b[1;32m     50\u001b[0m LGBM_SP500 \u001b[38;5;241m=\u001b[39m LGBMRegressor(n_estimators\u001b[38;5;241m=\u001b[39mbest_par[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m], max_depth\u001b[38;5;241m=\u001b[39mbest_par[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m], learning_rate\u001b[38;5;241m=\u001b[39mbest_par[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LGBMRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# run function and get results\n",
    "LGBM_Roos, LGBM_iteravg_Roos = LIGHT_GBRT(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs XGBoost\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [500,600,800,1000],\n",
    "    'max_depth': [1,2],\n",
    "    'random_state': [12308],\n",
    "    #'learning_rate': [.01]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(XGBRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        XGB = XGBRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = XGB.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results XGBoost\n",
    "XGB_Roos, XGB_iteravg_Roos = XGBoost(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: BART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP.bart import BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BARTrees(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Bayesian Addetive Regression Tree\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    tuning_par = {\n",
    "    'num_trees': [100,200,300],\n",
    "    'burnin': [50,150,200],\n",
    "    'max_stages': [500,1000,2000]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(BART, params=tuning_par, X_trn=np.asarray(X_train), y_trn=y_train, X_vld=np.asarray(X_val), y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        BART_SP500 = BART(num_trees=best_par['num_trees'], burnin=best_par['burnin'], max_stages=best_par['max_stages']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = BART_SP500.predict(np.asarray(X_test))\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results BART\n",
    "BART_Roos, BART_iteravg_Roos = BARTrees(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that adds Bagging to the Random Forest\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3,6],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(RF, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        BAG_SP500 = RF(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], max_features=X_train.shape[1], \n",
    "               random_state=best_par['random_state']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = BAG_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Bagging\n",
    "Bagging_Roos, Bagging_iteravg_Roos = Bagging(Dependent=y, Predictors=X, stock_weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_function(Dependent, Predictors, stock_weights, num_layers, ensemble = 10, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that creates Neural Network\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - num_layers: The number of layers in the Neural Network\n",
    "    - ensemble: Amount of Neural Networks to be trained on same data.\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    start_gen = time.time()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    iterations_Roo2 = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start = time.time()\n",
    "        iter_difference_list = []\n",
    "        iter_actual_list = []\n",
    "        \n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "         \n",
    "        tuning_par = {\n",
    "        'n_layers': [num_layers],\n",
    "        'loss': ['mse'],\n",
    "        'l1': [1e-5, 1e-3],\n",
    "        'learning_rate': [.001, .01],\n",
    "        'batch_size': [10000],\n",
    "        'epochs': [100],\n",
    "        'batch_norm': [True],\n",
    "        'random_state': [1],\n",
    "        'patience': [5],\n",
    "        'verbose': [0],\n",
    "        'monitor': ['val_loss']}\n",
    "        # NN class\n",
    "        class NN(nn.Module):\n",
    "            def __init__(\n",
    "                self, n_layers=1, loss='mse', l1=1e-5, l2=0, learning_rate=.01, batch_norm=True, patience=5,\n",
    "                epochs=100, batch_size=10000, verbose=1, random_state=1, monitor='val_loss', base_neurons=5\n",
    "            ):\n",
    "                super(NN, self).__init__()\n",
    "                self.n_layers = n_layers\n",
    "                self.l1 = l1\n",
    "                self.l2 = l2\n",
    "                self.learning_rate = learning_rate\n",
    "                self.batch_norm = batch_norm\n",
    "                self.patience = patience\n",
    "                self.epochs = epochs\n",
    "                self.batch_size = batch_size\n",
    "                self.verbose = verbose\n",
    "                self.monitor = monitor\n",
    "                self.base_neurons = base_neurons\n",
    "                self.random_state = random_state\n",
    "\n",
    "                # Initialize model layers\n",
    "                self.layers = nn.ModuleList()\n",
    "                input_size, output_size = None, 1\n",
    "\n",
    "                for i in range(self.n_layers, 0, -1):\n",
    "                    in_features = input_size if input_size is not None else X_train.shape[1]\n",
    "                    out_features = 2 ** (self.base_neurons - (self.n_layers - i))\n",
    "                    self.layers.append(nn.Linear(in_features, out_features))\n",
    "                    self.layers.append(nn.ReLU())\n",
    "                    input_size = out_features\n",
    "                    if self.batch_norm:\n",
    "                        self.layers.append(nn.BatchNorm1d(out_features))\n",
    "\n",
    "                self.layers.append(nn.Linear(input_size, output_size))\n",
    "\n",
    "                # Loss function\n",
    "                self.criterion = nn.L1Loss()\n",
    "\n",
    "                # Optimizer\n",
    "                self.optimizer = Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.l1 + self.l2)\n",
    "\n",
    "            def forward(self, x):\n",
    "                for layer in self.layers:\n",
    "                    x = layer(x)\n",
    "                return x\n",
    "\n",
    "            def fit(self, X_train, y_train, X_val, y_val):\n",
    "                torch.manual_seed(self.random_state)\n",
    "                np.random.seed(self.random_state)\n",
    "                random.seed(self.random_state)\n",
    "\n",
    "                X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "                y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "                X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device)\n",
    "                y_val_tensor = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "\n",
    "\n",
    "                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=False, pin_memory=True, num_workers=4) # Change this depending on number of CPU's\n",
    "\n",
    "                early_stop_counter = 0\n",
    "                best_loss = float('inf')\n",
    "\n",
    "                for epoch in range(self.epochs):\n",
    "                    self.train()\n",
    "                    for inputs, targets in train_loader:\n",
    "                        self.optimizer.zero_grad()\n",
    "                        outputs = self(inputs)\n",
    "                        loss = self.criterion(outputs, targets)\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    # Validation loss\n",
    "                    self.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self(X_val_tensor)\n",
    "                        val_loss = self.criterion(outputs, y_val_tensor)\n",
    "\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "\n",
    "                    if early_stop_counter >= self.patience:\n",
    "                        print(\"Early stopping.\")\n",
    "                        break\n",
    "\n",
    "                    if self.verbose and epoch % self.verbose == 0:\n",
    "                        print(f\"Epoch {epoch + 1}/{self.epochs}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "                return self\n",
    "\n",
    "            def predict(self, X):\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_tensor = torch.tensor(X.values, dtype=torch.float32).to(device)\n",
    "                    return self(X_tensor).cpu().numpy()\n",
    "\n",
    "\n",
    "        # Ensemble\n",
    "        ensemble_predictions = []\n",
    "        for _ in range(ensemble):\n",
    "            # Create and fit a new neural network instance\n",
    "            best_NN = val_fun_NN(NN, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "            # Store prediction for this model\n",
    "            ensemble_predictions.append(best_NN.predict(X_test).reshape(-1))  \n",
    "\n",
    "        # Average predictions from all models in the ensemble\n",
    "        r_stock_pred = np.mean(ensemble_predictions, axis=0)\n",
    "\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Portofolio test\n",
    "        dates = weights_test.index\n",
    "        \n",
    "        # Calculate portfolio return actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            \n",
    "            # Calculate weighted average return for the month\n",
    "            return_test = np.sum(month_weights['weight'] * month_y_test)\n",
    "            return_pred = np.sum(month_weights['weight'] * month_y_pred)\n",
    "\n",
    "            # Directly store the monthly values in the lists\n",
    "            r_port_difference_list.append((return_test - return_pred)**2)\n",
    "            r_port_actual_list.append(return_test**2)\n",
    "            iter_difference_list.append((return_test - return_pred)**2)\n",
    "            iter_actual_list.append(return_test**2)\n",
    "        \n",
    "        iter_Roo2 = R_oos(iter_difference_list,  iter_actual_list)\n",
    "        iterations_Roo2.append(iter_Roo2)\n",
    "        \n",
    "        stop = time.time()\n",
    "        print(f'Iteration number {i+1} finished, Running time {stop-start}, Roo2 iteration {i+1} = {iter_Roo2}')\n",
    "        \n",
    "    stop_gen = time.time()\n",
    "    print(f'TOTAL RUNNING TIME = {stop_gen-start_gen}.')\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "    Iterations_average_Roos = np.mean(iterations_Roo2)\n",
    "    \n",
    "    print('-'*46+'RESULTS'+'-'*46)\n",
    "    print(f'OverallModel Roo2: {Model_Roos} || Average of Single Iterations Roo2: {Iterations_average_Roos}')\n",
    "        \n",
    "    return Model_Roos, Iterations_average_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to CUDA if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN1-Regression-[32(relu)-1(linear)]\n",
    "NN_1_Roos, NN_1_iteravg_Roos = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=1, initial_train_years=18, validation_years=12, test_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "NN_2_Roos, NN_2_iteravg_Roos = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=2, initial_train_years=18, validation_years=12, test_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "NN_3_Roos, NN_3_iteravg_Roos = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=3, initial_train_years=18, validation_years=12, test_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "NN_4_Roos, NN_4_iteravg_Roos = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=4, initial_train_years=18, validation_years=12, test_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "NN_5_Roos, NN_5_iteravg_Roos = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=5, initial_train_years=18, validation_years=12, test_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
