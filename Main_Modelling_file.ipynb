{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and changing it to be usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datafile\n",
    "y = pd.read_csv('Dependent_y.csv', header=0, index_col=0)\n",
    "X = pd.read_csv('Features_X.csv', header=0, index_col=0)\n",
    "\n",
    "y.fillna(0, inplace=True)\n",
    "y.index = pd.to_datetime(y.index)\n",
    "X.index = pd.to_datetime(X.index)\n",
    "\n",
    "weights = pd.read_csv('Stocks_weights.csv', header=0)\n",
    "weights.index = weights['Date']\n",
    "weights = weights.drop('Date', axis=1)\n",
    "weights.index = pd.to_datetime(weights.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For small dataset to test methods\n",
    "eight_pred = ['mvel1', 'beta', 'betasq', 'chmom', 'dolvol', 'idiovol', 'indmom', 'mom1m']\n",
    "X_eight_pred = X[eight_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def R_oos(num, den):\n",
    "    R_oos_val = 1 - (np.sum(num)/np.sum(den))\n",
    "    return R_oos_val\n",
    "\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld, max_iter=10, tol=1e-4):\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    no_improvement_count = 0\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "        else:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_param = param\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= max_iter:\n",
    "                    break\n",
    "            if abs(ros - best_ros) < tol:\n",
    "                break\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - OLS(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with expanding window and with and without Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_regression_OLS(Dependent, Predictors, stock_weights, loss = 'OLS', initial_train_years = 18, validation_years = 12, test_years = 1):\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list =[]\n",
    "    years = Dependent.index.year.unique()\n",
    "\n",
    "    for i in range(len(years) - initial_train_years - validation_years):\n",
    "        start_year = years[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "           \n",
    "        if loss == 'OLS':\n",
    "            model = LinearRegression()\n",
    "        elif loss == 'Huber':\n",
    "            model = HuberRegressor(epsilon = 99.9) # Set the epsilon to 99.9%.\n",
    "        else:\n",
    "            raise ValueError(\"Invlaid loss function. Use OLS or Huber.\")\n",
    "        \n",
    "        # Training the model\n",
    "        OLS3 = model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict returns at stock level\n",
    "        r_stock_pred = OLS3.predict(X_test)\n",
    "        \n",
    "        # Portofolio test\n",
    "        r_portfolio_test = np.dot(y_test, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "        \n",
    "        # Calculate portfolio return prediction using the given stock weights\n",
    "        r_portfolio_pred = np.dot(r_stock_pred, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "\n",
    "        # Appending to lists\n",
    "        r_port_difference_list.append((r_portfolio_test-r_portfolio_pred)**2)\n",
    "        r_port_actual_list.append(r_portfolio_test**2)\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "       \n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007429295085335963"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUT-OF-SAMPLE R^2 for OLS-3\n",
    "OLS_3pred_Roos=expanding_regression_OLS(y,X_3pred,stock_weights=weights)\n",
    "OLS_3pred_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07175829363270425"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUT-OF-SAMPLE R^2 for OLS-3 with Huber Loss function\n",
    "OLS_3pred_Roos_H = expanding_regression_OLS(y, X_3pred,stock_weights=weights, loss = 'Huber')\n",
    "OLS_3pred_Roos_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dimension Reduction: PCR and PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcr(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \n",
    "    # Lists to save outcomes.\n",
    "    component_counts = []\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "\n",
    "    yrs = Dependent.index.year.unique() # List with all the years. (1957-2016)\n",
    "    n_components_list = range(1, 7) # To determine the number of components that are tested.\n",
    "    best_components = None # Initialize and later save the best amount of components.\n",
    "    best_r2 = -np.inf  # Initialize with negative infinity to find the maximum R-squared\n",
    "\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # Testing the different components in PCA.\n",
    "        for n_component in n_components_list:\n",
    "\n",
    "            pca = PCA(n_components=n_component)\n",
    "            X_train_pca = pca.fit_transform(X_train) \n",
    "            X_val_pca = pca.transform(X_val)\n",
    "\n",
    "            # Fit Linear Regression on the training set\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_pca, y_train)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_val_pred = model.predict(X_val_pca)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "            # Update best components if the current number of components yields a higher R-squared\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_components = n_component\n",
    "\n",
    "        # Save the best number of components to the list\n",
    "        component_counts.append(best_components)\n",
    "\n",
    "        # Use the best number of components to fit the final model on the combined training and validation sets\n",
    "        best_pca = PCA(n_components=best_components)\n",
    "        X_train_pca = best_pca.fit_transform(X_train)\n",
    "        X_test_pca = best_pca.transform(X_test)\n",
    "        \n",
    "        # Best Model\n",
    "        PCASP500 = LinearRegression()\n",
    "        PCASP500.fit(X_train_pca, y_train)\n",
    "\n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = PCASP500.predict(X_test_pca)\n",
    "        \n",
    "        # Portfolio test\n",
    "        r_portfolio_test = np.dot(y_test, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "        \n",
    "        # # Calculate portfolio return prediction using the given stock weights\n",
    "        r_portfolio_pred = np.dot(r_stock_pred, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "\n",
    "        # Appending Values to lists\n",
    "        r_port_difference_list.append((r_portfolio_test-r_portfolio_pred)**2)\n",
    "        r_port_actual_list.append(r_portfolio_test**2)\n",
    "        \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCR_Roo2 = pcr(Dependent=y, Predictors=X_eight_pred, stock_weights=weights)\n",
    "PCR_Roo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_pls(y_variable, x_variables,stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \n",
    "    # Lists to save outcomes.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list =[]\n",
    "    component_counts = []\n",
    "\n",
    "    years = x_variables.index.year.unique()\n",
    "    n_components_list = range(1, 7) # To determine the number of components that are tested.\n",
    "    best_components = None # Initialize and later save the best amount of components.\n",
    "    best_r2 = -np.inf  # Initialize with negative infinity to find the maximum R-squared\n",
    "\n",
    "    for i in range(len(years) - initial_train_years - validation_years): # i is 0 to the last start_year which is the last year minus the start training years and validation years.\n",
    "        start_year = years[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = x_variables[(x_variables.index.year < end_train_year)]\n",
    "        X_test = x_variables[(x_variables.index.year >= end_validation_year) & (x_variables.index.year < end_test_year)]\n",
    "        X_val = x_variables[(x_variables.index.year >= end_train_year) & (x_variables.index.year < end_validation_year)]\n",
    "        y_train = y_variable[(y_variable.index.year < end_train_year)].values.ravel()\n",
    "        y_test = y_variable[(y_variable.index.year >= end_validation_year) & (y_variable.index.year < end_test_year)].values.ravel()\n",
    "        y_val = y_variable[(y_variable.index.year >= end_train_year) & (y_variable.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # Testing the different components in PLS.\n",
    "        for n_component in n_components_list:\n",
    "\n",
    "            # Train the model once on the training set\n",
    "            pls = PLSRegression(n_components=n_component)\n",
    "            pls.fit(X_train, y_train) \n",
    "\n",
    "           # Predict on the validation set\n",
    "            y_val_pred = pls.predict(X_val)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "            # Update best components if the current number of components yields a higher R-squared\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_components = n_component\n",
    "\n",
    "        # Save the best number of components to the list\n",
    "        component_counts.append(best_components)\n",
    "\n",
    "        # Use the best number of components to fit the final model \n",
    "        best_pls = PLSRegression(n_components=best_components)\n",
    "        best_pls.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the final model on the test set\n",
    "        r_stock_pred = best_pls.predict(X_test)\n",
    "        \n",
    "        # Portofolio test\n",
    "        r_portfolio_test = np.dot(y_test, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "        \n",
    "        # Calculate portfolio return prediction using the given stock weights\n",
    "        r_portfolio_pred = np.dot(r_stock_pred.T, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "       \n",
    "        # Appending to lists\n",
    "        r_port_difference_list.append((r_portfolio_test-r_portfolio_pred)**2)\n",
    "        r_port_actual_list.append(r_portfolio_test**2)\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "       \n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs PLS and saves the results\n",
    "r_squared_scores_pls = walk_forward_pls(y, X_eight_pred,stock_weights=weights ,initial_train_years=18, validation_years=12, test_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05519827043915937\n"
     ]
    }
   ],
   "source": [
    "# Prints results.\n",
    "print(r_squared_scores_pls)\n",
    "#print(component_counts_pls)\n",
    "#print(np.mean(r_squared_scores_pls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred = ['mom1m', 'dy']\n",
    "X_red_rf = X[rf_pred]\n",
    "X_red_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "from datetime import datetime\n",
    "\n",
    "def Random_F(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3,6],\n",
    "    'max_features': [30,50,100],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(RF, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        RF_SP500 = RF(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], max_features=best_par['max_features'], \n",
    "               random_state=best_par['random_state']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = RF_SP500.predict(X_test)\n",
    "        \n",
    "        # Portofolio test\n",
    "        r_portfolio_test = np.dot(y_test, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "        \n",
    "        # Calculate portfolio return prediction using the given stock weights\n",
    "        r_portfolio_pred = np.dot(r_stock_pred, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "\n",
    "        # Appending values to lists\n",
    "        r_port_difference_list.append((r_portfolio_test-r_portfolio_pred)**2)\n",
    "        r_port_actual_list.append(r_portfolio_test**2)\n",
    "        \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "        \n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_Roo2 = Random_F(Dependent=y,Predictors=X_red_rf, stock_weights=weights)   \n",
    "RF_Roo2\n",
    "#(Running time with 2 predictors: 14 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of OLS-3\n",
    "OLS-3 includes size = 'mvel1', Book-to-Market = 'bm', momentum = 'mom12m' Does it also include 'mom1m','mom6m','mom12m','mom36m'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset for OLS-3 (Need to determine the momentum variable!)\n",
    "three_predictors = ['mvel1', 'bm','mom12m']\n",
    "X_3pred = X[three_predictors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBRT(Dependent, Predictors, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    yrs = data.index.year.unique()\n",
    "    r_squared_scores = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': range(1, 150),\n",
    "    'max_depth': range(1,5),\n",
    "    'learning_rate': range(0,1)\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        train_data = data[(data.index.year < end_train_year)]\n",
    "        val_data = data[(data.index.year >= end_train_year) & (data.index.year < end_validation_year)]\n",
    "        test_data = data[(data.index.year >= end_validation_year) & (data.index.year < end_test_year)]\n",
    "\n",
    "        X_train = train_data[Predictors]\n",
    "        X_val = val_data[Predictors]\n",
    "        X_test = test_data[Predictors]\n",
    "        y_train = train_data[Dependent].values.ravel()\n",
    "        y_val = val_data[Dependent].values.ravel()\n",
    "        y_test = test_data[Dependent].values.ravel()\n",
    "\n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(GradientBoostingRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        GBRT_SP500 = GradientBoostingRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], learning_rate=best_par['learning_rate']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared\n",
    "        y_test_pred = GBRT_SP500.predict(X_test)\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        r_squared_scores.append(test_r2)\n",
    "        \n",
    "    return r_squared_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function and print resulting R2 value\n",
    "GBRT_R2 = GBRT(Dependent=y_variable,Predictors=all_predictors)   \n",
    "print(GBRT_R2)\n",
    "print(np.mean(GBRT_R2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    Roos_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [500,600,800,1000],\n",
    "    'max_depth': [1,2],\n",
    "    'random_state': [12308],\n",
    "    #'learning_rate': [.01]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(XGBRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        XGB = XGBRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = XGB.predict(X_test)\n",
    "        \n",
    "        # Portofolio test\n",
    "        r_portfolio_test = np.dot(y_test, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "        \n",
    "        # Calculate portfolio return prediction using the given stock weights\n",
    "        r_portfolio_pred = np.dot(r_stock_pred, stock_weights[(stock_weights.index.year >= end_validation_year) & (stock_weights.index.year < end_test_year)])\n",
    "\n",
    "        # Calculate Roos\n",
    "        Roos_val = R_oos(r_portfolio_test, r_portfolio_pred)\n",
    "        Roos_list.append(Roos_val)\n",
    "        \n",
    "    return Roos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_Roo2 = XGBoost(Dependent=y,stock_weights=weights,Predictors=X_red_rf)   \n",
    "XGB_Roo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(XGB_Roo2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
