{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.linear_model import (LinearRegression, \n",
    "                                  HuberRegressor,\n",
    "                                  ElasticNet)\n",
    "from sklearn.metrics import (mean_squared_error, \n",
    "                             r2_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (TimeSeriesSplit, \n",
    "                                     ParameterGrid)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import (GradientBoostingRegressor,\n",
    "                              RandomForestRegressor as RF)\n",
    "from group_lasso import GroupLasso\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and changing it to be usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datafiles into dataframes.\n",
    "# y = the excess returns. X has the predictors.\n",
    "y = pd.read_csv('Dependent_y.csv', header=0, index_col=0)\n",
    "X = pd.read_csv('Features_X.csv', header=0, index_col=0)\n",
    "\n",
    "\n",
    "y.fillna(0, inplace=True) # Do we need this? Isn't it already done in Pre-Processing?\n",
    "\n",
    "# Converting data to Dates as index\n",
    "y.index = pd.to_datetime(y.index, format=\"%Y-%m\").to_period('M')\n",
    "X.index = pd.to_datetime(X.index, format=\"%Y-%m\").to_period('M')\n",
    "\n",
    "# Creating the weights of the stocks compared to the portfolio\n",
    "weights = pd.read_csv('Stocks_weights.csv', header=0)\n",
    "weights.index = weights['Date']\n",
    "weights = weights.drop('Date', axis=1)\n",
    "weights.index = pd.to_datetime(weights.index, format=\"%Y-%m\").to_period('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def R_oos(num, den):\n",
    "    \"\"\"\n",
    "Calculates the Out Of Sample R-squared\n",
    "Input: \n",
    "    - num: Numerator\n",
    "    - den: Denomenator\n",
    "\n",
    "    Output: Out of sample R-squared\n",
    "    \"\"\"\n",
    "    R_oos_val = 1 - (np.sum(num)/np.sum(den))\n",
    "    return R_oos_val\n",
    "\n",
    "\n",
    "\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld, max_iter=10, tol=1e-4):\n",
    "    \"\"\"\n",
    "Validates a model to get the best parameters\n",
    "Input: \n",
    "    - model: The model we are validating.\n",
    "    - params: A dictionary of parameters.\n",
    "    - X_trn: Predictors training set.\n",
    "    - y_trn: Dependent variable training set.\n",
    "    - X_vld: Predictors validation set.\n",
    "    - y_vld:Dependent variable validation set.\n",
    "    - max_iter: ...\n",
    "    - tol: ...\n",
    "\n",
    "    Output: Best parameters.\n",
    "    \"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    no_improvement_count = 0\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "        else:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_param = param\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= max_iter:\n",
    "                    break\n",
    "            if abs(ros - best_ros) < tol:\n",
    "                break\n",
    "    return best_param\n",
    "\n",
    "\n",
    "def Sharpe_gain(Sharpe_val, Roo2_val):\n",
    "    \"\"\"\n",
    "Here: what this function does.\n",
    "Input: \n",
    "    - Sharpe_val: \n",
    "    - Roo2_Val:\n",
    "\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    SR_star = np.sqrt(((Sharpe_val**2)+Roo2_val)/(1-Roo2_val))\n",
    "    return SR_star - Sharpe_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fun_NN(model, params: dict, X_trn, y_trn, X_vld, y_vld, illustration=True):\n",
    "    \"\"\"\n",
    "Validates a Neural Network to return the best mode.\n",
    "Input: \n",
    "    - model: The model we are validating.\n",
    "    - params: A dictionary of parameters.\n",
    "    - X_trn: Predictors training set.\n",
    "    - y_trn: Dependent variable training set.\n",
    "    - X_vld: Predictors validation set.\n",
    "    - y_vld: Dependent variable validation set.\n",
    "\n",
    "    Output: The best Neural Network model.\n",
    "    \"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros is None:\n",
    "            mod = model(n_layers=param['n_layers'], loss=param['loss'], l1=param['l1'], \n",
    "                            learning_rate=param['learning_rate'], batch_size=param['batch_size'], \n",
    "                            epochs=param['epochs'], random_state=param['random_state'], \n",
    "                            batch_norm=param['batch_norm'], patience=param['patience'], \n",
    "                            verbose=param['verbose'], monitor=param['monitor'])\n",
    "            mod.fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            best_mod = mod\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {best_ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "        else:\n",
    "            mod = model(n_layers=param['n_layers'], loss=param['loss'], l1=param['l1'], \n",
    "                            learning_rate=param['learning_rate'], batch_size=param['batch_size'], \n",
    "                            epochs=param['epochs'], random_state=param['random_state'], \n",
    "                            batch_norm=param['batch_norm'], patience=param['patience'], \n",
    "                            verbose=param['verbose'], monitor=param['monitor'])\n",
    "            mod.fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_mod = mod\n",
    "                best_param = param\n",
    "    if illustration:\n",
    "        print('\\n'+'#'*60)\n",
    "        print('Tuning process finished!!!')\n",
    "        print(f'The best setting is: {best_param}')\n",
    "        print(f'with R2oos {best_ros*100:.2f}% on validation set.')\n",
    "        print('#'*60)\n",
    "    return best_mod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - OLS(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics of OLS-3\n",
    "OLS-3 includes size = 'mvel1', Book-to-Market = 'bm', momentum = 'mom1m','mom6m','mom12m','mom36m'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictors needed for the OLS-3 in a seperate file\n",
    "X_3pred = X[['mvel1', 'bm', 'mom1m', 'mom6m', 'mom12m', 'mom36m']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with expanding window and with and without Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_regression_OLS(Dependent, Predictors, stock_weights, loss = 'OLS', initial_train_years = 18, validation_years = 12, test_years = 1):\n",
    "    \"\"\"\n",
    "Function that runs OLS with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio.\n",
    "    - loss: Specify if you want to use 'OLS' loss, or 'Huber' loss\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store values.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list =[]\n",
    "    # List of 1957-2016\n",
    "    years = Dependent.index.year.unique()\n",
    "\n",
    "    # Loop for expanding window.\n",
    "    for i in range(len(years) - initial_train_years - validation_years):\n",
    "        start_year = years[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "\n",
    "        # Specifies which loss function to use.    \n",
    "        if loss == 'OLS':\n",
    "            model = LinearRegression()\n",
    "        elif loss == 'Huber':\n",
    "            model = HuberRegressor(epsilon = 99.9) # Set the epsilon to 99.9%.\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function. Use OLS or Huber.\")\n",
    "        \n",
    "        # Training the model\n",
    "        OLS3 = model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict returns at stock level\n",
    "        r_stock_pred = OLS3.predict(X_test).reshape(-1)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "        \n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.001969218131502082"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUT-OF-SAMPLE R^2 for OLS-3\n",
    "OLS_3pred_Roos=expanding_regression_OLS(y,X_3pred,stock_weights=weights)\n",
    "OLS_3pred_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004975755819653926"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUT-OF-SAMPLE R^2 for OLS-3 with Huber Loss function\n",
    "OLS_3pred_Roos_H = expanding_regression_OLS(y, X_3pred,stock_weights=weights, loss = 'Huber')\n",
    "OLS_3pred_Roos_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dimension Reduction: PCR and PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcr(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs PCR with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # Lists to save outcomes.\n",
    "    component_counts = []\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "\n",
    "    yrs = Dependent.index.year.unique() # List with all the years. (1957-2016)\n",
    "    n_components_list = range(1, 7) # To determine the number of components that are tested.\n",
    "    best_components = None # Initialize and later save the best amount of components.\n",
    "    best_r2 = -np.inf  # Initialize with negative infinity to find the maximum R-squared\n",
    "\n",
    "    # Expanding window.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # Testing the different components in PCA.\n",
    "        for n_component in n_components_list:\n",
    "\n",
    "            pca = PCA(n_components=n_component)\n",
    "            X_train_pca = pca.fit_transform(X_train) \n",
    "            X_val_pca = pca.transform(X_val)\n",
    "\n",
    "            # Fit Linear Regression on the training set\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_pca, y_train)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_val_pred = model.predict(X_val_pca)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "            # Update best components if the current number of components yields a higher R-squared\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_components = n_component\n",
    "\n",
    "        # Save the best number of components to the list\n",
    "        component_counts.append(best_components)\n",
    "\n",
    "        # Use the best number of components to fit the final model on the combined training and validation sets\n",
    "        best_pca = PCA(n_components=best_components)\n",
    "        X_train_pca = best_pca.fit_transform(X_train)\n",
    "        X_test_pca = best_pca.transform(X_test)\n",
    "        \n",
    "        # Best Model\n",
    "        PCASP500 = LinearRegression()\n",
    "        PCASP500.fit(X_train_pca, y_train)\n",
    "\n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = PCASP500.predict(X_test_pca)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Out Of Sample R_squared\n",
    "PCR_Roo2 = pcr(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "PCR_Roo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_pls(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs PLS with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\" \n",
    "    # # Initalize to store r-squared for portfolio..\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list =[]\n",
    "    component_counts = [] # Initialize to save number of components.\n",
    "    years = Predictors.index.year.unique()#List of years (1957-2016)\n",
    "    n_components_list = range(1, 7) # To determine the number of components that are tested.\n",
    "    best_components = None # Initialize and later save the best amount of components.\n",
    "    best_r2 = -np.inf  # Initialize with negative infinity to find the maximum R-squared\n",
    "\n",
    "    for i in range(len(years) - initial_train_years - validation_years): \n",
    "        start_year = years[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # Testing the different components in PLS.\n",
    "        for n_component in n_components_list:\n",
    "\n",
    "            # Train the model once on the training set\n",
    "            pls = PLSRegression(n_components=n_component)\n",
    "            pls.fit(X_train, y_train) \n",
    "\n",
    "           # Predict on the validation set\n",
    "            y_val_pred = pls.predict(X_val)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "            # Update best components if the current number of components yields a higher R-squared\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_components = n_component\n",
    "\n",
    "        # Save the best number of components to the list\n",
    "        component_counts.append(best_components)\n",
    "\n",
    "        # Use the best number of components to fit the final model \n",
    "        best_pls = PLSRegression(n_components=best_components)\n",
    "        best_pls.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the final model on the test set\n",
    "        r_stock_pred = best_pls.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs PLS and saves the results\n",
    "r_squared_scores_pls = walk_forward_pls(y, X, stock_weights=weights ,initial_train_years=18, validation_years=12, test_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05519827043915937\n"
     ]
    }
   ],
   "source": [
    "print(r_squared_scores_pls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Elastic Net & Lasso & Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net -> l1_ratio=0.5\n",
    "Ridge -> l1_ratio=0\n",
    "Lasso -> l1_ratio=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENet(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Elastic Net with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0.5], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        ENet_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = ENet_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENet = ENet(Dependent=y,Predictors=X,stock_weights=weights) \n",
    "ENet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lasso(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Lasso Regression with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [1], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        LAS_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = LAS_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso results\n",
    "Lasso_score = Lasso(Dependent=y,Predictors=X,stock_weights=weights) \n",
    "Lasso_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        RID_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = RID_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Ridge regression\n",
    "Ridge_score = Ridge(Dependent=y,Predictors=X,stock_weights=weights) \n",
    "Ridge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Huber-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber-Loss-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_val, y_pred, delta):\n",
    "    \"\"\"\n",
    "Function that ...\n",
    "Input: \n",
    "    - y_val: ...\n",
    "    - y_pred: ...\n",
    "    - delta: ...\n",
    "\n",
    "    Output: ...\n",
    "\"\"\"\n",
    "    error = y_val - y_pred\n",
    "    is_small_error = np.abs(error) <= delta\n",
    "    squared_loss = 0.5 * (error ** 2)\n",
    "    linear_loss = delta * (np.abs(error) - 0.5 * delta)\n",
    "    return np.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fun_with_huber(model, params: dict, X_trn, y_trn, X_vld, y_vld):\n",
    "    \"\"\"\n",
    "Function that ...\n",
    "Input: \n",
    "    - model: ...\n",
    "    - params: ...\n",
    "    - X_trn: ...\n",
    "    - y_trn: ...\n",
    "    - X_vld ...\n",
    "    - y_vld ...\n",
    "\n",
    "    Output: ...\n",
    "\"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            smallest_loss = huber_loss(y_vld, y_pred, delta=99.9)\n",
    "            best_param = param\n",
    "        else:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            loss = huber_loss(y_vld, y_pred, delta=99.9)\n",
    "            if loss < smallest_loss:\n",
    "                smallest_loss = loss\n",
    "                best_param = param\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-Net with Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENet_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Elastic Net using Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0.5], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        ENetH_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = ENetH_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2450819024981108"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Elastic Net with Huber loss function\n",
    "ENet_huber_scores = ENet_with_huber(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "ENet_huber_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso + H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Lasso_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Lasso Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [1], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        LASH_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = LASH_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24486347729415947"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Lasso Regression with Huber loss function\n",
    "Lasso_with_huber_scores = Lasso_with_huber(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "Lasso_with_huber_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge + H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        RIDH_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = RIDH_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.218e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.218e+02, tolerance: 8.441e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.538e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.538e+02, tolerance: 1.108e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+02, tolerance: 1.230e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.472e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.472e+02, tolerance: 1.895e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.169e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.169e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24203593905499088"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Ridge Regression with Huber loss function\n",
    "Ridge_with_huber_scores = Ridge_with_huber(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "Ridge_with_huber_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - GLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLM(y, X,stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = X.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    Roos_list = []\n",
    "    tuning_par = {\n",
    "    #'knots': [3],\n",
    "    'group_reg':[1e-4,1e-1],\n",
    "    'l1_reg': [1e-4,0],\n",
    "    'groups': [],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "\n",
    "    #GETTING THE SPLINES\n",
    "    spline_data = pd.DataFrame(np.ones((X.shape[0],1)),index=X.index,columns=['const'])\n",
    "    for i in X.columns:\n",
    "        i_dat = X.loc[:,i]\n",
    "        i_sqr = i_dat**2\n",
    "        i_cut, bins = pd.cut(i_dat, 3, right=True, ordered=True, retbins=True)\n",
    "        i_dum = pd.get_dummies(i_cut)\n",
    "        for j in np.arange(3):\n",
    "            i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
    "        i_dum.columns = [f\"{i}_{k}\" for k in np.arange(1,3+1)]\n",
    "        spline_data = pd.concat((spline_data,i_dat,i_dum),axis=1)\n",
    "\n",
    "\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = spline_data[(X.index.year < end_train_year)]\n",
    "        X_test = spline_data[(X.index.year >= end_validation_year) & (X.index.year < end_test_year)]\n",
    "        X_val = spline_data[(X.index.year >= end_train_year) & (X.index.year < end_validation_year)]\n",
    "        y_train = y[(y.index.year < end_train_year)].values.ravel()\n",
    "        y_test = y[(y.index.year >= end_validation_year) & (y.index.year < end_test_year)].values.ravel()\n",
    "        y_val = y[(y.index.year >= end_train_year) & (y.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        groups = [0]+flatten([list(np.repeat(i,3+1))[:] for i in np.arange(1,X.shape[1]+1)])\n",
    "        tuning_par['groups'] = groups\n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(GroupLasso, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        GL=GroupLasso(groups=best_par['groups'], group_reg=best_par['group_reg'], l1_reg=best_par['l1_reg'], fit_intercept=False, random_state=best_par['random_state'],supress_warning=True).fit(X_train,y_train)\n",
    "        #GL=GroupLasso(groups=best_par.groups,group_reg=best_par.lmd,l1_reg=best_par.l1_reg,fit_intercept=False,random_state=best_par.random_state)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = GL.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.315774\n",
      "1957-01-01    0.344997\n",
      "1957-01-01    0.328586\n",
      "1957-01-01    0.361710\n",
      "1957-01-01    0.328800\n",
      "                ...   \n",
      "2016-12-01    0.329053\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.318052\n",
      "2016-12-01    0.350582\n",
      "2016-12-01    0.363564\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "                ...   \n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.002201\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "             ... \n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "                ...   \n",
      "2016-12-01    0.000047\n",
      "2016-12-01    0.008226\n",
      "2016-12-01    0.050468\n",
      "2016-12-01    0.041507\n",
      "2016-12-01    0.000449\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "                ...   \n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "             ... \n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2548427126697421"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results GLM\n",
    "GLM_Roo2 = GLM(y=y, X=X, stock_weights=weights)   \n",
    "GLM_Roo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM Roos^2 for a small subsample of predictons: 0.2548427126697421\n"
     ]
    }
   ],
   "source": [
    "print('GLM Roos^2 for a small subsample of predictons:',GLM_Roo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mom1m</th>\n",
       "      <th>dy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.440062</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.414635</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.428776</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.400577</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.428589</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.428368</td>\n",
       "      <td>-0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.286416</td>\n",
       "      <td>-0.911305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.438039</td>\n",
       "      <td>-0.777349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.409901</td>\n",
       "      <td>-0.798266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.399037</td>\n",
       "      <td>-0.980820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359357 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               mom1m        dy\n",
       "Date                          \n",
       "1957-01-01 -0.440062  0.000000\n",
       "1957-01-01 -0.414635  0.000000\n",
       "1957-01-01 -0.428776  0.000000\n",
       "1957-01-01 -0.400577  0.000000\n",
       "1957-01-01 -0.428589  0.000000\n",
       "...              ...       ...\n",
       "2016-12-01 -0.428368 -0.995178\n",
       "2016-12-01 -0.286416 -0.911305\n",
       "2016-12-01 -0.438039 -0.777349\n",
       "2016-12-01 -0.409901 -0.798266\n",
       "2016-12-01 -0.399037 -0.980820\n",
       "\n",
       "[359357 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred = ['mom1m', 'dy']\n",
    "X_red_rf = X[rf_pred]\n",
    "X_red_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_F(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Random Forest\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3,6],\n",
    "    'max_features': [30,50,100],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(RF, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        RF_SP500 = RF(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], max_features=best_par['max_features'], \n",
    "               random_state=best_par['random_state']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = RF_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Random Forest\n",
    "RF_Roo2 = Random_F(Dependent=y,Predictors=X, stock_weights=weights)   \n",
    "RF_Roo2\n",
    "#(Running time with 2 predictors: 14 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Gradient Boosted Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBRT(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Gradient Boosted Regression Tree\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Predictors.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': range(1, 150),\n",
    "    'max_depth': range(1,2),\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(GradientBoostingRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        GBRT_SP500 = GradientBoostingRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], learning_rate=best_par['learning_rate']).fit(X_train, y_train)\n",
    "\n",
    "        r_stock_pred = GBRT_SP500.predict(X_test)\n",
    "   \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function and print resulting R2 value\n",
    "GBRT_R2 = GBRT(Dependent=y,Predictors=X, stock_weights=weights)   \n",
    "print(GBRT_R2)\n",
    "print(np.mean(GBRT_R2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs XGBoost\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [500,600,800,1000],\n",
    "    'max_depth': [1,2],\n",
    "    'random_state': [12308],\n",
    "    #'learning_rate': [.01]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(XGBRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        XGB = XGBRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = XGB.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results XGBoost\n",
    "XGB_Roo2 = XGBoost(Dependent=y,stock_weights=weights,Predictors=X)   \n",
    "XGB_Roo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(XGB_Roo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: BART Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP.bart import BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BARTrees(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Bayesian Addetive Regression Tree\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'num_trees': [100,200,300],\n",
    "    'burnin': [50,150,200],\n",
    "    'max_stages': [500,1000,2000]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(BART, params=tuning_par, X_trn=np.asarray(X_train), y_trn=y_train, X_vld=np.asarray(X_val), y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        BART_SP500 = BART(num_trees=best_par['num_trees'], burnin=best_par['burnin'], max_stages=best_par['max_stages']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = BART_SP500.predict(np.asarray(X_test))\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results BART\n",
    "BART_Roo2 = BARTrees(Dependent=y,Predictors=X, stock_weights=weights)   \n",
    "BART_Roo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(BART_Roo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that adds Bagging to the Random Forest\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3,6],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(RF, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        BAG_SP500 = RF(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], max_features=X_train.shape[1], \n",
    "               random_state=best_par['random_state']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = BAG_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Bagging\n",
    "Bagg_Roo2 = Bagging(Dependent=y,Predictors=X,stock_weights=weights)\n",
    "Bagg_Roo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_function(Dependent, Predictors, stock_weights, num_layers, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that adds Bagging to the Random Forest\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - num_layers: The number of layers in the Neural Network\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "         \n",
    "        tuning_par = {\n",
    "        'n_layers': [num_layers],\n",
    "        'loss': ['mse'],\n",
    "        'l1': [1e-5, 1e-3],\n",
    "        'learning_rate': [.001, .01],\n",
    "        'batch_size': [10000],\n",
    "        'epochs': [100],\n",
    "        'batch_norm': [True],\n",
    "        'random_state': [1],\n",
    "        'patience': [5],\n",
    "        'verbose': [0],\n",
    "        'monitor': ['val_loss']}\n",
    "        # NN class\n",
    "        class NN(nn.Module):\n",
    "            def __init__(\n",
    "                self, n_layers=1, loss='mse', l1=1e-5, l2=0, learning_rate=.01, batch_norm=True, patience=5,\n",
    "                epochs=100, batch_size=10000, verbose=1, random_state=1, monitor='val_loss', base_neurons=5\n",
    "            ):\n",
    "                super(NN, self).__init__()\n",
    "                self.n_layers = n_layers\n",
    "                self.l1 = l1\n",
    "                self.l2 = l2\n",
    "                self.learning_rate = learning_rate\n",
    "                self.batch_norm = batch_norm\n",
    "                self.patience = patience\n",
    "                self.epochs = epochs\n",
    "                self.batch_size = batch_size\n",
    "                self.verbose = verbose\n",
    "                self.monitor = monitor\n",
    "                self.base_neurons = base_neurons\n",
    "                self.random_state = random_state\n",
    "\n",
    "                # Initialize model layers\n",
    "                self.layers = nn.ModuleList()\n",
    "                input_size, output_size = None, 1\n",
    "\n",
    "                for i in range(self.n_layers, 0, -1):\n",
    "                    if self.n_layers > self.base_neurons:\n",
    "                        in_features = input_size if input_size is not None else X_train.shape[1]\n",
    "                        out_features = 2 ** i\n",
    "                        self.layers.append(nn.Linear(in_features, out_features))\n",
    "                        self.layers.append(nn.ReLU())\n",
    "                    else:\n",
    "                        in_features = input_size if input_size is not None else X_train.shape[1]\n",
    "                        out_features = 2 ** (self.base_neurons - (self.n_layers - i + 1))\n",
    "                        self.layers.append(nn.Linear(in_features, out_features))\n",
    "                        self.layers.append(nn.ReLU())\n",
    "                    input_size = out_features\n",
    "                    if self.batch_norm:\n",
    "                        self.layers.append(nn.BatchNorm1d(out_features))\n",
    "\n",
    "                self.layers.append(nn.Linear(input_size, output_size))\n",
    "\n",
    "                # Loss function\n",
    "                self.criterion = nn.MSELoss()\n",
    "\n",
    "                # Optimizer\n",
    "                self.optimizer = Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.l1 + self.l2)\n",
    "\n",
    "            def forward(self, x):\n",
    "                for layer in self.layers:\n",
    "                    x = layer(x)\n",
    "                return x\n",
    "\n",
    "            def fit(self, X_train, y_train, X_val, y_val):\n",
    "                torch.manual_seed(self.random_state)\n",
    "                np.random.seed(self.random_state)\n",
    "                random.seed(self.random_state)\n",
    "\n",
    "                X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "                y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "                X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "                early_stop_counter = 0\n",
    "                best_loss = float('inf')\n",
    "\n",
    "                for epoch in range(self.epochs):\n",
    "                    self.train()\n",
    "                    for inputs, targets in train_loader:\n",
    "                        self.optimizer.zero_grad()\n",
    "                        outputs = self(inputs)\n",
    "                        loss = self.criterion(outputs, targets)\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    # Validation loss\n",
    "                    self.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self(X_val_tensor)\n",
    "                        val_loss = self.criterion(outputs, y_val_tensor)\n",
    "\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "\n",
    "                    if early_stop_counter >= self.patience:\n",
    "                        print(\"Early stopping.\")\n",
    "                        break\n",
    "\n",
    "                    if self.verbose and epoch % self.verbose == 0:\n",
    "                        print(f\"Epoch {epoch + 1}/{self.epochs}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "                return self\n",
    "\n",
    "            def predict(self, X):\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "                    return self(X_tensor).numpy()\n",
    "\n",
    "        #Setting best NN model\n",
    "        best_NN = val_fun_NN(NN, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "    \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = best_NN.predict(X_test).reshape(-1)\n",
    "\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Portofolio test\n",
    "        dates = weights_test.index\n",
    "        \n",
    "        r_portfolio = pd.DataFrame(index=dates, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate portfolio return actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the result directly in the DataFrame  \n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Appending values to lists\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        #print(r_port_difference_list)\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "        #print(r_port_actual_list)\n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "        \n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "NN_1_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=1, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_1_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "NN_2_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=2, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_2_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "NN_3_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=3, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_3_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "NN_4_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=4, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_4_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "NN_5_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=5, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_5_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
