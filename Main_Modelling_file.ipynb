{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.linear_model import (LinearRegression, \n",
    "                                  HuberRegressor,\n",
    "                                  ElasticNet)\n",
    "from sklearn.metrics import (mean_squared_error, \n",
    "                             r2_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (TimeSeriesSplit, \n",
    "                                     ParameterGrid)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import (GradientBoostingRegressor,\n",
    "                              RandomForestRegressor as RF)\n",
    "#from group_lasso import GroupLasso\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and changing it to be usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read datafiles into dataframes.\n",
    "# y = the excess returns. X has the predictors.\n",
    "y = pd.read_csv('Dependent_y_excess.csv', header=0, index_col=0)\n",
    "X = pd.read_csv('Features_lagged_X.csv', header=0, index_col=0)\n",
    "\n",
    "\n",
    "y.fillna(0, inplace=True) # Do we need this? Isn't it already done in Pre-Processing?\n",
    "\n",
    "# Converting data to Dates as index\n",
    "y.index = pd.to_datetime(y.index, format=\"%Y-%m\").to_period('M')\n",
    "X.index = pd.to_datetime(X.index, format=\"%Y-%m\").to_period('M')\n",
    "\n",
    "# Creating the weights of the stocks compared to the portfolio\n",
    "weights = pd.read_csv('Stocks_weights.csv', header=0)\n",
    "weights.index = weights['Date']\n",
    "weights = weights.drop('Date', axis=1)\n",
    "weights.index = pd.to_datetime(weights.index, format=\"%Y-%m\").to_period('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def R_oos(num, den):\n",
    "    \"\"\"\n",
    "Calculates the Out Of Sample R-squared\n",
    "Input: \n",
    "    - num: Numerator\n",
    "    - den: Denomenator\n",
    "\n",
    "    Output: Out of sample R-squared\n",
    "    \"\"\"\n",
    "    R_oos_val = 1 - (np.sum(num)/np.sum(den))\n",
    "    return R_oos_val\n",
    "\n",
    "\n",
    "\n",
    "def val_fun(model, params: dict, X_trn, y_trn, X_vld, y_vld, max_iter=10, tol=1e-4):\n",
    "    \"\"\"\n",
    "Validates a model to get the best parameters\n",
    "Input: \n",
    "    - model: The model we are validating.\n",
    "    - params: A dictionary of parameters.\n",
    "    - X_trn: Predictors training set.\n",
    "    - y_trn: Dependent variable training set.\n",
    "    - X_vld: Predictors validation set.\n",
    "    - y_vld:Dependent variable validation set.\n",
    "    - max_iter: ...\n",
    "    - tol: ...\n",
    "\n",
    "    Output: Best parameters.\n",
    "    \"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    no_improvement_count = 0\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "        else:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_param = param\n",
    "                no_improvement_count = 0\n",
    "            else:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= max_iter:\n",
    "                    break\n",
    "            if abs(ros - best_ros) < tol:\n",
    "                break\n",
    "    return best_param\n",
    "\n",
    "\n",
    "def Sharpe_gain(Sharpe_val, Roo2_val):\n",
    "    \"\"\"\n",
    "Here: what this function does.\n",
    "Input: \n",
    "    - Sharpe_val: \n",
    "    - Roo2_Val:\n",
    "\n",
    "    Output: \n",
    "    \"\"\"\n",
    "    SR_star = np.sqrt(((Sharpe_val**2)+Roo2_val)/(1-Roo2_val))\n",
    "    return SR_star - Sharpe_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fun_NN(model, params: dict, X_trn, y_trn, X_vld, y_vld, illustration=True):\n",
    "    \"\"\"\n",
    "Validates a Neural Network to return the best mode.\n",
    "Input: \n",
    "    - model: The model we are validating.\n",
    "    - params: A dictionary of parameters.\n",
    "    - X_trn: Predictors training set.\n",
    "    - y_trn: Dependent variable training set.\n",
    "    - X_vld: Predictors validation set.\n",
    "    - y_vld: Dependent variable validation set.\n",
    "\n",
    "    Output: The best Neural Network model.\n",
    "    \"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros is None:\n",
    "            mod = model(n_layers=param['n_layers'], loss=param['loss'], l1=param['l1'], \n",
    "                            learning_rate=param['learning_rate'], batch_size=param['batch_size'], \n",
    "                            epochs=param['epochs'], random_state=param['random_state'], \n",
    "                            batch_norm=param['batch_norm'], patience=param['patience'], \n",
    "                            verbose=param['verbose'], monitor=param['monitor'])\n",
    "            mod.fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            best_mod = mod\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            best_ros = R_oos(y_vld, y_pred)\n",
    "            best_param = param\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {best_ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "        else:\n",
    "            mod = model(n_layers=param['n_layers'], loss=param['loss'], l1=param['l1'], \n",
    "                            learning_rate=param['learning_rate'], batch_size=param['batch_size'], \n",
    "                            epochs=param['epochs'], random_state=param['random_state'], \n",
    "                            batch_norm=param['batch_norm'], patience=param['patience'], \n",
    "                            verbose=param['verbose'], monitor=param['monitor'])\n",
    "            mod.fit(X_trn, y_trn, X_vld, y_vld)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            ros = R_oos(y_vld, y_pred)\n",
    "            if illustration:\n",
    "                print(f'Model with params: {param} finished.')\n",
    "                print(f'with out-of-sample R squared on validation set: {ros*100:.5f}%')\n",
    "                print('*'*60)\n",
    "            if ros > best_ros:\n",
    "                best_ros = ros\n",
    "                best_mod = mod\n",
    "                best_param = param\n",
    "    if illustration:\n",
    "        print('\\n'+'#'*60)\n",
    "        print('Tuning process finished!!!')\n",
    "        print(f'The best setting is: {best_param}')\n",
    "        print(f'with R2oos {best_ros*100:.2f}% on validation set.')\n",
    "        print('#'*60)\n",
    "    return best_mod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - OLS(-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characteristics of OLS-3\n",
    "OLS-3 includes size = 'mvel1', Book-to-Market = 'bm', momentum = 'mom1m','mom6m','mom12m','mom36m'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictors needed for the OLS-3 in a seperate file\n",
    "X_3pred = X[['mvel1', 'bm', 'mom1m', 'mom6m', 'mom12m', 'mom36m']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with expanding window and with and without Huber loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expanding_regression_OLS(Dependent, Predictors, stock_weights, loss = 'OLS', initial_train_years = 18, validation_years = 12, test_years = 1):\n",
    "    \"\"\"\n",
    "Function that runs OLS with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio.\n",
    "    - loss: Specify if you want to use 'OLS' loss, or 'Huber' loss\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store values.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list =[]\n",
    "    # List of 1957-2016\n",
    "    years = Dependent.index.year.unique()\n",
    "\n",
    "    # Loop for expanding window.\n",
    "    for i in range(len(years) - initial_train_years - validation_years):\n",
    "        start_year = years[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "\n",
    "        # Specifies which loss function to use.    \n",
    "        if loss == 'OLS':\n",
    "            model = LinearRegression()\n",
    "        elif loss == 'Huber':\n",
    "            model = HuberRegressor(epsilon = 99.9) # Set the epsilon to 99.9%.\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function. Use OLS or Huber.\")\n",
    "        \n",
    "        # Training the model\n",
    "        OLS3 = model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict returns at stock level\n",
    "        r_stock_pred = OLS3.predict(X_test).reshape(-1)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "        \n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [107915, 106915]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#OUT-OF-SAMPLE R^2 for OLS-3\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m OLS_3pred_Roos\u001b[38;5;241m=\u001b[39m\u001b[43mexpanding_regression_OLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_3pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstock_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m OLS_3pred_Roos\n",
      "Cell \u001b[1;32mIn[7], line 43\u001b[0m, in \u001b[0;36mexpanding_regression_OLS\u001b[1;34m(Dependent, Predictors, stock_weights, loss, initial_train_years, validation_years, test_years)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid loss function. Use OLS or Huber.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m OLS3 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Predict returns at stock level\u001b[39;00m\n\u001b[0;32m     46\u001b[0m r_stock_pred \u001b[38;5;241m=\u001b[39m OLS3\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:578\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    574\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    576\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 578\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    582\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\sklearn\\utils\\validation.py:1210\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1192\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1193\u001b[0m     X,\n\u001b[0;32m   1194\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1206\u001b[0m )\n\u001b[0;32m   1208\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1210\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\sklearn\\utils\\validation.py:430\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    428\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    433\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [107915, 106915]"
     ]
    }
   ],
   "source": [
    "#OUT-OF-SAMPLE R^2 for OLS-3\n",
    "OLS_3pred_Roos=expanding_regression_OLS(y,X_3pred,stock_weights=weights)\n",
    "OLS_3pred_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6748481988792236"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OUT-OF-SAMPLE R^2 for OLS-3 with Huber Loss function\n",
    "OLS_3pred_Roos_H = expanding_regression_OLS(y, X_3pred,stock_weights=weights, loss = 'Huber')\n",
    "OLS_3pred_Roos_H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Dimension Reduction: PCR and PLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - PCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcr(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs PCR with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # Lists to save outcomes.\n",
    "    component_counts = []\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "\n",
    "    yrs = Dependent.index.year.unique() # List with all the years. (1957-2016)\n",
    "    n_components_list = range(1, 7) # To determine the number of components that are tested.\n",
    "    best_components = None # Initialize and later save the best amount of components.\n",
    "    best_r2 = -np.inf  # Initialize with negative infinity to find the maximum R-squared\n",
    "\n",
    "    # Expanding window.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # Testing the different components in PCA.\n",
    "        for n_component in n_components_list:\n",
    "\n",
    "            pca = PCA(n_components=n_component)\n",
    "            X_train_pca = pca.fit_transform(X_train) \n",
    "            X_val_pca = pca.transform(X_val)\n",
    "\n",
    "            # Fit Linear Regression on the training set\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train_pca, y_train)\n",
    "\n",
    "            # Predict on the validation set\n",
    "            y_val_pred = model.predict(X_val_pca)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "            # Update best components if the current number of components yields a higher R-squared\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_components = n_component\n",
    "\n",
    "        # Save the best number of components to the list\n",
    "        component_counts.append(best_components)\n",
    "\n",
    "        # Use the best number of components to fit the final model on the combined training and validation sets\n",
    "        best_pca = PCA(n_components=best_components)\n",
    "        X_train_pca = best_pca.fit_transform(X_train)\n",
    "        X_test_pca = best_pca.transform(X_test)\n",
    "        \n",
    "        # Best Model\n",
    "        PCASP500 = LinearRegression()\n",
    "        PCASP500.fit(X_train_pca, y_train)\n",
    "\n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = PCASP500.predict(X_test_pca)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18599826420768895"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Out Of Sample R_squared\n",
    "PCR_Roo2 = pcr(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "PCR_Roo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_pls(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs PLS with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\" \n",
    "    # # Initalize to store r-squared for portfolio..\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list =[]\n",
    "    component_counts = [] # Initialize to save number of components.\n",
    "    years = Predictors.index.year.unique()#List of years (1957-2016)\n",
    "    n_components_list = range(1, 7) # To determine the number of components that are tested.\n",
    "    best_components = None # Initialize and later save the best amount of components.\n",
    "    best_r2 = -np.inf  # Initialize with negative infinity to find the maximum R-squared\n",
    "\n",
    "    for i in range(len(years) - initial_train_years - validation_years): \n",
    "        start_year = years[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # Testing the different components in PLS.\n",
    "        for n_component in n_components_list:\n",
    "\n",
    "            # Train the model once on the training set\n",
    "            pls = PLSRegression(n_components=n_component)\n",
    "            pls.fit(X_train, y_train) \n",
    "\n",
    "           # Predict on the validation set\n",
    "            y_val_pred = pls.predict(X_val)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "            # Update best components if the current number of components yields a higher R-squared\n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_components = n_component\n",
    "\n",
    "        # Save the best number of components to the list\n",
    "        component_counts.append(best_components)\n",
    "\n",
    "        # Use the best number of components to fit the final model \n",
    "        best_pls = PLSRegression(n_components=best_components)\n",
    "        best_pls.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the final model on the test set\n",
    "        r_stock_pred = best_pls.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs PLS and saves the results\n",
    "r_squared_scores_pls = walk_forward_pls(y, X, stock_weights=weights ,initial_train_years=18, validation_years=12, test_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05519827043915937\n"
     ]
    }
   ],
   "source": [
    "print(r_squared_scores_pls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Elastic Net & Lasso & Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net -> l1_ratio=0.5\n",
    "Ridge -> l1_ratio=0\n",
    "Lasso -> l1_ratio=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENet(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Elastic Net with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "    \"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0.5], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        ENet_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = ENet_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENet = ENet(Dependent=y,Predictors=X,stock_weights=weights) \n",
    "ENet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lasso(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Lasso Regression with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [1], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        LAS_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = LAS_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lasso results\n",
    "Lasso_score = Lasso(Dependent=y,Predictors=X,stock_weights=weights) \n",
    "Lasso_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with expanding window.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        RID_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = RID_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25721887452371794"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Ridge regression\n",
    "Ridge_score = Ridge(Dependent=y,Predictors=X,stock_weights=weights) \n",
    "Ridge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Huber-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber-Loss-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_val, y_pred, delta):\n",
    "    \"\"\"\n",
    "Function that ...\n",
    "Input: \n",
    "    - y_val: ...\n",
    "    - y_pred: ...\n",
    "    - delta: ...\n",
    "\n",
    "    Output: ...\n",
    "\"\"\"\n",
    "    error = y_val - y_pred\n",
    "    is_small_error = np.abs(error) <= delta\n",
    "    squared_loss = 0.5 * (error ** 2)\n",
    "    linear_loss = delta * (np.abs(error) - 0.5 * delta)\n",
    "    return np.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_fun_with_huber(model, params: dict, X_trn, y_trn, X_vld, y_vld):\n",
    "    \"\"\"\n",
    "Function that ...\n",
    "Input: \n",
    "    - model: ...\n",
    "    - params: ...\n",
    "    - X_trn: ...\n",
    "    - y_trn: ...\n",
    "    - X_vld ...\n",
    "    - y_vld ...\n",
    "\n",
    "    Output: ...\n",
    "\"\"\"\n",
    "    best_ros = None\n",
    "    lst_params = list(ParameterGrid(params))\n",
    "    for param in lst_params:\n",
    "        if best_ros == None:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            smallest_loss = huber_loss(y_vld, y_pred, delta=99.9)\n",
    "            best_param = param\n",
    "        else:\n",
    "            mod = model().set_params(**param).fit(X_trn, y_trn)\n",
    "            y_pred = mod.predict(X_vld)\n",
    "            loss = huber_loss(y_vld, y_pred, delta=99.9)\n",
    "            if loss < smallest_loss:\n",
    "                smallest_loss = loss\n",
    "                best_param = param\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-Net with Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ENet_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Elastic Net using Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0.5], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        ENetH_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = ENetH_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2450819024981108"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Elastic Net with Huber loss function\n",
    "ENet_huber_scores = ENet_with_huber(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "ENet_huber_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso + H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Lasso_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Lasso Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [1], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        LASH_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = LASH_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24486347729415947"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Lasso Regression with Huber loss function\n",
    "Lasso_with_huber_scores = Lasso_with_huber(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "Lasso_with_huber_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge + H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ridge_with_huber(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    # All the years(1957-2016)\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    # Initalize to store r-squared for portfolio.\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    # Tested tuning parameters\n",
    "    tuning_par = {\n",
    "        \"alpha\": np.linspace(1e-1, 1e-4, num=10),\n",
    "        \"l1_ratio\": [0], \"tol\":[1e-2]\n",
    "    } \n",
    "    \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years\n",
    "        end_validation_year = end_train_year + validation_years\n",
    "        end_test_year = end_validation_year + test_years\n",
    "\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        best_par = val_fun_with_huber(ElasticNet, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        RIDH_SP500 = ElasticNet(alpha=best_par['alpha'], l1_ratio=best_par['l1_ratio']).fit(X_train, y_train)\n",
    "\n",
    "        # Calculate R_squared        \n",
    "        r_stock_pred = RIDH_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.382e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e+02, tolerance: 6.768e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+02, tolerance: 7.448e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.917e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.916e+02, tolerance: 7.835e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.032e+02, tolerance: 8.067e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.219e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.218e+02, tolerance: 8.441e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.218e+02, tolerance: 8.441e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.413e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.412e+02, tolerance: 8.827e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.757e+02, tolerance: 9.516e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+02, tolerance: 1.004e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.328e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+02, tolerance: 1.066e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.539e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.538e+02, tolerance: 1.108e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.538e+02, tolerance: 1.108e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.742e+02, tolerance: 1.149e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.920e+02, tolerance: 1.184e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.152e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+02, tolerance: 1.230e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.151e+02, tolerance: 1.230e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.571e+02, tolerance: 1.314e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.737e+02, tolerance: 1.348e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.892e+02, tolerance: 1.378e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.155e+02, tolerance: 1.431e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.383e+02, tolerance: 1.477e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.536e+02, tolerance: 1.507e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.699e+02, tolerance: 1.540e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.859e+02, tolerance: 1.572e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.023e+02, tolerance: 1.605e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.210e+02, tolerance: 1.642e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+02, tolerance: 1.693e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.876e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.875e+02, tolerance: 1.775e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.472e+02, tolerance: 1.895e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.472e+02, tolerance: 1.895e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.047e+03, tolerance: 2.095e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.108e+03, tolerance: 2.216e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.149e+03, tolerance: 2.297e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.169e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.169e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.168e+03, tolerance: 2.337e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24203593905499088"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results Ridge Regression with Huber loss function\n",
    "Ridge_with_huber_scores = Ridge_with_huber(Dependent=y, Predictors=X, stock_weights=weights)\n",
    "Ridge_with_huber_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - GLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLM(y, X,stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Ridge Regression with Huber Loss function.\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = X.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    Roos_list = []\n",
    "    tuning_par = {\n",
    "    #'knots': [3],\n",
    "    'group_reg':[1e-4,1e-1],\n",
    "    'l1_reg': [1e-4,0],\n",
    "    'groups': [],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "\n",
    "    #GETTING THE SPLINES\n",
    "    spline_data = pd.DataFrame(np.ones((X.shape[0],1)),index=X.index,columns=['const'])\n",
    "    for i in X.columns:\n",
    "        i_dat = X.loc[:,i]\n",
    "        i_sqr = i_dat**2\n",
    "        i_cut, bins = pd.cut(i_dat, 3, right=True, ordered=True, retbins=True)\n",
    "        i_dum = pd.get_dummies(i_cut)\n",
    "        for j in np.arange(3):\n",
    "            i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
    "        i_dum.columns = [f\"{i}_{k}\" for k in np.arange(1,3+1)]\n",
    "        spline_data = pd.concat((spline_data,i_dat,i_dum),axis=1)\n",
    "\n",
    "\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = spline_data[(X.index.year < end_train_year)]\n",
    "        X_test = spline_data[(X.index.year >= end_validation_year) & (X.index.year < end_test_year)]\n",
    "        X_val = spline_data[(X.index.year >= end_train_year) & (X.index.year < end_validation_year)]\n",
    "        y_train = y[(y.index.year < end_train_year)].values.ravel()\n",
    "        y_test = y[(y.index.year >= end_validation_year) & (y.index.year < end_test_year)].values.ravel()\n",
    "        y_val = y[(y.index.year >= end_train_year) & (y.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        groups = [0]+flatten([list(np.repeat(i,3+1))[:] for i in np.arange(1,X.shape[1]+1)])\n",
    "        tuning_par['groups'] = groups\n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(GroupLasso, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        GL=GroupLasso(groups=best_par['groups'], group_reg=best_par['group_reg'], l1_reg=best_par['l1_reg'], fit_intercept=False, random_state=best_par['random_state'],supress_warning=True).fit(X_train,y_train)\n",
    "        #GL=GroupLasso(groups=best_par.groups,group_reg=best_par.lmd,l1_reg=best_par.l1_reg,fit_intercept=False,random_state=best_par.random_state)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = GL.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.315774\n",
      "1957-01-01    0.344997\n",
      "1957-01-01    0.328586\n",
      "1957-01-01    0.361710\n",
      "1957-01-01    0.328800\n",
      "                ...   \n",
      "2016-12-01    0.329053\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.318052\n",
      "2016-12-01    0.350582\n",
      "2016-12-01    0.363564\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "                ...   \n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.002201\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "             ... \n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "1957-01-01    0.000000\n",
      "                ...   \n",
      "2016-12-01    0.000047\n",
      "2016-12-01    0.008226\n",
      "2016-12-01    0.050468\n",
      "2016-12-01    0.041507\n",
      "2016-12-01    0.000449\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "1957-01-01    0.111111\n",
      "                ...   \n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "2016-12-01    0.000000\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/var/folders/lv/4s79bs8j60n07b1d88ptj5l00000gn/T/ipykernel_52862/3791382187.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'Date\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "1957-01-01    0.0\n",
      "             ... \n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "2016-12-01    0.0\n",
      "Length: 359357, dtype: float64' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  i_dum.iloc[:,j] = i_dum.iloc[:,j]*((i_dat-bins[j])**2)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_group_lasso.py:457: UserWarning: \n",
      "The behaviour has changed since v1.1.1, before then, a bug in the optimisation\n",
      "algorithm made it so the regularisation parameter was scaled by the largest\n",
      "eigenvalue of the covariance matrix.\n",
      "\n",
      "To use the old behaviour, initialise the class with the keyword argument\n",
      "`old_regularisation=True`.\n",
      "\n",
      "To supress this warning, initialise the class with the keyword argument\n",
      "`supress_warning=True`\n",
      "\n",
      "  warnings.warn(_OLD_REG_WARNING)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/group_lasso/_fista.py:114: ConvergenceWarning: The FISTA iterations did not converge to a sufficient minimum.\n",
      "You used subsampling then this is expected, otherwise, try increasing the number of iterations or decreasing the tolerance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2548427126697421"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results GLM\n",
    "GLM_Roo2 = GLM(y=y, X=X, stock_weights=weights)   \n",
    "GLM_Roo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM Roos^2 for a small subsample of predictons: 0.2548427126697421\n"
     ]
    }
   ],
   "source": [
    "print('GLM Roos^2 for a small subsample of predictons:',GLM_Roo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mom1m</th>\n",
       "      <th>dy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.440062</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.414635</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.428776</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.400577</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957-01-01</th>\n",
       "      <td>-0.428589</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.428368</td>\n",
       "      <td>-0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.286416</td>\n",
       "      <td>-0.911305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.438039</td>\n",
       "      <td>-0.777349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.409901</td>\n",
       "      <td>-0.798266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>-0.399037</td>\n",
       "      <td>-0.980820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359357 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               mom1m        dy\n",
       "Date                          \n",
       "1957-01-01 -0.440062  0.000000\n",
       "1957-01-01 -0.414635  0.000000\n",
       "1957-01-01 -0.428776  0.000000\n",
       "1957-01-01 -0.400577  0.000000\n",
       "1957-01-01 -0.428589  0.000000\n",
       "...              ...       ...\n",
       "2016-12-01 -0.428368 -0.995178\n",
       "2016-12-01 -0.286416 -0.911305\n",
       "2016-12-01 -0.438039 -0.777349\n",
       "2016-12-01 -0.409901 -0.798266\n",
       "2016-12-01 -0.399037 -0.980820\n",
       "\n",
       "[359357 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred = ['mom1m', 'dy']\n",
    "X_red_rf = X[rf_pred]\n",
    "X_red_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_F(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Random Forest\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3,6],\n",
    "    'max_features': [30,50,100],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(RF, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        RF_SP500 = RF(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], max_features=best_par['max_features'], \n",
    "               random_state=best_par['random_state']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = RF_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Random Forest\n",
    "RF_Roo2 = Random_F(Dependent=y,Predictors=X, stock_weights=weights)   \n",
    "RF_Roo2\n",
    "#(Running time with 2 predictors: 14 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Gradient Boosted Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBRT(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Gradient Boosted Regression Tree\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Predictors.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': range(1, 150),\n",
    "    'max_depth': range(1,2),\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "\n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(GradientBoostingRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        GBRT_SP500 = GradientBoostingRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], learning_rate=best_par['learning_rate']).fit(X_train, y_train)\n",
    "\n",
    "        r_stock_pred = GBRT_SP500.predict(X_test)\n",
    "   \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function and print resulting R2 value\n",
    "GBRT_R2 = GBRT(Dependent=y,Predictors=X, stock_weights=weights)   \n",
    "print(GBRT_R2)\n",
    "print(np.mean(GBRT_R2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs XGBoost\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [500,600,800,1000],\n",
    "    'max_depth': [1,2],\n",
    "    'random_state': [12308],\n",
    "    #'learning_rate': [.01]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(XGBRegressor, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        XGB = XGBRegressor(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = XGB.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results XGBoost\n",
    "XGB_Roo2 = XGBoost(Dependent=y,stock_weights=weights,Predictors=X)   \n",
    "XGB_Roo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(XGB_Roo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: BART Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install ISLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ISLP.bart import BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BARTrees(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that runs Bayesian Addetive Regression Tree\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'num_trees': [100,200,300],\n",
    "    'burnin': [50,150,200],\n",
    "    'max_stages': [500,1000,2000]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(BART, params=tuning_par, X_trn=np.asarray(X_train), y_trn=y_train, X_vld=np.asarray(X_val), y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        BART_SP500 = BART(num_trees=best_par['num_trees'], burnin=best_par['burnin'], max_stages=best_par['max_stages']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = BART_SP500.predict(np.asarray(X_test))\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results BART\n",
    "BART_Roo2 = BARTrees(Dependent=y,Predictors=X, stock_weights=weights)   \n",
    "BART_Roo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(BART_Roo2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Method: Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bagging(Dependent, Predictors, stock_weights, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that adds Bagging to the Random Forest\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "    tuning_par = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [3,6],\n",
    "    'random_state': [12308]\n",
    "    }\n",
    "    \n",
    "    # Now the model runs for every time of the 30 splits and for every possible combination of the tuning parameters.\n",
    "    # In this case is 30*60 = 1800, but only the best R2 for every split are stored. \n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "        \n",
    "        # This part runs the tuning to find the best combination of the tuning parameters for every split\n",
    "        best_par = val_fun(RF, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "        \n",
    "        # Now we test the model\n",
    "        BAG_SP500 = RF(n_estimators=best_par['n_estimators'], max_depth=best_par['max_depth'], max_features=X_train.shape[1], \n",
    "               random_state=best_par['random_state']).fit(X_train, y_train)\n",
    "        \n",
    "        # Predict returns at the stock level\n",
    "        r_stock_pred = BAG_SP500.predict(X_test)\n",
    "        \n",
    "        # Gets weights from current testing year\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Initialize dataframe to store predicted and actual returns\n",
    "        r_portfolio = pd.DataFrame(index=weights_test.index, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate monthly return predicted and actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the results in a DataFrame\n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Store numerator and denominator to calculate out of sample R-Squared\n",
    "        r_port_difference_list.extend(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        r_port_actual_list.extend(((r_portfolio['return_test'])**2).tolist())\n",
    "    \n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "\n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Bagging\n",
    "Bagg_Roo2 = Bagging(Dependent=y,Predictors=X,stock_weights=weights)\n",
    "Bagg_Roo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out-of-Sample R-squared (Tensor-aware version)\n",
    "def R_oos_torch(num, den):\n",
    "    R_oos_val = 1 - torch.sum(num) / torch.sum(den)\n",
    "    return R_oos_val.item()  # Convert to a Python scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Remove loss and other things not accessed\n",
    "2. Check Monitor vall_loss\n",
    "3. Check verbose\n",
    "4. Check number of neurons in each layer.\n",
    "5. Check loss function L1 or MSE\n",
    "6. Check random_state\n",
    "7. Check order of things. Compare with PHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_function(Dependent, Predictors, stock_weights, num_layers, ensemble = 10, initial_train_years=18, validation_years=12, test_years=1):\n",
    "    \"\"\"\n",
    "Function that adds Bagging to the Random Forest\n",
    "Input: \n",
    "    - Dependent: Dependent variable data\n",
    "    - Predictors: Independent variables data\n",
    "    - stock_weights: The weights of a stock as percentage of portfolio\n",
    "    - num_layers: The number of layers in the Neural Network\n",
    "    - ensemble: Amount of Neural Networks to be trained on same data.\n",
    "    - initial_train_years: Number of initial training years. (Default is 18)\n",
    "    - validation_years: Number of years for the validation set. (Default is 12)\n",
    "    - test_years: Number of years for the test set. (Default is 1)\n",
    "\n",
    "    Output: Out of sample R-squared.\n",
    "\"\"\"\n",
    "    yrs = Dependent.index.year.unique()\n",
    "    r_port_difference_list = []\n",
    "    r_port_actual_list = []\n",
    "\n",
    "    for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "        start_year = yrs[i]\n",
    "        end_train_year = start_year + initial_train_years  # 18 years of training and increasing with 1 year every iteration\n",
    "        end_validation_year = end_train_year + validation_years  # 12 years of validation\n",
    "        end_test_year = end_validation_year + test_years  # 1 year of test\n",
    "\n",
    "        # Creating training, validation and test sets.\n",
    "        X_train = Predictors[(Predictors.index.year < end_train_year)]\n",
    "        X_test = Predictors[(Predictors.index.year >= end_validation_year) & (Predictors.index.year < end_test_year)]\n",
    "        X_val = Predictors[(Predictors.index.year >= end_train_year) & (Predictors.index.year < end_validation_year)]\n",
    "        y_train = Dependent[(Dependent.index.year < end_train_year)].values.ravel()\n",
    "        y_test = Dependent[(Dependent.index.year >= end_validation_year) & (Dependent.index.year < end_test_year)].values.ravel()\n",
    "        y_val = Dependent[(Dependent.index.year >= end_train_year) & (Dependent.index.year < end_validation_year)].values.ravel()\n",
    "         \n",
    "        tuning_par = {\n",
    "        'n_layers': [num_layers],\n",
    "        'loss': ['mse'],\n",
    "        'l1': [1e-5, 1e-3],\n",
    "        'learning_rate': [.001, .01],\n",
    "        'batch_size': [10000],\n",
    "        'epochs': [100],\n",
    "        'batch_norm': [True],\n",
    "        'random_state': [1],\n",
    "        'patience': [5],\n",
    "        'verbose': [0],\n",
    "        'monitor': ['val_loss']}\n",
    "        # NN class\n",
    "        class NN(nn.Module):\n",
    "            def __init__(\n",
    "                self, n_layers=1, loss='mse', l1=1e-5, l2=0, learning_rate=.01, batch_norm=True, patience=5,\n",
    "                epochs=100, batch_size=10000, verbose=1, random_state=1, monitor='val_loss', base_neurons=5\n",
    "            ):\n",
    "                super(NN, self).__init__()\n",
    "                self.n_layers = n_layers\n",
    "                self.l1 = l1\n",
    "                self.l2 = l2\n",
    "                self.learning_rate = learning_rate\n",
    "                self.batch_norm = batch_norm\n",
    "                self.patience = patience\n",
    "                self.epochs = epochs\n",
    "                self.batch_size = batch_size\n",
    "                self.verbose = verbose\n",
    "                self.monitor = monitor\n",
    "                self.base_neurons = base_neurons\n",
    "                self.random_state = random_state\n",
    "\n",
    "                # Initialize model layers\n",
    "                self.layers = nn.ModuleList()\n",
    "                input_size, output_size = None, 1\n",
    "\n",
    "                \n",
    "                for i in range(self.n_layers, 0, -1):\n",
    "                    in_features = input_size if input_size is not None else X_train.shape[1]\n",
    "                    out_features = 2 ** (self.base_neurons - (self.n_layers - i))\n",
    "                    self.layers.append(nn.Linear(in_features, out_features))\n",
    "                    self.layers.append(nn.ReLU())\n",
    "                    input_size = out_features\n",
    "                    if self.batch_norm:\n",
    "                        self.layers.append(nn.BatchNorm1d(out_features))\n",
    "\n",
    "                self.layers.append(nn.Linear(input_size, output_size))\n",
    "\n",
    "                # Loss function\n",
    "                self.criterion = nn.L1Loss()\n",
    "\n",
    "                # Optimizer\n",
    "                self.optimizer = Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.l1 + self.l2)\n",
    "\n",
    "            def forward(self, x):\n",
    "                for layer in self.layers:\n",
    "                    x = layer(x)\n",
    "                return x\n",
    "\n",
    "            def fit(self, X_train, y_train, X_val, y_val):\n",
    "                torch.manual_seed(self.random_state)\n",
    "                np.random.seed(self.random_state)\n",
    "                random.seed(self.random_state)\n",
    "\n",
    "                X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "                y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "                X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "                train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "                train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "                early_stop_counter = 0\n",
    "                best_loss = float('inf')\n",
    "\n",
    "                for epoch in range(self.epochs):\n",
    "                    self.train()\n",
    "                    for inputs, targets in train_loader:\n",
    "                        self.optimizer.zero_grad()\n",
    "                        outputs = self(inputs)\n",
    "                        loss = self.criterion(outputs, targets)\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                    # Validation loss\n",
    "                    self.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self(X_val_tensor)\n",
    "                        val_loss = self.criterion(outputs, y_val_tensor)\n",
    "\n",
    "                    if val_loss < best_loss:\n",
    "                        best_loss = val_loss\n",
    "                        early_stop_counter = 0\n",
    "                    else:\n",
    "                        early_stop_counter += 1\n",
    "\n",
    "                    if early_stop_counter >= self.patience:\n",
    "                        print(\"Early stopping.\")\n",
    "                        break\n",
    "\n",
    "                    if self.verbose and epoch % self.verbose == 0:\n",
    "                        print(f\"Epoch {epoch + 1}/{self.epochs}, Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "                return self\n",
    "\n",
    "            def predict(self, X):\n",
    "                self.eval()\n",
    "                with torch.no_grad():\n",
    "                    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "                    return self(X_tensor).numpy()\n",
    "\n",
    "        # Ensemble\n",
    "        ensemble_predictions = []\n",
    "        for _ in range(ensemble):\n",
    "            # Create and fit a new neural network instance\n",
    "            best_NN = val_fun_NN(NN, params=tuning_par, X_trn=X_train, y_trn=y_train, X_vld=X_val, y_vld=y_val)\n",
    "            # Store prediction for this model\n",
    "            ensemble_predictions.append(best_NN.predict(X_test).reshape(-1))  \n",
    "\n",
    "        # Average predictions from all models in the ensemble\n",
    "        r_stock_pred = np.mean(ensemble_predictions, axis=0)\n",
    "    \n",
    "\n",
    "        weights_test = stock_weights.loc[str(end_validation_year)]\n",
    "        # Portofolio test\n",
    "        dates = weights_test.index\n",
    "        \n",
    "        r_portfolio = pd.DataFrame(index=dates, columns=['return_test', 'return_pred'])\n",
    "        \n",
    "        # Calculate portfolio return actual \n",
    "        for month in range(1, 13):\n",
    "            start_index = (month - 1) * weights_test.shape[0] // 12  \n",
    "            end_index = month * weights_test.shape[0] // 12\n",
    "            month_weights = weights_test.iloc[start_index:end_index]\n",
    "            month_y_test = y_test[start_index:end_index]\n",
    "            month_y_pred = r_stock_pred[start_index:end_index]\n",
    "            # Store the result directly in the DataFrame  \n",
    "            r_portfolio.loc[f'{end_validation_year}-{month:02d}', ['return_test', 'return_pred']] = np.sum(month_weights['weight'] * month_y_test), np.sum(month_weights['weight'] * month_y_pred) \n",
    "\n",
    "        # Appending values to lists\n",
    "        r_port_difference_list.append(((r_portfolio['return_test']-r_portfolio['return_pred'])**2).tolist())\n",
    "        #print(r_port_difference_list)\n",
    "        r_port_actual_list.append(((r_portfolio['return_test'])**2).tolist())\n",
    "        #print(r_port_actual_list)\n",
    "    # Calculate Roos\n",
    "    Model_Roos = R_oos(r_port_difference_list, r_port_actual_list)\n",
    "        \n",
    "    return Model_Roos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Results NN1-Regression-[32(relu)-1(linear)]\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m NN_1_ROO2 \u001b[38;5;241m=\u001b[39m \u001b[43mNN_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDependent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPredictors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_train_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m NN_1_ROO2\n",
      "Cell \u001b[1;32mIn[8], line 150\u001b[0m, in \u001b[0;36mNN_function\u001b[1;34m(Dependent, Predictors, stock_weights, num_layers, ensemble, initial_train_years, validation_years, test_years)\u001b[0m\n\u001b[0;32m    147\u001b[0m ensemble_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ensemble):\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# Create and fit a new neural network instance\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m     best_NN \u001b[38;5;241m=\u001b[39m \u001b[43mval_fun_NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuning_par\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_trn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_vld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_vld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# Store prediction for this model\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     ensemble_predictions\u001b[38;5;241m.\u001b[39mappend(best_NN\u001b[38;5;241m.\u001b[39mpredict(X_test)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36mval_fun_NN\u001b[1;34m(model, params, X_trn, y_trn, X_vld, y_vld, illustration)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_ros \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     mod \u001b[38;5;241m=\u001b[39m model(n_layers\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], loss\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], l1\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     19\u001b[0m                     learning_rate\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     20\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m], random_state\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     21\u001b[0m                     batch_norm\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_norm\u001b[39m\u001b[38;5;124m'\u001b[39m], patience\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     22\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m], monitor\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonitor\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_vld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_vld\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     best_mod \u001b[38;5;241m=\u001b[39m mod\n\u001b[0;32m     25\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mpredict(X_vld)\n",
      "Cell \u001b[1;32mIn[8], line 112\u001b[0m, in \u001b[0;36mNN_function.<locals>.NN.fit\u001b[1;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 112\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Results NN1-Regression-[32(relu)-1(linear)]\n",
    "\n",
    "NN_1_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=1, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_1_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN2-Regression-[32(relu)-16(relu)-1(linear)]\n",
    "NN_2_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=2, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_2_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN3-Regression-[32(relu)-16(relu)-8(relu)-1(linear)]\n",
    "NN_3_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=3, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_3_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results NN4-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-1(linear)]\n",
    "NN_4_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=4, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_4_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NN5-Regression-[32(relu)-16(relu)-8(relu)-4(relu)-2(relu)-1(linear)]\n",
    "NN_5_ROO2 = NN_function(Dependent=y, Predictors=X, stock_weights=weights, num_layers=5, initial_train_years=18, validation_years=12, test_years=1)\n",
    "NN_5_ROO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 95\u001b[0m\n\u001b[0;32m     91\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# Training with early stopping\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reshape to (batch_size, 1) \u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\hidde\\anaconda3\\envs\\ISLP\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Out-of-Sample R-squared (Tensor-aware version)\n",
    "def R_oos_torch(num, den):\n",
    "    R_oos_val = 1 - torch.sum(num) / torch.sum(den)\n",
    "    return R_oos_val.item()  # Convert to a Python scalar\n",
    "\n",
    "# Neural Network Model\n",
    "class NewNN(nn.Module):\n",
    "    def __init__(self, n_layers, n_inputs, n_outputs, hidden_size, l1_penalty):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(n_inputs, hidden_size))\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_size))  \n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_size, n_outputs))\n",
    "        self.l1_penalty = l1_penalty\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def l1_regularization(self):\n",
    "        l1_reg = torch.tensor(0.)\n",
    "        for param in self.parameters():\n",
    "            l1_reg += torch.norm(param, 1)\n",
    "        return self.l1_penalty * l1_reg\n",
    "\n",
    "\n",
    "# Parameter Grid \n",
    "param_grid = {\n",
    "    'n_layers': [1, 2, 3, 4, 5],  \n",
    "    'hidden_size': [32, 16], \n",
    "    'l1': [10**-5, 10**-3],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [10000],\n",
    "    'epochs': [100],\n",
    "    'patience': [5],  \n",
    "    'ensemble_size': [10]\n",
    "}\n",
    "\n",
    "# Expanding Window Setup\n",
    "initial_train_years = 18\n",
    "validation_years = 12\n",
    "test_years = 1\n",
    "yrs = y.index.year.unique()  \n",
    "results = []  \n",
    "\n",
    "# Expanding Window Loop\n",
    "for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "    start_year = yrs[i]\n",
    "    end_train_year = start_year + initial_train_years\n",
    "    end_validation_year = end_train_year + validation_years\n",
    "    end_test_year = end_validation_year + test_years\n",
    "\n",
    "    X_train = X.loc[X.index.year < end_train_year]\n",
    "    X_val = X.loc[(X.index.year >= end_train_year) & (X.index.year < end_validation_year)]\n",
    "    X_test = X.loc[(X.index.year >= end_validation_year) & (X.index.year < end_test_year)]\n",
    "\n",
    "    y_train = y.loc[y.index.year < end_train_year].values.ravel()\n",
    "    y_val = y.loc[(y.index.year >= end_train_year) & (y.index.year < end_validation_year)].values.ravel()\n",
    "    y_test = y.loc[(y.index.year >= end_validation_year) & (y.index.year < end_test_year)].values.ravel()\n",
    "\n",
    "    # Data Preparation for PyTorch \n",
    "    train_dataset = TensorDataset(torch.Tensor(X_train.values), torch.Tensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.Tensor(X_val.values), torch.Tensor(y_val))\n",
    "    test_dataset = TensorDataset(torch.Tensor(X_test.values), torch.Tensor(y_test))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=10000, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=10000, shuffle=False)  \n",
    "\n",
    "    # ... Inside the expanding window loop ...\n",
    "\n",
    "    for param in ParameterGrid(param_grid):\n",
    "        ensemble_predictions = []  # Store predictions from multiple seeds\n",
    "\n",
    "        for _ in range(param['ensemble_size']):\n",
    "            model = NewNN(\n",
    "                n_layers=param['n_layers'],\n",
    "                n_inputs=X_train.shape[1],  # Number of features\n",
    "                n_outputs=1,  # Single output for stock returns\n",
    "                hidden_size=param['hidden_size'],\n",
    "                l1_penalty=param['l1']\n",
    "            )\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "            loss_fn = nn.MSELoss()\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "\n",
    "            for epoch in range(param['epochs']):\n",
    "                # Training with early stopping\n",
    "                for x_batch, y_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch = y_batch.view(-1, 1)  # Reshape to (batch_size, 1) \n",
    "                    y_pred = model(x_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch) + model.l1_regularization()  # L1 added\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Validation\n",
    "                with torch.inference_mode(): \n",
    "                    val_loss = 0.0\n",
    "                    for x_val, y_val in val_loader:\n",
    "                        y_pred_val = model(x_val)\n",
    "                        val_loss += loss_fn(y_pred_val, y_val).item() # No L1 here\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= param['patience']:\n",
    "                    break\n",
    "\n",
    "            # Ensemble prediction on test data\n",
    "            with torch.inference_mode():\n",
    "                test_preds = model(torch.Tensor(X_test.values))\n",
    "                ensemble_predictions.append(test_preds.numpy())\n",
    "\n",
    "        ensemble_preds = np.mean(ensemble_predictions, axis=0)  # Average predictions\n",
    "        r_oos = R_oos.torch(np.power(y_test - ensemble_preds.ravel(), 2), np.power(y_test - y_test.mean(), 2))\n",
    "\n",
    "        results.append({'params': param, 'R_oos': r_oos}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import r2_score  # For R-squared\n",
    "\n",
    "# Reproducibility (if desired)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# ***Step 1: Load your data from 'X' and 'y' DataFrames here***\n",
    "\n",
    "# ... \n",
    "\n",
    "# ***Step 2: Define your Neural Network architecture***\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_layers, input_dim, hidden_dims, l1_penalty, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            layers.append(nn.Linear(input_dim if i == 0 else hidden_dims[i - 1], hidden_dims[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_dims[i]))  # Batch normalization\n",
    "            layers.append(nn.Dropout(dropout))  # Dropout\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dims[-1], 1))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        # L1 Regularization\n",
    "        self.l1_penalty = l1_penalty\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    def l1_regularization(self):\n",
    "        l1_reg = torch.tensor(0., requires_grad=True)\n",
    "        for param in self.parameters():\n",
    "            l1_reg += torch.norm(param, 1)\n",
    "        return self.l1_penalty * l1_reg\n",
    "\n",
    "# ***Step 3: Validation Function with Expanded Logic***\n",
    "def val_fun_NN(model_class, params, X_trn, y_trn, X_vld, y_vld):\n",
    "    best_ros = None\n",
    "    best_model = None\n",
    "\n",
    "    for param in ParameterGrid(params):\n",
    "        # Model instance for each parameter combination \n",
    "        model = model_class(input_dim=X_trn.shape[1], **param)\n",
    "\n",
    "        # Loss with L1 regularization\n",
    "        criterion = nn.MSELoss()  # Mean Squared Error\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                     lr=param['learning_rate'])\n",
    "\n",
    "        # Training with Early Stopping\n",
    "        patience = param['patience']\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(param['epochs']):\n",
    "            # ... (Training loop - see below)\n",
    "\n",
    "            val_loss = criterion(model(X_vld), y_vld)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    break\n",
    "\n",
    "        y_pred = best_model(X_vld)\n",
    "        ros = r2_score(y_vld, y_pred)  # Using R-squared\n",
    "\n",
    "        #  ... (Logging as in your original function)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "# ... (Rest of your code) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# ... (other potential imports) \n",
    "\n",
    "# Load and preprocess your 'X' and 'y' dataframes (ensure proper preprocessing)\n",
    "\n",
    "# Out-of-Sample R-squared (Tensor-aware version)\n",
    "def R_oos(num, den):\n",
    "    R_oos_val = 1 - torch.sum(num) / torch.sum(den)\n",
    "    return R_oos_val.item()  # Convert to a Python scalar\n",
    "\n",
    "# Neural Network Model\n",
    "class StockReturnNN(nn.Module):\n",
    "    def __init__(self, n_layers, n_inputs, n_outputs, hidden_size, l1_penalty):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(n_inputs, hidden_size))\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_size))  # Batch Normalization \n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_size, n_outputs))\n",
    "        self.l1_penalty = l1_penalty\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def l1_regularization(self):\n",
    "        l1_reg = torch.tensor(0.)\n",
    "        for param in self.parameters():\n",
    "            l1_reg += torch.norm(param, 1)\n",
    "        return self.l1_penalty * l1_reg\n",
    "\n",
    "# Validation Function (Tensor-aware)\n",
    "def val_fun_NN(model_class, params: dict, X_trn, y_trn, X_vld, y_vld, illustration=True):\n",
    "  # (Similar core logic as your provided function, using PyTorch constructs)\n",
    "  # ... \n",
    " \n",
    "# Expanding Window Loop\n",
    "initial_train_years = 18\n",
    "validation_years = 12\n",
    "test_years = 1\n",
    "\n",
    "yrs = y.index.year.unique()  # Assuming 'y' is your target DataFrame\n",
    "\n",
    "results = []  # Store out-of-sample R-squared for each iteration\n",
    "\n",
    "for i in range(len(yrs) - initial_train_years - validation_years):\n",
    "    # ... (Window slicing logic)\n",
    "\n",
    "    # Data Preparation for PyTorch \n",
    "    train_dataset = TensorDataset(torch.Tensor(X_train.values), torch.Tensor(y_train))\n",
    "    val_dataset = TensorDataset(torch.Tensor(X_val.values), torch.Tensor(y_val))\n",
    "    test_dataset = TensorDataset(torch.Tensor(X_test.values), torch.Tensor(y_test))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=10000, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=10000, shuffle=False)  \n",
    "\n",
    "    # ... (Parameter grid, model training, evaluation as below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StockReturnNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m ensemble_predictions \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Store predictions from multiple seeds\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble_size\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m----> 7\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mStockReturnNN\u001b[49m(\n\u001b[0;32m      8\u001b[0m         n_layers\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m         n_inputs\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],  \u001b[38;5;66;03m# Number of features\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         n_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Single output for stock returns\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         hidden_size\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     12\u001b[0m         l1_penalty\u001b[38;5;241m=\u001b[39mparam[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     16\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StockReturnNN' is not defined"
     ]
    }
   ],
   "source": [
    "# ... Inside the expanding window loop ...\n",
    "\n",
    "for param in ParameterGrid(param_grid):\n",
    "    ensemble_predictions = []  # Store predictions from multiple seeds\n",
    "\n",
    "    for _ in range(param['ensemble_size']):\n",
    "        model = StockReturnNN(\n",
    "            n_layers=param['n_layers'],\n",
    "            n_inputs=X_train.shape[1],  # Number of features\n",
    "            n_outputs=1,  # Single output for stock returns\n",
    "            hidden_size=param['hidden_size'],\n",
    "            l1_penalty=param['l1']\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(param['epochs']):\n",
    "            # Training with early stopping\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_batch = y_batch.view(-1, 1)  # Reshape to (batch_size, 1) \n",
    "\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch) + model.l1_regularization()  # L1 added\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Validation\n",
    "            with torch.inference_mode(): \n",
    "                val_loss = 0.0\n",
    "                for x_val, y_val in val_loader:\n",
    "                    y_pred_val = model(x_val)\n",
    "                    val_loss += loss_fn(y_pred_val, y_val).item() # No L1 here\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= param['patience']:\n",
    "                break\n",
    "\n",
    "        # Ensemble prediction on test data\n",
    "        with torch.inference_mode():\n",
    "            test_preds = model(torch.Tensor(X_test.values))\n",
    "            ensemble_predictions.append(test_preds.numpy())\n",
    "\n",
    "    ensemble_preds = np.mean(ensemble_predictions, axis=0)  # Average predictions\n",
    "    r_oos = R_oos.torch(np.power(y_test - ensemble_preds.ravel(), 2), np.power(y_test - y_test.mean(), 2))\n",
    "\n",
    "    results.append({'params': param, 'R_oos': r_oos}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            layers.append(nn.Linear(input_size if i == 0 else hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_size))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'hidden_layers': [[32], [32, 16], [32, 16, 8], [32, 16, 8, 4], [32, 16, 8, 4, 2]],\n",
    "    'l1_penalty':  [1e-5, 1e-3],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    # ... other hyperparameters\n",
    "}\n",
    "\n",
    "# Expanding window loop\n",
    "for train_data, val_data, test_data in create_expanding_windows(X, y, train_years=18, ...):\n",
    "    X_train, y_train = train_data  # ... split data\n",
    "    X_val, y_val = val_data\n",
    "    X_test, y_test = test_data\n",
    "\n",
    "    best_params = None\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Hyperparameter search\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        val_loss = train_and_evaluate(X_train, y_train, X_val, y_val, params)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = params\n",
    "\n",
    "    # Test set evaluation (retraining with best hyperparameters)\n",
    "    # ... calculate out-of-sample R-squared \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_layers)):\n",
    "            layers.append(nn.Linear(input_size if i == 0 else hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_size))  # Output layer\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_val, y_val, hyperparams, num_ensembles=10):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
