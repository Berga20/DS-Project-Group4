{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datashare file contains all the time-series stocks data of all the U.S. markets, including NYSE, NASDAQ and AMEX. \n",
    "every stock also contains all the corresponding stock characteristics that are used as predictors in the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "Given the size of the origial dataset is too big to be imported on Github, the importing on the data has to be done locally. The Notebook provided includes all the steps we did in the datapreprocessing, but in order for the notebook to run smoothly it is necessary to dowload the original dataset first (a link is provided).\n",
    "\n",
    "[Stocks Data Dowlnoad](https://dachxiu.chicagobooth.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stocks = pd.read_csv(\"datashare.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stocks.index = Stocks[\"DATE\"]\n",
    "Stocks.index = pd.to_datetime(Stocks.index, format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stocks = Stocks.drop(\"DATE\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stocks = Stocks['1957-03':\"2016\"] \n",
    "Stocks['Month'] = Stocks.index\n",
    "Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Stocks Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_df = Stocks.groupby(Stocks.index).apply(lambda x: x.nlargest(500, 'mvel1')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_df.index = top500_df[\"Month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_df= top500_df.rename_axis('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top500_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List with Stock Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characteristics_l = list(set(top500_df.columns).difference({'permno', 'Month', 'sic2', 'weight', 'total_market_cap'}))\n",
    "len(characteristics_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Adding Stock Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we add to the dataframe the returns of every stock in the corresponding point in time. The data is retrieved from the CRSP databse (via WRDS) based on the PERMNO number of the stock. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rets = pd.read_csv(\"Permno_date_return.csv\")\n",
    "Rets['date'] = pd.to_datetime(Rets['date'])\n",
    "Rets.index = Rets['date']\n",
    "Rets.rename({'date':'Month'}, inplace=True, axis=1)\n",
    "Rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rets.rename({'PERMNO':'permno'}, inplace=True, axis=1)\n",
    "Rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(top500_df, Rets, on=['permno', 'Month'])\n",
    "merged.index = merged['Month']\n",
    "merged = merged.rename_axis('Date')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Removing stocks without returns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.dropna(subset=['RET'])\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing observations without stock returns a total of 643 rows is removed (0.18%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Stock-Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use the stock market cap to compute the corresponding weight in that point in time for every company in the replicating portfolio (S&P500). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Month'] = pd.to_datetime(merged['Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_market_cap = merged.groupby(merged['Month'].dt.to_period(\"M\"))['mvel1'].sum()\n",
    "merged = merged.merge(total_market_cap.rename('total_market_cap'), left_on=merged['Month'].dt.to_period(\"M\"), right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['weight'] = merged['mvel1'] / merged['total_market_cap']\n",
    "merged = merged.drop('key_0', axis=1)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Month'] = merged['Month'].dt.strftime('%Y-%m')\n",
    "merged.index = merged['Month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.rename_axis('Date')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Creating Dummy variables for SIC2 characteristic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we compiute the dummy variables for the industry code (variable) \"SIC2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_variables = pd.get_dummies(merged['sic2'], prefix='SIC')\n",
    "merged_dum = pd.concat([merged, dummy_variables], axis=1)\n",
    "\n",
    "merged_dum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dum['sic2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper 74 dummies are obtained, in our case only 65 since we are reducing the analysis only to the firms contained in the S&P500 in the time period considered, which results in 9 less industry dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Replicating Portfolio - Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we compute the (weighted) monthly return of every stock in order to compute the return of the replicating portfolio in every period. This step will also allow to check the correlation or the replicating portfolio returns with the ones of the index, to check the accuracy of the portfolio with the benchmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['RET'] = merged['RET'].astype(float)\n",
    "merged['weighted_RET'] = merged['weight'] * merged['RET']\n",
    "replicating_returns = merged.groupby('Date')['weighted_RET'].sum().reset_index()\n",
    "replicating_returns.index = replicating_returns['Date']\n",
    "replicating_returns = replicating_returns.drop('Date', axis=1)\n",
    "replicating_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Macro Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data source is professor's Amid Goyal's personal website, but this dataset has been included in the Github folder under the name \"PredictorData2022\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, firm characteristics are lagged due to the data being released with a delay. To match this we lag macro predictors by one month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Macro_pred = pd.read_csv('PredictorData2022.csv', parse_dates=True, index_col=0)\n",
    "Macro_pred.index = pd.to_datetime(Macro_pred.index, format=\"%Y%m\").to_period('M')\n",
    "Macro_pred_lag = Macro_pred.shift(1) #lag values by one month\n",
    "Macro_pred_lag = Macro_pred_lag.rename_axis('Date')\n",
    "Macro_pred_lag = Macro_pred_lag['1957-03':'2016']\n",
    "Macro_pred_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Macro_pred= Macro_pred.rename_axis('Date')\n",
    "Macro_pred = Macro_pred['1957-03':'2016']\n",
    "Macro_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Macroeconomic Factors: \n",
    "* including dividend-price ratio (dp): (d/p) is the difference between the log of dividends and the log of prices.\n",
    "* earnings-price ratio (ep): (e/p) is the difference between the log of earnings and the log of prices.\n",
    "* book-to-market ratio (bm)\n",
    "* net equity expansion (ntis)\n",
    "* Treasury-bill rate (tbl)\n",
    "* term spread (tms): The Term Spread (tms) is the difference between the long term yield on government bonds (lty) and the Treasury-bill (tbl)\n",
    "* default spread (dfy): The Default Yield Spread (dfy) is the difference between BAA and AAA-rated corporate bond yields.\n",
    "* stock variance (svar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Macro_pred_lag['Index'] = Macro_pred_lag['Index'].str.replace(',', '').astype(float)\n",
    "Macro_pred_lag[\"d/p\"] = np.log(Macro_pred_lag[\"D12\"]) - np.log(Macro_pred_lag[\"Index\"])\n",
    "Macro_pred_lag[\"e/p\"] = np.log(Macro_pred_lag[\"E12\"]) - np.log(Macro_pred_lag[\"Index\"])\n",
    "Macro_pred_lag[\"tms\"] = Macro_pred_lag[\"lty\"] - Macro_pred_lag[\"tbl\"]\n",
    "Macro_pred_lag['dfy'] = Macro_pred_lag['BAA'] - Macro_pred_lag['AAA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the macroeconomic varibales used in the paper are selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the redundent variables - Selecting the needed Macro predictors used in the paper\n",
    "Macro_pred_sel = Macro_pred_lag.drop(columns=['Index','D12', 'E12', 'AAA', 'BAA', 'CRSP_SPvwx', 'corpr','Rfree', 'CRSP_SPvw', 'lty','infl', 'ltr','csp'])\n",
    "Macro_pred_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating list with Macro Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Macro_pred_l = ['b/m', 'tbl', 'ntis', 'svar', 'd/p', 'e/p', 'tms', 'dfy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'months' column from the existing DatetimeIndex\n",
    "Macro_pred_sel['Month'] = Macro_pred_sel.index.strftime('%Y-%m')\n",
    "\n",
    "# Display the DataFrame to verify the new column\n",
    "print(Macro_pred_sel.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Reliability of Replicating Portfolio - S&P500 Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.99543735],\n",
       "       [0.99543735, 1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(replicating_returns['weighted_RET'], Macro_pred['CRSP_SPvw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00027194334994176846"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(np.mean(replicating_returns['weighted_RET'])-np.mean(Macro_pred['CRSP_SPvw']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.990573194619459e-06"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(np.var(replicating_returns['weighted_RET'])-np.var(Macro_pred['CRSP_SPvw']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewness:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0639163545294062"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(skew(replicating_returns['weighted_RET'])-skew(Macro_pred['CRSP_SPvw']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kurtosis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057543532982029966"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(kurtosis(replicating_returns['weighted_RET'])-kurtosis(Macro_pred['CRSP_SPvw']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Creating Interaction Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_macro_char = pd.merge(Macro_pred_sel, merged_dum, on=['Month'])\n",
    "merged_macro_char.index = merged_macro_char['Month']\n",
    "merged_macro_char = merged_macro_char.rename_axis('Date')\n",
    "merged_macro_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = merged_macro_char.copy()\n",
    "for fc in characteristics_l:\n",
    "    for mp in Macro_pred_l:\n",
    "        data[fc + '*' + mp] = merged_macro_char[fc] * merged_macro_char[mp]\n",
    "        \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Computing Excess Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['RET'] = pd.to_numeric(data['RET'], errors='coerce')\n",
    "data['tbl'] = pd.to_numeric(data['tbl'], errors='coerce')\n",
    "\n",
    "# Perform subtraction operation\n",
    "data['RET'] = data['RET'] - data['tbl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'RET': 'Exc_RET'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns=Macro_pred_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Scaling Variables to [-1,1] and Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = list(set(data.columns).difference({'permno','Month','Exc_RET','total_market_cap','weight','sic2'})) # a list storing all features\n",
    "\n",
    "X = MinMaxScaler((-1,1)).fit_transform(data[features])\n",
    "X = pd.DataFrame(X, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Exc_RET']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.index = data.index\n",
    "X.fillna(0, inplace=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Exporting Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to export the processed datasets, delete the '#' symbols from the cell below. Note that it takes a bit of time depending on the computer, generally between 3 and 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_csv('Features_lagged_X.csv')\n",
    "y.to_csv('Dependent_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame(merged['weight'])\n",
    "weights.to_csv('Stocks_weights.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
